<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>机器学习 on 墨纹</title>
    <link>https://MUNLELEEhttps://github.com/MUNLELEE/MUNLELEE.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/</link>
    <description>Recent content in 机器学习 on 墨纹</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 06 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://MUNLELEEhttps://github.com/MUNLELEE/MUNLELEE.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>机器学习算法 | K近邻</title>
      <link>https://MUNLELEEhttps://github.com/MUNLELEE/MUNLELEE.github.io/post/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://MUNLELEEhttps://github.com/MUNLELEE/MUNLELEE.github.io/post/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid>
      <description>K近邻算法 K近邻是一种有监督学习算法。因为没有对数据进行训练，而是通过新数据与旧数据的比较得到相应的结果。因此是一种隐式的学习过程和训练过程。K近邻算法可以用来解决分类问题，也可以用来解决回归问题。
步骤 对未知类别的属性的数据集中的每个点依次执行以下操作：
计算已知类别数据集中点与当前点之间的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在的类别出现的频率 返回前k个点出现频率最高的类别作为当前点的预测分类 在确定样本和当前点的距离时，通常采用的是欧式距离公式$$d=\sqrt{(x_1-x_0)+(y_1-y_0)}$$ 当公式中的指数变化时，随之也会得到相应的不同的距离公式。
如下图所示的例子中
1、当采用实线的圆作为k近邻的范围，也就是$k=3$时，此时与绿点距离更近的三个点中，三角形出现的频率更大，因此将绿点归为三角形一类
2、当采用虚线的圆作为k近邻的范围，也就是$k=5$，时，此时与绿点距离更近的五个点中，正方形的频率更大，因此将绿点归为正方形一类。
以下采用鸢尾花作为例子进行KNN测试
代码 import numpy as np import pandas as pd from sklearn.datasets import load_iris import matplotlib.pyplot as plt iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) # print(df) df[&amp;#39;label&amp;#39;] = iris.target # print(len(df)) 150 # 绘散点图 # print(df.info()) # 显示数据类型 # 前两个特征 Colors = [] for i in range(df.shape[0]): item = df.iloc[i, -1] # 定位到标签 if item == 0: Colors.</description>
    </item>
    
  </channel>
</rss>
