<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>算法 on 墨纹</title>
    <link>https://MUNLELEE.github.io/tags/%E7%AE%97%E6%B3%95/</link>
    <description>Recent content in 算法 on 墨纹</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <lastBuildDate>Wed, 01 Feb 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://MUNLELEE.github.io/tags/%E7%AE%97%E6%B3%95/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>社区发现 | CPM算法及其实现</title>
      <link>https://MUNLELEE.github.io/post/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0-cpm%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 01 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://MUNLELEE.github.io/post/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0-cpm%E7%AE%97%E6%B3%95/</guid>
      <description>前言 近日有了解社区发现相关内容以及实现这个比较古老的算法的需要，索性就了解一下社区发现，并尝试着复现这个算法。
社区发现概述 什么是社区 在现代人们的日常生活中，每个人都会进行社交，这在无形中就形成了一个社交网络。在这个网络中，每个人可以视为一个点，而用户之间的点赞、关注以及其他行为形成了边。在这样的一个网络中，有的用户之间联系较为紧密，有的用户之间联系较为稀疏。联系较为紧密的几个用户可以形成一个社区，在社区内，每个节点的链接都较为紧密，而两个社区之间的联系就较为稀疏。整体的结构可以被称为社团结构。如下图，圈起来的部分即为社区。
社团中的边结构其实是一种逻辑抽象，并不是一种空间位置上的关系，而是节点之间的共有关系，假如节点代表的是消费者，那么节点间的边则可能代表消费者购买了共同一类商品，边的权重则可以代表购买物品的数量。
社区发现的目的及简单思路 社区发现的目的就是为了在图中找到具有一定共同关系或者潜在特定关系的组织，也就是社区。社区发现也就是为了寻找图网络中联系较为紧密的社区也称为块（cluster）。基于这种想法，我们可以想到如果要进行社区发现那么就要寻找一个联系较为紧密的块，我们可以将其解释为具有较大的密度，之后便是考虑用什么方法去评估这种密度，类似于神经网络中的损失函数。
CPM算法 CPM（派系过滤算法）算法是最早的重叠社区发现算法，它的思想是基于团渗透的，这个算法认为社区是具有共享节点的全连通子集集合，并通过一种团过滤算法来识别网络中的社区结构。在这个算法中遵循以下两个概念
在图网络中视为团的部分是任意两个节点都存在一条边相连的，也就是完全子图 所有彼此连通的k-团（拥有k个节点）构成一个k-团社区，当两个k-团之间存在$k-1$个节点共享那么认为这两个k-团连通 算法步骤 算法首先需要找到网络中的所有团，并构建一个用来表示团团重叠情况的矩阵（matrix），在这个矩阵中matrix[i][j]表示网络中第i个团和第j个团之间的公共节点数。 根据给定的参数k，将矩阵（matrix）中对角线上小于$k$的元素和非对角线上的小于$k-1$的元素置零，其他元素置为一，这样所有对角线为1的即为k-团，而非对角线为1的即为两个相邻的k-团。 将相邻的k-团合并为一个较大的团社区，并可以使用模块度进行评价。 从上面可以得知算法中首先需要做到是得到k-团，因此为了提高效率，在这里需要使用Born_Kerbosch算法来寻找图中的团。在这里也简单介绍一下这个算法。
Born_Kerbosch算法 算法的初始化包括了以下三个集合
R集合：记录当前极大团中已经加入的点 P集合：记录可能可以继续加入极大团中的点（这些点应该与R集合中的所有点都相连） X集合：记录已经加入过极大团的点（用于判重，因为会从每个节点开始，枚举所有的团） 简单流程：
对于每一个在集合P中的点v，将v加入R集合中，之后更新P集合，确保集合中的节点与v相连。 进行回溯时，将v节点从P集合中取出，并加入X集合，表示包含v节点的极大团已经寻找完毕了 当R集合满足为极大团时，P集合和X集合必须为空。因为P集合中包含的点是可能加入R集合中的点，同样的X集合中的点也与R集合中的点都相邻，因此也属于可能称为R集合中极大团点的情况。 算法缺点 CPM算法较为简单，但存在不能为单节点分配社团以及比较使用于完全子图较多的网络中的问题，也即边密集网络中，在稀疏网络中算法的效率较低。
模块度 模块度是评估一个社区网络划分好坏度量方法，其含义为社区内节点的边数与随机情况下的边数之差。定义如下：
$$Q=\frac{1}{2m} \sum_{i, j}[A_{i j}-\frac{k_i k_j}{2m}]\delta(c_i, c_j) \qquad \delta(u, v)=\begin{cases}1,&amp;amp;u==v \newline 0, &amp;amp;else \end{cases}$$
其中$A_{i j}$是节点$i$和节点$j$之间边的权重，当图网络不带权时，可以将其视为1；$k_i=\sum_j{A_{ij}}$表示所有与节点$i$相连的边的权重之和；$c_i$表示节点$i$所属的社区；$m={1\over2}\sum_{ij}A_{ij}$表示所有边的权重之和。$\frac{k_i k_j}{2m}$表示随机情况下节点$i$与节点$j$之间产生的边。
对这个公式做进一步简化可以得到如下的公式：
$$Q=\frac{1}{2m}\sum_c{[\sum{in}-\frac{{(\sum{tot})}^2}{2m}]}$$
其中$\sum{in}$表示社区$c$内边的权重之和，$\sum{tot}$表示与社区$c$节点相连的边的权重之和。
可以这么简单的理解模块度，即社区内部边的权重和减去社区外部与社区内部相连的边的权重和。
算法实现 在这部分将讲述算法实现的一些较为重要的地方，完整的代码可以点此，需要的朋友可以直接前往。
实现算法要先实现寻找极大团，这里使用Born_Kerbosch算法，这个算法有两个版本，但我这里采用了经过一些剪枝处理的版本，在递归之前需要先确定需要的枢纽元素，数据结构方面采用集合（自动去重属实是太好用了^_^） /** * 使用BornKerbosch算法寻找最大团 * @param R 存在于极大团中的点 * @param P 可能可以加入极大团的点 * @param X 用于判重的点集合 * @param ans 将最后寻找到的所有极大团保存 * @param neighbor 存储每个节点的邻居节点，是一个map */ def bornKerbosch(R: Set[VertexId], P: Set[VertexId], X: Set[VertexId], ans: Set[Set[VertexId]], neighbor: Map[VertexId, Set[VertexId]]): Unit = { if (P.</description>
    </item>
    
    <item>
      <title>机器学习算法 | K近邻</title>
      <link>https://MUNLELEE.github.io/post/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</link>
      <pubDate>Wed, 06 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://MUNLELEE.github.io/post/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/</guid>
      <description>K近邻算法 K近邻是一种有监督学习算法。因为没有对数据进行训练，而是通过新数据与旧数据的比较得到相应的结果。因此是一种隐式的学习过程和训练过程。K近邻算法可以用来解决分类问题，也可以用来解决回归问题。
步骤 对未知类别的属性的数据集中的每个点依次执行以下操作：
计算已知类别数据集中点与当前点之间的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在的类别出现的频率 返回前k个点出现频率最高的类别作为当前点的预测分类 在确定样本和当前点的距离时，通常采用的是欧式距离公式$$d=\sqrt{(x_1-x_0)+(y_1-y_0)}$$ 当公式中的指数变化时，随之也会得到相应的不同的距离公式。
如下图所示的例子中
1、当采用实线的圆作为k近邻的范围，也就是$k=3$时，此时与绿点距离更近的三个点中，三角形出现的频率更大，因此将绿点归为三角形一类
2、当采用虚线的圆作为k近邻的范围，也就是$k=5$，时，此时与绿点距离更近的五个点中，正方形的频率更大，因此将绿点归为正方形一类。
以下采用鸢尾花作为例子进行KNN测试
代码 import numpy as np import pandas as pd from sklearn.datasets import load_iris import matplotlib.pyplot as plt iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) # print(df) df[&amp;#39;label&amp;#39;] = iris.target # print(len(df)) 150 # 绘散点图 # print(df.info()) # 显示数据类型 # 前两个特征 Colors = [] for i in range(df.shape[0]): item = df.iloc[i, -1] # 定位到标签 if item == 0: Colors.</description>
    </item>
    
    <item>
      <title>岛屿问题 | DFS框架</title>
      <link>https://MUNLELEE.github.io/post/%E5%B2%9B%E5%B1%BF%E9%97%AE%E9%A2%98/</link>
      <pubDate>Sat, 02 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>https://MUNLELEE.github.io/post/%E5%B2%9B%E5%B1%BF%E9%97%AE%E9%A2%98/</guid>
      <description>在DFS这一块一直是比较弱的，因此就去看了DFS相关的一些题目，并找到了岛屿的一系列问题。
岛屿数量 岛屿的最大面积 最大人工岛 封闭岛屿的数目 岛屿的周长 图类DFS方法 图通常是由方格组成，通过方格中的元素来对图的DFS进行限制。岛屿问题便是其中经典的一类。在岛屿问题中，通常由$1$表示陆地，由$2$表示海洋，当每个方格都相邻时，组成的一个全$1$方格域即为一个岛屿。（这里不包含对角相邻）
DFS框架 DFS也是一种意义上的递归，因此在一个DFS程序中首先要做的便是设定递归出口。类似于树结构可以利用指针为空等条件，在岛屿问题中，我们需要判断边界，也需要判断当前方格是否是陆地。因此在递归出口设置中就需要两个条件进行限制。
之后DFS就要考虑下一步搜索与前一轮搜索之间的区别。岛屿问题中最主要的区别便是遍历方格的四个邻格，其余的区别便需要依题目而定。因此我们可以得到以下的岛屿类问题的DFS框架
class Solution { private: bool inRange(vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;amp; grid, int x, int y) { if (x &amp;gt;= 0 &amp;amp;&amp;amp; x &amp;lt; grid.size() &amp;amp;&amp;amp; y &amp;gt;= 0 &amp;amp;&amp;amp; y &amp;lt; grid[0].size()) { return true; } return false; } void dfs(vector&amp;lt;vector&amp;lt;int&amp;gt;&amp;gt;&amp;amp; grid, int pos_x, int pos_y) { if (!inRange(grid, pos_x, pos_y)) return; if (grid[pos_x][pos_y] != 1) return; dfs(grid, pos_x - 1, pos_y); dfs(grid, pos_x + 1, pos_y); dfs(grid, pos_x, pos_y + 1); dfs(grid, pos_x, pos_y - 1); } } 避免重复遍历 在DFS中一个重要的问题就是要避免重复遍历，不然可能会造成程序原地打转的现象。在岛屿一类的问题中，可以将已经遍历过的方块修改其元素为$0$或者为非题中给出元素。因此我们可以修改上述的DFS模板如下</description>
    </item>
    
  </channel>
</rss>
