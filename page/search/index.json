[{"content":"前言 最新在开发学习的时候使用到了WSL（苦于没有Mac电脑っ╥╯﹏╰╥c），在使用时发现了一些痛点。打开WSL时，会在任务栏保留一个程序窗口，如果将这个程序窗口关闭，那么存在于WSL内的Docker容器也随之关闭，这点十分难受。于是就想有什么办法能够在不显示窗口的情况下在后台运行WSL。在实践的过程中还蛮有意思，遂稍微记录。\n让WSL“成为”应用程序 我希望的就是能够通过某种实现来随时控制WSL的启动和停止，这里能够想到的就是使用脚本来控制。\n启动 首先需要一个命令来启动WSL并让它保持运行。这里可以选择使用 sleep infinity ，但是，单纯让WSL保持运行会在界面上保留一个黑色控制台，就膈应。这时候可以考虑使用 -WindowStyle Hidden 来隐藏这个控制台，可以利用脚本来实现这个命令。\n脚本的实现有两种方式，分别是powershell和cmd，这里选择了powershell（个人觉得cmd的命令没有powershell易读）。那么首先就要创建这样一个脚本，使用记事本或者vscode创建一个新的文本，在文本中键入以下命令\nStart-Process -FilePath \u0026#34;wsl.exe\u0026#34; -ArgumentList \u0026#34;--distribution \u0026lt;Ubuntu发行版本\u0026gt; --exec sleep infinity\u0026#34; -WindowStyle Hidden 在保存之前，需要将命令中缺少的Ubuntu发行版本替换为你自己的版本，可以使用 wsl -l -v 这个命令来查看。最后只要将这个文件保存为 .ps1 文件即可。之后执行这个脚本就能够实现想要的功能。\n优化 不过，随之而来的是第二个问题，执行这个脚本是不是还得敲命令呢？说到底，还是没有绕开启动的这一个命令。就有了下面这个方法，将脚本的启动绑定到快捷方式中。\n在桌面右键，选择 新建-\u0026gt;快捷方式 ，在弹出的窗口中键入以下命令\npowershell.exe -ExecutionPolicy Bypass -File \u0026lt;script path\u0026gt; 你需要将 \u0026lt;script path\u0026gt; 替换为你刚刚创建的脚本路径。命令输入的位置如下图\n之后你就可以得到一个快捷方式，点击这个快捷方式就能够直接启动WSL了。\n停止 做完启动的功能，停止就显得十分简单了。同样新建一个快捷方式，输入下面这个命令\nwsl --shutdown 保存即可。\n有了启动和停止，自然会联想到查看当前WSL的状态，这里就再使用同样的方法，创建查看状态的快捷方式，命令如下\ncmd /k wsl -l -v 其中 cmd /k 作用是打开窗口执行之后的命令，同时保留窗口显示结果。\n到此为止，就可以通过三个快捷方式点击实现WSL的开启、关闭和查看状态了。既没有了任务窗口，Docker容器也不会停止了。\n","date":"2025-11-01T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E8%AE%A9wsl%E5%83%8F%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E4%B8%80%E6%A0%B7/","title":"让WSL像应用程序一样"},{"content":"前言 在一个项目中，经常会遇到这样一个需求，希望能够在程序运行过程中，直接动态变更某些属性配置。这些动态变更的配置可能包括降级和切量开关等等。\n当然，如果是一个微服务项目，动态配置这一功能实现显然是可以通过注册中心（Nacos）来满足。但实际上，亦可以通过代码本身来实现这个功能。在这篇博客中就使用 Java反射+Redis发布/订阅 的方式来实现。\n实现方法 在真正开始实现前，不妨可以想想技术路线。能够知道的是， Java反射机制的核心是在程序运行时动态加载类并获取类的详细信息，从而操作类或对象的属性和方法。 这个能力显然和需求十分匹配。那么如何能够在反射时知道要修改哪些字段以及修改的值是什么呢？前者可以通过自定义注解来实现，在希望进行动态配置的字段上添加相应的注解作为标记，当Spring扫描Bean对象时就可以管理这些类的属性。后者可以通过Redis的发布/订阅来实现（其实也就是消息队列），监听某一个topic获取相应要更改的值。\n经过上面的描述，要自己实现动态配置大概可以分为以下几个步骤：\n添加一个自定义注解，用于Spring扫描Bean对象的时候，可以直接管理这些配置了自定义注解的类的属性。 给服务类的属性添加自定义注解，这里的服务类也就是包含了动态配置属性的对象。 添加一个动态配置管理的工厂，用来自动完成属性信息的填充和动态变更操作 业务的使用，这里会调用步骤2中的属性服务。当配置有变动时，可以把配置信息直接刷新到内存属性上。 实现动态变更接口，当调用相应接口时，触发Redis的发布/订阅，以此来更新类上的属性。 自定义注解的实现和使用 自定义注解 @Retention(RetentionPolicy.RUNTIME) @Target({ElementType.FIELD}) @Documented public @interface DCCValue { String value() default \u0026#34;\u0026#34;; } 按照常规方式配置一个自定义注解，同时具有一个value属性值，默认值为空字符串即可。\n自定义注解的使用 @Service public class DCCService { // 是否降级 @DCCValue(\u0026#34;downgradeSwitch:0\u0026#34;) private String downgradeSwitch; // 是否切量 @DCCValue(\u0026#34;cutRange:100\u0026#34;) private String cutRange; public boolean isDowngradeSwitchOn() { return \u0026#34;1\u0026#34;.equals(downgradeSwitch); } public boolean isCutRange(String userId) { int hashCode = Math.abs(userId.hashCode()); // 获取最后两位 int lastTwoDigit = hashCode % 100; if (lastTwoDigit \u0026lt;= Integer.parseInt(cutRange)) { return true; } return false; } } 创建DCCService服务类，交给Spring管理，也是动态配置项的实际使用类，在这个类中定义了 downgradeSwitch，cutRange两个具有动态配置能力的属性（也就是使用 @DCCValue 注解）。两个属性分别表示是否降级以及切量，降级简单的通过0和1来决定开启或关闭。切量通过userId的哈希值的最后两位进行判断。\n动态变更属性值 Bean对象初始化后操作 在这部分中，就要开始真正实现动态变更操作。需要创建一个配置类实现 BeanPostProcessor 接口，同时重写 postProcessAfterInitialization(Object bean, String beanName) 方法。如下：\n@Slf4j @Configuration public class DCCValueBeanFactory implements BeanPostProcessor { private final String BASE_CONFIG_PAHT = \u0026#34;group_buy_market_dcc_\u0026#34;; private RedissonClient redissonClient; private final Map\u0026lt;String, Object\u0026gt; dccObjGroup = new HashMap\u0026lt;\u0026gt;(); public DCCValueBeanFactory(RedissonClient redissonClient) { this.redissonClient = redissonClient; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { Class\u0026lt;?\u0026gt; targetBeanClass = bean.getClass(); Object targetBeanObject = bean; // 判断是否有代理 if (AopUtils.isAopProxy(bean)) { targetBeanClass = AopUtils.getTargetClass(bean); targetBeanObject = AopProxyUtils.getSingletonTarget(bean); } // 获取对象的字段 Field[] fields = targetBeanClass.getDeclaredFields(); for (Field field : fields) { // 判断这个字段是否有DCCValue注解 if (!field.isAnnotationPresent(DCCValue.class)) { continue; } // 获取字段的DCCValue注解 DCCValue dccValue = field.getAnnotation(DCCValue.class); // 判断字段是否为空，为空抛出异常 String value = dccValue.value(); if (StringUtils.isBlank(value)) { throw new RuntimeException(\u0026#34;...\u0026#34;); } String[] split = value.split(\u0026#34;:\u0026#34;); String key = BASE_CONFIG_PAHT.concat(split[0]); String defaultValue = split.length == 2 ? split[1] : null; String setValue = defaultValue; try { // 判断是否设置了对应的值 设置错误抛出异常 if (StringUtils.isBlank(defaultValue)) { throw new RuntimeException(\u0026#34;...\u0026#34;); } // 从redis中获取设置值 RBucket\u0026lt;String\u0026gt; bucket = redissonClient.getBucket(key); boolean exists = bucket.isExists(); if (!exists) { bucket.set(defaultValue); } else { setValue = bucket.get(); } // 填充值 field.setAccessible(true); // 允许访问 field.set(targetBeanObject, setValue); field.setAccessible(false); } catch (IllegalAccessException e) { throw new RuntimeException(e); } dccObjGroup.put(key, targetBeanObject); } return bean; } } 在这个配置类中，首先是实现了 BeanPostProcessor接口，意味着它能够在Spring容器初始化Bean之后，对Bean进行额外的处理。重写 postProcessAfterInitialization 方法就是为了实现对Bean初始化之后的操作。 因为在DCCService类上加上了Service注解，让这个服务类被Spring管理，所以这个方法能够扫描到服务类对象。\n在重写的 postProcessAfterInitialization 方法中\n这个方法会对所有Bean对象进行扫描，通过反射获取字节码文件以及Object对象。 这里需要注意的是，要判断拿到的对象是否是代理对象，如果是，需要借助AopUtils重新获取相应的字节码文件和Object对象。 利用Class文件获取对象声明的属性，并在判断属性是否加上了 @DCCValue 注解，如果有，就判解析出注解上的配置项和默认值。同时，由于将配置值存放在Redis中，也需要去判断Redis中是否存在，存在就使用Redis中的值。 将反射获取到的对象字段设置为可访问，并更新其相应的配置值。这里可以利用一个Map来存储反射获取的对象，避免之后再次通过反射去寻找。 Redis订阅/发布 为了实现能够多次更改配置项的值，还需要使用Redis中的订阅/发布功能。还是在这个类中，创建一个Redis主题的监听方法，并将这个主题作为Bean对象返回（交给Spring管理，在发布消息时能够使用）。\npublic RTopic dccRedisTopicListener(RedissonClient redissonClient) { // 创建Topic RTopic topic = redissonClient.getTopic(\u0026#34;group_buy_market_dcc\u0026#34;); // 为这个topic添加监听器 topic.addListener(String.class, new MessageListener\u0026lt;String\u0026gt;() { // 第一个参数代表哪个主题（也就是topic)，第二个参数就是实际的消息内容 @Override public void onMessage(CharSequence charSequence, String s) { String[] split = s.split(Constants.SPLIT); String attribute = split[0]; String key = BASE_CONFIG_PAHT + attribute; String value = split[1]; RBucket\u0026lt;Object\u0026gt; bucket = redissonClient.getBucket(key); boolean exists = bucket.isExists(); if (!exists) return; bucket.set(value); // 通过之前存储的map获取一下对象 Object objBean = dccObjGroup.get(key); if (null == objBean) return; // 获取class Class\u0026lt;?\u0026gt; objBeanClass = objBean.getClass(); if (AopUtils.isAopProxy(objBeanClass)) { objBeanClass = AopUtils.getTargetClass(objBean); } try { Field field = objBeanClass.getDeclaredField(attribute); field.setAccessible(true); field.set(objBean, value); field.setAccessible(false); } catch (Exception e) { throw new RuntimeException(e); } } }); return topic; } 这个方法实现思路同上面的方法异曲同工，也是通过反射来修改对象的配置值。只不过经过 postProcessAfterInitialization 的执行之后，可以通过之前建立的Map来直接获取对象。当然，依旧需要判断获取到的对象是否是代理对象。\n与 postProcessAfterInitialization 方法不同的是，在设置配置值时要从Redis中获取，如果Redis中不存在相应的配置项，那么就可以直接返回了。\n外部接口的实现 到了这一步就是简单的接口实现了，只需要在调用接口时，往Redis中的相应key中发送一条消息即可。\npublic interface IDCCService { Response\u0026lt;Boolean\u0026gt; updateConfig(String key, String value); } @Slf4j @RestController() @CrossOrigin(\u0026#34;*\u0026#34;) @RequestMapping(\u0026#34;/api/v1/gbm/dcc/\u0026#34;) public class DCCController implements IDCCService { @Resource private RTopic dccTopic; @RequestMapping(value = \u0026#34;update_config\u0026#34;, method = RequestMethod.GET) @Override public Response\u0026lt;Boolean\u0026gt; updateConfig(@RequestParam String key, @RequestParam String value) { try { // 往topic发送消息 dccTopic.publish(key + \u0026#34;,\u0026#34; + value); return Response.\u0026lt;Boolean\u0026gt;builder() .code(ResponseCode.SUCCESS.getCode()) .info(ResponseCode.SUCCESS.getInfo()) .build(); } catch (Exception e) { return Response.\u0026lt;Boolean\u0026gt;builder() .code(ResponseCode.UN_ERROR.getCode()) .info(ResponseCode.UN_ERROR.getInfo()) .build(); } } } 为什么要检测对象是否是代理对象 在博客的最后，回过头去想一想，为什么在 DCCValueBeanFactory 中有这样一段代码\nif (AopUtils.isAopProxy(bean)) { targetBeanClass = AopUtils.getTargetClass(bean); targetBeanObject = AopProxyUtils.getSingletonTarget(bean); } 加入这段代码的核心原因是为了正确处理被Spring AOP代理过的Bean对象，确保反射能够作用于原始的目标对象，而不是代理对象。 下面就解释Spring AOP与代理对象\nAOP核心概念 AOP (Aspect-Oriented Programming) ： 面向切面编程，用于将 横切关注点 （如日志、事务、权限）从业务逻辑中分离出来，形成可重用的 切面 (Aspect) 。 目标对象 (Target Object) ：包含核心业务逻辑的原始对象。 AOP代理 (AOP Proxy) ：AOP框架创建的一个对象，它包裹了目标对象。应用程序中实际引用的是这个代理对象。 代理对象（Proxy）的工作原理 代理对象就像一个“中介”。当客户端调用一个Bean的方法时，调用首先被代理对象拦截。其调用过程如下：\n客户端调用 proxy.doSomething() 。 代理对象 拦截调用，执行 前置通知（如打印日志）。 代理对象将调用 委托 给被它包裹的 原始目标对象 的 doSomething() 方法。 目标对象 执行真正的业务逻辑。 业务逻辑执行完毕后，返回给 代理对象。 代理对象 获得控制权，执行 后置通知（如记录方法耗时）。 最后，代理对象 将最终结果返回给客户端。 因此，如果不检测反射获取的对象是否是代理对象，就会发现所作的修改是作用在了无用的代理对象上，而真正执行业务的目标对象内部实际上是没有改变的，业务最后也会继续按照旧的配置进行。\n","date":"2025-10-26T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E5%8A%A8%E6%80%81%E9%85%8D%E7%BD%AE/","title":"如何实现动态配置"},{"content":"前言 最新在学习项目的时候发现了一个很巧妙的设计模式，虽然长期处于MVC架构的桎梏下，遇见这种设计模式难免令人眼前一亮，遂打算记录一下。\n背景 项目背景是在一个拼团活动的场景下。对于这么一个业务，当点击进入商品页面时，如果商品存在优惠信息我们总是能够看到优惠之后的价格，这也就是商品的试算。这时候不妨想想，如果有多种优惠活动，会产生不同的优惠金额，那么要怎么实现这种逻辑呢？在传统的MVC架构下，最常见的可能就是将不同的优惠活动使用不同的Service进行封装，在商品试算的时候使用if else来决定采用哪种优惠策略。但这显然是不够优雅的，如果是在DDD架构下，设计模式能够摆脱MVC的限制，从而更好的融合到项目中，这也就是这篇博客要阐述的内容，即”链式多分支规则树模型“\n碍于笔者的技术所限，MVC架构应当也有一些其他的方式能够避免上述if else的情况，这里权且就不再深究\n链式多分支规则树模型 先整体看一下模型的结构，如下图所示\n链式多分支规则树模型结构 首先，定义抽象的通用的规则树模型结构。涵盖；StrategyMapper - 策略映射器、StrategyHandler - 策略处理器、AbstractStrategyRouter\u0026lt;T, D, R\u0026gt; - 策略路由抽象类。通过泛型设计允许使用方可以自定义出入参和动态上下文，让抽象模板模型具有通用性。 之后，由使用方自定义出工厂、功能抽象类和一个个流程流转的节点。这些节点可以自由组装进行流转，相比于责任链它的实现方式更具有灵活性。 策略处理器 策略处理器的接口代码如下所示：\npublic interface StrategyHandler\u0026lt;T, D, R\u0026gt; { /** 默认处理 */ /// 函数式接口，因此DEFAULT等效于一个始终返回null的apply方法的有效实现 StrategyHandler DEFAULT = (T, D) -\u0026gt; null; /** * 处理业务流程 * @param requestParam 入参 * @param dynamicContext 上下文 * @return 返回参数 * @throws Exception */ R apply(T requestParam, D dynamicContext) throws Exception; } 策略处理器用来处理执行的业务流程。在每个业务流程执行时，如果有某些数据是前面的节点到后面的节点都会使用到的，那么就可以将这些数据存放到上下文中。同时，实现一个默认的处理器，对所有的入参都返回空。可以用来停止流程。\n策略映射器 策略映射器的接口代码如下：\npublic interface StrategyMapper\u0026lt;T, D, R\u0026gt; { /** * 获取对应的策略处理器 * @param requestParam 入参 * @param dynamicContext 上下文 * @return 返回参数 * @throws Exception */ StrategyHandler\u0026lt;T, D, R\u0026gt; get(T requestParam, D dynamicContext) throws Exception; } 策略映射器提供了 get方法来获取策略处理器，也就相当于获取业务流程的每个节点，这也就避免了将所有逻辑都写到一个类中。\n策略路由器 处理器和映射器分别用来处理业务流程和获取业务流程的每个节点，那么真正决定业务流程的走向便是路由器。其具体实现如下：\n/** * 路由器实现两个映射器和处理其两个接口 * @param \u0026lt;T\u0026gt; * @param \u0026lt;D\u0026gt; * @param \u0026lt;R\u0026gt; */ public abstract class AbstractStrategyRouter\u0026lt;T, D, R\u0026gt; implements StrategyHandler\u0026lt;T, D, R\u0026gt;, StrategyMapper\u0026lt;T, D, R\u0026gt; { @Getter @Setter protected StrategyHandler\u0026lt;T, D, R\u0026gt; defaultHandler = StrategyHandler.DEFAULT; public R router(T requestParam, D dynamicContext) throws Exception { StrategyHandler\u0026lt;T, D, R\u0026gt; handler = get(requestParam, dynamicContext); if (null != handler) { return handler.apply(requestParam, dynamicContext); } return defaultHandler.apply(requestParam, dynamicContext); } } AbstractStrategyRouter实现了映射器和处理器两个接口，这样就能够在本类中实现的 router方法内使用映射器和处理器中的 get方法和 apply方法。这里较为巧妙的是使用了抽象类来继承，这样既可以有自己的实现方法，也不需要在当前这个类中实现 get和 apply方法。\n场景 现在将上述的处理器、映射器、路由器结合到拼团的实际场景中来。先创建销售商品信息以及试算结果实体，如下。\n当然，这里只是为了在整个流程的实现过程中不会报错，实际上可能用不上这几个类。\n/** * 销售商品信息，也就是业务节点的入参 */ @Data @Builder @NoArgsConstructor @AllArgsConstructor public class MarketProductEntity { /** 活动ID */ private Long activityId; /** 用户ID */ private String userId; /** 商品ID */ private String goodsId; } /** * 试算结果实体类，也就是业务节点的返参 */ @Data @Builder @NoArgsConstructor @AllArgsConstructor public class TrialBalanceEntity { /** 商品ID */ private String goodsId; /** 商品名称 */ private String goodsName; /** 原始价格 */ private BigDecimal originalPrice; /** 折扣价格 */ private BigDecimal deductionPrice; } 接下来创建功能服务支撑类，以便之后的业务流程节点继承。这里继续使用抽象类即可，因为方法的最终实现还是要交给业务流程节点来实现，不必在服务支撑类中实现，同时，到了服务支撑类这一层，就可以将之前准备的入参和返回参数替代接口中的泛型了。到这里还存在一些问题，之前提到的上下文要怎么存放呢？最开始的节点要怎么进入并使用？这些可以通过创建另一个类来同时管理，这里命名为 DefaultActivityStrategyFactory，于是，服务支撑类的代码以及 DefaultActivityStrategyFactory代码如下：\n/** * 服务支撑类，继承原来的策略路由器 * @param \u0026lt;MarketProductEntity\u0026gt; 入参 * @param \u0026lt;DynamicContext\u0026gt; 动态上下文 * @param \u0026lt;TrialBalanceEntity\u0026gt; 返参 */ public abstract class AbstractGroupMarketSupport\u0026lt;MarketProductEntity, DynamicContext, TrialBalanceEntity\u0026gt; extends AbstractStrategyRouter\u0026lt;MarketProductEntity, DynamicContext, TrialBalanceEntity\u0026gt; { } /** * 一个工厂类，用来管理初始流程节点和动态上下文 */ @Service public class DefaultActivityStrategyFactory { /** 业务流程中的起始节点 */ private final RootNode rootNode; public DefaultActivityStrategyFactory(RootNode rootNode) { this.rootNode = rootNode; } public StrategyHandler\u0026lt;MarketProductEntity, DynamicContext, TrialBalanceEntity\u0026gt; strategyHandler() { return rootNode; } @Data @Builder @NoArgsConstructor @AllArgsConstructor public static class DynamicContext { /** 保存上下文信息 */ } } 如上面的代码，服务支撑类继承了原先的策略路由器，使得其同时拥有了映射器和处理器的方法，而使用一个工厂类来管理动态上下文和初始节点， 随着业务的不断扩展，流程可能需要更多的信息，这时候就可以往动态上下文中添加信息来满足流程的执行。\n最后就是创建每个节点，节点的创建方式是一样的，唯一有区别的可能就是在业务逻辑上。四个节点分别如下：\nRootNode\n@Slf4j @Service public class RootNode extends AbstractGroupMarketSupport\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; { @Resource private SwitchNode switchNode; @Override public TrialBalanceEntity apply(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { log.info(\u0026#34;==== RootNode 开始执行 ====\u0026#34;); dynamicContext.setInfo(\u0026#34;### 这里是业务流程要使用的信息\u0026#34;); log.info(\u0026#34;==== 在RootNode中存入上下文信息 信息为:{}\u0026#34;, dynamicContext.getInfo()); return router(requestParam, dynamicContext); } /** * 获得起始节点的下一个节点 * @param requestParam 入参 * @param dynamicContext 上下文 * @return 返参 * @throws Exception */ @Override public StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; get(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { return switchNode; } } SwitchNode\n@Slf4j @Service public class SwitchNode extends AbstractGroupMarketSupport\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; { @Resource private MarketNode marketNode; @Override public TrialBalanceEntity apply(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { log.info(\u0026#34;==== SwitchNode 开始执行 ====\u0026#34;); return router(requestParam, dynamicContext); } @Override public StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; get(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { return marketNode; } } MarketNode\n@Slf4j @Service public class MarketNode extends AbstractGroupMarketSupport\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; { @Resource private EndNode endNode; @Override public TrialBalanceEntity apply(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { log.info(\u0026#34;==== MarketNode 开始执行 ====\u0026#34;); // 可以在这里执行查询商品和活动信息并试算，同时利用上下文信息 log.info(\u0026#34;==== 取出上下文信息，信息为:{}\u0026#34;, dynamicContext.getInfo()); return router(requestParam, dynamicContext); } @Override public StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; get(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { return endNode; } } EndNode\n@Slf4j @Service public class EndNode extends AbstractGroupMarketSupport\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; { @Override public TrialBalanceEntity apply(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { log.info(\u0026#34;==== EndNode 开始执行 ====\u0026#34;); /* 此处就可以返回试算结果了 */ return new TrialBalanceEntity(); } @Override public StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; get(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { return defaultHandler; } } 最后，用单元测试来验证一下执行的顺序，单元测试类如下：\n@Slf4j @RunWith(SpringRunner.class) @SpringBootTest public class IndexTest { @Resource private DefaultActivityStrategyFactory defaultActivityStrategyFactory; @Test public void test() throws Exception { StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; handler = defaultActivityStrategyFactory.strategyHandler(); TrialBalanceEntity trialBalance = handler.apply(new MarketProductEntity(), new DefaultActivityStrategyFactory.DynamicContext()); } } 得到的执行结果为：\n可以看到，每个节点按照既定的顺序执行，并可以在流程中使用上下文信息。\n优化 通过上述对链式多分支规则树模型的讲述，已经有了较为详细的了解。下面，针对实际可能出现的业务场景，我们引入线程池来优化程序的执行时间。\n背景 在规则树模型结构中，如果要进行商品的试算，至少需要查询商品信息和活动信息才有可能计算商品的折扣。如果这个步骤放在主线程中执行，势必会拖慢主线程的执行。如果查询的数据很多，那么这个代价肯定是不可接受的。所以，这时候可以考虑引入线程池，将必要的查询交由线程池中的线程实现，尽可能的减少对主线程的影响。\n线程池创建 创建线程池的方式有很多种，这里也是准备了一种较为优雅的方式。首先在yml配置文件中先配置需要用到的配置信息。\n# 线程池配置 thread: pool: executor: config: core-pool-size: 20 max-pool-size: 50 keep-alive-time: 5000 block-queue-size: 5000 policy: CallerRunsPolicy 接下来我们创建一个线程池配置属性类（ ThreadPoolConfigProperties ）来接收yml文件中的配置信息。\n@Data @ConfigurationProperties(prefix = \u0026#34;thread.pool.executor.config\u0026#34;, ignoreInvalidFields = true) public class ThreadPoolConfigProperties { /** 核心线程数 */ private Integer corePoolSize; /** 最大线程数 */ private Integer maxPoolSize; /** 最大等待时间 */ private Long keepAliveTime; /** 最大队列数 */ private Integer blockQueueSize; /* * AbortPolicy：丢弃任务并抛出RejectedExecutionException异常。 * DiscardPolicy：直接丢弃任务，但是不会抛出异常 * DiscardOldestPolicy：将最早进入队列的任务删除，之后再尝试加入队列的任务被拒绝 * CallerRunsPolicy：如果任务添加线程池失败，那么主线程自己执行该任务 * */ private String policy; } 这里需要使用注解 @ConfigurationPorperties来指定在yml文件中的配置信息， prefix也就是yml配置中的前缀信息。最后就是创建真正的线程池配置类（ ThreadPoolConfig )。\n/** * 线程池配置信息 * @EnableAsync 这个注解允许方法在后台线程中异步执行，而不是阻塞调用线程 */ @Slf4j @EnableAsync @Configuration @EnableConfigurationProperties(ThreadPoolConfigProperties.class) public class ThreadPoolConfig { @Bean @ConditionalOnMissingBean(ThreadPoolExecutor.class) // 这个注解用来保证线程池的唯一性，当不存在线程池时才会创建 public ThreadPoolExecutor threadPoolExecutor(ThreadPoolConfigProperties properties) { // 实例化策略，用于之后实际创建用 RejectedExecutionHandler handler; switch (properties.getPolicy()) { case \u0026#34;AbortPolicy\u0026#34;: handler = new ThreadPoolExecutor.AbortPolicy(); break; case \u0026#34;DiscardPolicy\u0026#34;: handler = new ThreadPoolExecutor.DiscardPolicy(); break; case \u0026#34;DiscardOldestPolicy\u0026#34;: handler = new ThreadPoolExecutor.DiscardOldestPolicy(); break; case \u0026#34;CallerRunsPolicy\u0026#34;: handler = new ThreadPoolExecutor.CallerRunsPolicy(); break; default: handler = new ThreadPoolExecutor.AbortPolicy(); break; } return new ThreadPoolExecutor(properties.getCorePoolSize(), properties.getMaxPoolSize(), properties.getKeepAliveTime(), TimeUnit.SECONDS, new LinkedBlockingQueue\u0026lt;\u0026gt;(properties.getBlockQueueSize()), Executors.defaultThreadFactory(), handler); } } 其中一些比较关键的信息通过注释呈现。\n改造链式多分支规则树模型 回想一下，每个流程节点其实都是继承自 AbstractGroupMarketSupport 类，而这个类又是继承自 AbstractStrategyRouter 这个策略路由器。所以从最上层开始，引入需要的线程池方法。\n我们重新创建一个类 AbstractMutiThreadStrategyRouter 代替原先的 AbstractStrategyRouter 。\npublic abstract class AbstractMutiThreadStrategyRouter\u0026lt;T, D, R\u0026gt; implements StrategyMapper\u0026lt;T, D, R\u0026gt;, StrategyHandler\u0026lt;T, D, R\u0026gt; { @Getter @Setter protected StrategyHandler\u0026lt;T, D, R\u0026gt; defaultStrategyHandler = StrategyHandler.DEFAULT; public R router(T requestParam, D dynamicContext) throws Exception { StrategyHandler\u0026lt;T, D, R\u0026gt; strategyHandler = get(requestParam, dynamicContext); if (null != strategyHandler) { return strategyHandler.apply(requestParam, dynamicContext); } return defaultStrategyHandler.apply(requestParam, dynamicContext); } @Override public R apply(T requestParam, D dynamicContext) throws Exception { mutiThread(requestParam, dynamicContext); return doApply(requestParam, dynamicContext); } protected abstract R doApply(T requestParam, D dynamicContext) throws Exception; protected abstract void mutiThread(T requestParam, D dynamicContext) throws Exception; } 与原先不同的是，在这个策略路由器中，对 apply 方法进行了简单的实现，在内部执行两个方法：\nmutiThread 方法，也就是线程池查找的相应方法 doApply 方法，真正的业务逻辑实现 值得注意的是，这两个方法依旧为抽象方法，具体的实现交由子类负责。\n在服务支持类上没有过多的改动，但需要实现缺省的 mutiThread 方法，这样可以避免之后的每个业务流程节点都要实现这一方法。\npublic abstract class AbstractGroupBuyMarketSupport\u0026lt;MarketProductEntity, DynamicContext, TrialBalanceEntity\u0026gt; extends AbstractMutiThreadStrategyRouter\u0026lt;MarketProductEntity, DynamicContext, TrialBalanceEntity\u0026gt; { /** 在这里注入查询接口，当流程节点使用时可以不用再次注入 */ @Resource protected IActivityRepository repository; @Override protected void mutiThread(MarketProductEntity requestParam, DynamicContext dynamicContext) throws Exception { // 缺省的方法，避免每个子类都要实现 } } 最后就是业务流程节点的实现，每个业务流程节点只需要实现自己对应的业务逻辑，也就是 doApply 方法，在需要查询的时候额外实现 mutiThread 方法即可，这里使用MarketNode作为例子。假如我们要在MarketNode执行时获得商品信息，可以这么做。\n首先需要创建交由线程池执行的线程，再因为查询需要返回结果，所以使用 FutureTask的方式来创建线程。如下：\npublic class QuerySkuVOThreadTask implements Callable\u0026lt;SkuVO\u0026gt; { /** 商品ID */ private final String goodsId; /** 查询接口 */ private final IActivityRepository repository; public QuerySkuVOThreadTask(String goodsId, IActivityRepository repository) { this.goodsId = goodsId; this.repository = repository; } @Override public SkuVO call() throws Exception { return repository.querySkuVO(goodsId); } } 最后只需要在MarketNode节点中实现 mutiThread 方法，并将查询任务交给线程池执行即可。\n@Slf4j @Service public class MarketNode extends AbstractGroupBuyMarketSupport \u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; { @Resource private ThreadPoolExecutor threadPoolExecutor; @Resource private EndNode endNode; /** * 实现异步数据加载，并存储到动态上下文中 * @param requestParam * @param dynamicContext * @throws Exception */ @Override protected void mutiThread(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { // 通过线程池去查询商品信息 QuerySkuVOFromDBThreadTask querySkuVOFromDBThreadTask = new QuerySkuVOFromDBThreadTask(requestParam.getGoodsId(), repository); FutureTask\u0026lt;SkuVO\u0026gt; skuVOFutureTask = new FutureTask\u0026lt;\u0026gt;(querySkuVOFromDBThreadTask); threadPoolExecutor.execute(skuVOFutureTask); } @Override public TrialBalanceEntity doApply(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) throws Exception { return router(requestParam, dynamicContext); } @Override public StrategyHandler\u0026lt;MarketProductEntity, DefaultActivityStrategyFactory.DynamicContext, TrialBalanceEntity\u0026gt; get(MarketProductEntity requestParam, DefaultActivityStrategyFactory.DynamicContext dynamicContext) { return endNode; } } 到这里，也就完成了链式多分支规则树模型的线程池引入。\n","date":"2025-09-26T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E9%93%BE%E5%BC%8F%E5%A4%9A%E5%88%86%E6%94%AF%E8%A7%84%E5%88%99%E6%A0%91%E6%A8%A1%E5%9E%8B/","title":"链式多分支规则树模型"},{"content":"前言 通常情况下，我们不会手动停止一个线程，而是允许线程运行到结束，然后让它自然停止。但是依然会有许多特殊的情况需要我们提前停止线程，比如：用户突然关闭程序，或程序运行出错重启等。在这种情况下，即将停止的线程在很多业务场景下仍然很有价值。尤其是我们想写一个健壮性很好，能够安全应对各种场景的程序时，正确停止线程就显得格外重要。\n为什么不强制停止？ 对于Java来说，最正确的停止线程的方式就是使用 interrupt。但是 interrupt仅仅是起到通知被停止线程的作用。对于被停止的线程来说，它拥有完全的自主权， 既可以选择立即停止，也可以选择过一段时间后再停止，甚至压根不停止。 在Java中，程序通过互相通知、相互协作来管理线程，如果不了解线程的工作贸然停止可能会造成一些安全问题。就比如线程正在写入一个文件，但是收到了终止信号，这时候线程就要根据自身业务情况决定是立即停止还是写完文件再停止，如果选择立即停止可能会造成数据的不完整。\ninterrupt 如何使用interrupt终止线程？ 一般来说，被停止线程总是需要不断的判断线程是否收到了终止信号，常见的循环如下：\nwhile (Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; do sth.) { // do sth. of thread; } 接下来用代码来看看Java是怎么实现停止线程的逻辑的。一旦调用某个线程的interrupt方法之后，这个线程的中断标志位就会被置为true。 每个线程都有这样的标志位，当线程执行时应该定期检查这个标志位，如果标志位被置为true，说明有程序想要终止该线程。 示例代码如下：\npublic class StopThread implements Runnable { @Override public void run() { int cnt = 0; while (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; cnt \u0026lt;= 1000) { System.out.println(cnt++); } } public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(new StopThread()); thread.start(); Thread.sleep(2); thread.interrupt(); } } 此时会得到如下输出：\n在上面的代码中，StopThread类的run()方法首先判断进程是否被中断，之后再判断cnt的值是否小于等于1000。线程的主要工作就是打印0~1000的数字。从最后的输出结果中也可以看到，线程并未完整的打印所有数字，这是因为在主线程中创建了子线程来执行这个任务，之后主线程休眠了2毫秒就使用interrupt方法对子线程打断，子线程的中断标志被置为true，同时在进入循环判断时就会因为标志位的改变而中断。\n当然，主线程不能休眠太久，否则等子线程执行完任务之后再打断就没有任何意义了。\nsleep期间能否感受到中断？ 我们知道Java线程在执行阻塞式IO操作、调用sleep函数等一些情况下会进入休眠状态。那么，如果是在sleep期间，线程是否能够感受到中断呢？所以，我们改写一下之前的代码：\npublic class StopDuringSleep { public static void main(String[] args) throws InterruptedException { Runnable run = () -\u0026gt; { int cnt = 0; while (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; cnt \u0026lt;= 1000) { System.out.println(\u0026#34;current cnt: \u0026#34; + cnt); cnt++; try { Thread.sleep(10000); } catch (InterruptedException e) { e.printStackTrace(); } } }; Thread thread = new Thread(run); thread.start(); Thread.sleep(5); thread.interrupt(); } } 在上面这段代码中，我们在循环中加入子线程的休眠，模拟线程在工作时进入休眠状态，主线程依旧在5毫秒后中断子线程。此时得到的输出结果如下：\n可以看到，处于休眠状态的线程被中断是 **可以感受到中断信号的。**同时会抛出一个 InterruptedException 异常，但是此时子线程并没有像之前那样结束运行，而是继续打印接下来的数。这是因为在抛出异常之后， 子线程会清除中断标志位，也就是此时中断标志位为false，自然再进入下一次循环判断时就不会中断。\n这里有两点需要注意，在一开始写代码的时候，将catch代码块放在了循环外面，导致一直无法复现出想要的结果。因为在捕获异常之后循环不会再次检查，这时候自然也就会终止所有程序。\n另外一点是，如果你照着上面的代码写，IDEA就会建议你不要在循环中加入sleep函数，这会导致忙等。在看了相关资料后更重要的原因应该是sleep函数并不会释放锁，因此造成死锁的可能性就上升了。（当然，应当是有其他原因的，可能得之后遇到相关场景之后才会有更深刻的理解了）。\n合理的实现中断 从之前代码了解到，子线程是因为接收到中断信号后抛出异常，而异常处理后会清除中断信号，导致进入循环时中断标志位为false无法中断。了解了原理之后，自然可以从根源上来解决这个问题。\n方法签名中抛出异常 可以考虑在方法中抛出异常，并进行捕获。因为 run() 本身是不能抛出异常的，只能通过try/catch来处理异常，如果在方法中抛出异常，那么就可以将中断信号层层传递到顶层，最终让 run() 方法可以捕获异常进行处理，在这种处理方式中可以考虑将 run() 方法中的逻辑抽象为另一方法，并抛出异常。代码如下所示：\npublic class StopDuringSleep { public static void main(String[] args) throws InterruptedException { Thread thread = new Thread(new InterruptTask()); thread.start(); Thread.sleep(500); thread.interrupt(); } static class InterruptTask implements Runnable { @Override public void run() { try { process(); } catch (InterruptedException e) { e.printStackTrace(); System.out.println(\u0026#34;task is interrupted\u0026#34;); } } private void process() throws InterruptedException { int cnt = 0; while (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; cnt \u0026lt;= 1000) { System.out.println(\u0026#34;current cnt:\u0026#34; + cnt); cnt++; Thread.sleep(10000); } } } } 再次中断 之前没能实现中断是因为在捕获异常之后没有进行任何处理，子线程又清除了中断标志，所以就有一种较为暴力的方式，只要在捕获异常之后，在catch方法块中将中断标志再次置为true，待子线程再次进入循环之后便能够收到中断请求，线程也就可以正常退出了。示例代码如下：\npublic class StopDuringSleep { public static void main(String[] args) throws InterruptedException { Runnable run = () -\u0026gt; { int cnt = 0; while (!Thread.currentThread().isInterrupted() \u0026amp;\u0026amp; cnt \u0026lt;= 1000) { System.out.println(\u0026#34;current cnt: \u0026#34; + cnt); cnt++; try { Thread.sleep(10000); } catch (InterruptedException e) { Thread.currentThread().interrupt(); e.printStackTrace(); } } }; Thread thread = new Thread(run); thread.start(); Thread.sleep(5); thread.interrupt(); } } 在实际生产环境中，不能盲目的屏蔽中断请求，否则可能导致线程无法正确停止。\nvolatile 我们知道volatile变量 **能够保证变量修改在多线程中仍保持可见性，强制线程去主存中获取这个变量，**所以，在一些代码中，也能见到使用volatile变量来实现线程停止的案例。如下面的代码所示，就是使用volatile代码来实现线程停止的案例。\npublic class VolatileStopThread implements Runnable { private volatile boolean cancenled = false; @Override public void run() { int cnt = 0; while (!cancenled \u0026amp;\u0026amp; cnt \u0026lt;= 1000) { System.out.println(\u0026#34;current cnt \u0026#34; + cnt); cnt++; try { Thread.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) throws InterruptedException { VolatileStopThread run = new VolatileStopThread(); Thread thread = new Thread(run); thread.start(); Thread.sleep(5); run.cancenled = true; } } 代码声明了一个叫作 VolatileStopThread 的类， 它实现了 Runnable 接口，然后在 run() 中进行 while 循环。之后启动主线程，主线程经过5毫秒之后将volatile修饰的变量更改为true。这样不再满足while条件，因此子线程会退出循环。当然，这是volatile停止线程方法适用的情况，如果这种方法是正确的，那么应该在其他情况下也适用才是合理的。接下来就来看看这种方法不适用的场景。代码示例如下：\npublic class Producer implements Runnable { private BlockingQueue\u0026lt;Integer\u0026gt; storage; public volatile boolean canceled = false; public Producer(BlockingQueue\u0026lt;Integer\u0026gt; storage) { this.storage = storage; } @Override public void run() { int num = 0; try { while (!canceled \u0026amp;\u0026amp; num \u0026lt;= 100000) { if (num % 50 == 0) { storage.put(num); System.out.println(\u0026#34;put num into storage\u0026#34;); } ++num; } } catch (InterruptedException e) { e.printStackTrace(); } finally { System.out.println(\u0026#34;producer is stopped\u0026#34;); } } } public class Consumer { BlockingQueue\u0026lt;Integer\u0026gt; storage; public Consumer(BlockingQueue\u0026lt;Integer\u0026gt; storage) { this.storage = storage; } public boolean needMoreNum() { if (Math.random() \u0026gt; 0.97) { return false; } return true; } } public class Main { public static void main(String[] args) throws InterruptedException { ArrayBlockingQueue\u0026lt;Integer\u0026gt; storage = new ArrayBlockingQueue\u0026lt;\u0026gt;(8); Producer producer = new Producer(storage); Thread producerThread = new Thread(producer); producerThread.start(); Thread.sleep(500); Consumer consumer = new Consumer(storage); while (consumer.needMoreNum()) { System.out.println(consumer.storage.take() + \u0026#34;is consumed\u0026#34;); Thread.sleep(100); } System.out.println(\u0026#34;consumer don\u0026#39;t need numbers\u0026#34;); producer.canceled = true; System.out.println(producer.canceled); } } 这是一个消费者生产者模型，声明了一个生产者 Producer，通过 volatile 标记的初始值为 false 的布尔值 canceled 来停止线程。而在 run() 方法中，while 的判断语句是 num 是否小于等于 100000 及 canceled 是否被标记。while 循环体中判断 num 如果是 50 的倍数就放到 storage 仓库中，storage 是生产者与消费者之间进行通信的存储器，当 num 大于 100000 或被通知停止时，会跳出 while 循环并执行 finally 语句块，告诉大家“生产者结束运行”。\n而对于消费者 Consumer，它与生产者共用同一个仓库 storage，并且在方法内通过 needMoreNum() 方法判断是否需要继续使用更多的数字，刚才生产者生产了一些 50 的倍数供消费者使用，消费者是否继续使用数字的判断条件是产生一个随机数并与 0.97 进行比较，大于 0.97 就不再继续使用数字。\n在main 函数中，首先创建了生产者/消费者共用的仓库 storage，仓库容量是 8，并且建立生产者并将生产者放入线程后启动线程，启动后进行 500 毫秒的休眠，休眠时间保障生产者有足够的时间把仓库塞满，而仓库达到容量后就不会再继续往里塞，这时生产者会阻塞，500 毫秒后消费者也被创建出来，并判断是否需要使用更多的数字，然后每次消费后休眠 100 毫秒\n最后我们来看最终的输出结果，如下图所示：\n很明显，生产者并没有像预料的那样终止生产，反而是阻塞住了。这是因为 使用了阻塞队列，消费者的在一开始的消费速度远低于生产者的生产速度，因此阻塞队列中始终是满的状态，而 put() 方法会阻塞生产者线程，在阻塞状态中的线程被唤醒之前无法进行下一次循环的判断，所以这种情况下是无法通过volatile标志变量来终止线程。\n","date":"2025-07-20T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%81%9C%E6%AD%A2%E7%BA%BF%E7%A8%8B/","title":"如何正确地停止线程"},{"content":"前言 数据库锁的设计初衷是处理并发问题。根据加锁的范围，MySQL里的锁大致可以分为 全局锁、 表级锁 和 行级锁 三类。\n全局锁 全局锁就是对整个数据库实例加锁。MySQL 提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)。当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。\n如果使用全局锁对整个库进行备份（期间整个库都处于只读状态），会有以下的问题：\n如果你在主库上备份，那么在备份期间都不能执行更新，业务基本上就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的 binlog，会导致主从延迟。 如果在备份时不加锁，容易出现并发问题。这里用网站给出的例子稍微解释：\n假设要维护用户账户余额和课程表，现在发起一个逻辑备份。假设备份期间，有一个用户，他购买了一门课程，业务逻辑里就要扣掉他的余额，然后往已购课程里面加上一门课。如果时间顺序上是先备份账户余额表 (u_account)，然后用户购买，然后备份用户课程表 (u_course)，流程示意图如下：\n从图中可以看出，用户余额没有扣减，但是却多出了一门课，也就是用户赚了。同样，如果先备份用户课程表再备份用户账户表，就会发现用户亏了。\n官方自带的逻辑备份工具是 mysqldump。当 mysqldump 使用参数–single-transaction 的时候，导数据之前就会启动一个事务，来确保拿到一致性视图。也就是在可重复读的隔离级别下开启一个事务。但是这种方法只适用于所有表使用事务引擎的库。\n这里还有一个问题，既然要全库只读，为什么不使用 set global readonly=true 的方式呢？，不使用这个方法的原因有两点：\n在有些系统中，readonly 的值会被用来做其他逻辑，比如用来判断一个库是主库还是备库。因此，修改 global 变量的方式影响面更大，我不建议你使用。 在异常处理机制上有差异。如果执行 FTWRL 命令之后由于客户端发生异常断开，那么 MySQL 会自动释放这个全局锁，整个库回到可以正常更新的状态。而将整个库设置为 readonly 之后，如果客户端发生异常，则数据库就会一直保持 readonly 状态，这样会导致整个库长时间处于不可写状态，风险较高。 表级锁 MySQL里的表级锁有两种： 表锁 和 元数据锁（meta data lock，MDL）\n表锁 **表锁的语法是 lock tables [table_name] read/write ** 。与 FTWRL 类似，可以用 unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。 需要注意，lock tables 语法除了会限制别的线程的读写外，也限定了本线程接下来的操作对象。\n如果在某个线程 A 中执行 lock tables t1 read, t2 write; 这个语句，则其他线程写 t1、读写 t2 的语句都会被阻塞。同时，线程 A 在执行 unlock tables 之前，也只能执行读 t1、读写 t2 的操作。连写 t1 都不允许，自然也不能访问其他表。\n但对于InnoDB这种支持行锁的引擎来说，一般不会选择使用lock tables来控制并发，毕竟锁住整个表对性能的影响还是比较大的。\nMDL MDL 不需要显式使用，在访问一个表的时候会被自动加上。MDL 的作用是，保证读写的正确性。你可以想象一下，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个表结构做变更，删了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。\n当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。\nMDL锁是基于表元数据（表结构）的锁，MDL锁是为了保证并发环境下元数据和表数据的结构一致性。这也就可以理解为什么对表进行增删改查时只需要加MDL读锁。\n读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 MDL 锁是系统默认会加的，但却是你不能忽略的一个机制。如下面这个例子：\n可以看到 session A 先启动，这时候会对表 t 加一个 MDL 读锁。由于 session B 需要的也是 MDL 读锁，因此可以正常执行。之后 session C 会被 blocked，是因为 session A 的 MDL 读锁还没有释放，而 session C 需要 MDL 写锁，因此只能被阻塞。如果只有 session C 自己被阻塞还没什么关系，但是之后所有要在表 t 上新申请 MDL 读锁的请求也会被 session C 阻塞。 所有对表的增删改查操作都需要先申请 MDL 读锁，就都被锁住，等于这个表现在完全不可读写了。\n事务中的 MDL 锁，在语句执行开始时申请，但是语句结束后并不会马上释放，而会等到整个事务提交后再释放。\n如何安全的给小表加字段？ 首先我们要解决长事务，事务不提交，就会一直占着 MDL 锁。在 MySQL 的 information_schema 库的 innodb_trx 表中，你可以查到当前执行中的事务。如果你要做 DDL 变更的表刚好有长事务在执行，要考虑先暂停 DDL，或者 kill 掉这个长事务。\n如果你要变更的表是一个热点表，虽然数据量不大，但是上面的请求很频繁，而你不得不加个字段，你该怎么做呢？\n这时候 kill 可能未必管用，因为新的请求马上就来了。比较理想的机制是，在 alter table 语句里面设定等待时间，如果在这个指定的等待时间里面能够拿到 MDL 写锁最好，拿不到也不要阻塞后面的业务语句，先放弃。之后开发人员或者 DBA 再通过重试命令重复这个过程。\n这里已经涉及到了不太了解的地方，也只是将源站内容搬运了过来。待日后技术精进了再详细研究这一部分的内容吧。\n行锁 行锁就是针对数据表中行记录的锁。如果事务A正在更新一行，事务B要更新同一行，就需要等待事务A结束操作后才能进行更新。\n两阶段锁协议 假定有如下两个事务执行：\n此时事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。在这个例子中，事务A持有两条记录的行锁，且在commit之后才释放。\n在 InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 根据这个协议，如果事务中需要锁多个行， 那就要把最可能造成锁冲突，最可能影响并发度的锁尽量往后放。\n这里使用源站的例子，我觉得说得很简练。假设负责实现一个电影票在线交易业务，顾客 A 要在影院 B 购买电影票。这个业务需要涉及到以下操作：\n从顾客 A 账户余额中扣除电影票价； 给影院 B 的账户余额增加这张电影票价； 记录一条交易日志。 要完成这个交易，需要 update 两条记录，并 insert 一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，要怎样安排这三个语句在事务中的顺序呢？\n试想如果同时有另外一个顾客 C 要在影院 B 买票，那么这两个事务冲突的部分就是语句 2 了。因为它们要更新同一个影院账户的余额，需要修改同一行数据。\n根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句 2 安排在最后，比如按照 3、1、2 这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。\n死锁和死锁检测 当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁。用行锁举例如下：\n这时候事务A和B都在等待对方释放行锁，也就进入了死锁状态。出现死锁后有两种策略：\n一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 innodb_deadlock_detect 设置为 on，表示开启这个逻辑。 由于第一种方式的超时时间较难把控，所以一般采用第二种方式。但实际上死锁的检测需要耗费大量的CPU，如果多个线程要更新同一行，由于是一个时间复杂度O(n)的操作，综合量级将非常大。怎么解决这种热点行更新导致的性能问题呢？\n如果能确定这个业务一定不会出现死锁，可以临时把死锁检测关掉。 但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关掉死锁检测意味着可能会出现大量的超时，这是业务有损的。\n控制并发度 。如果并发能够控制住，比如同一行同时最多只有 10 个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。（可以考虑在客户端控制并发度，但实际上如果客户端一多，限制单个客户端的并发度汇总到数据库上还是会有很大的压力）。 因此，这个并发控制要在数据库服务端实现，基本思路就是对于相同行的更新，在进入InnoDB引擎之前进行排队，这样就不会有大量的死锁检测工作。\n当然也可以考虑从设计上来解决这个问题。使用上述电影票的例子，可以将数据库中的一行改为逻辑上的多行，如影院的账户可以是数据库多行的和，这样在修改影院账户记录的时候，就可以从中随机选择一行。不过这种方式可能需要在其他业务逻辑上做出特殊处理。\n参考 MySQL45讲——06 全局锁和表锁 ：给表加个字段怎么有这么多阻碍？ MySQL45讲——07 行锁功过：怎么减少行锁对性能的影响？ ","date":"2025-07-04T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/mysql%E5%AD%A6%E4%B9%A0%E4%BA%94-%E9%94%81/","title":"MySQL学习（五）| 锁"},{"content":"前言 索引是数据库中重要的概念，简单来说，索引就像是书的目录一样，其目的就是为了提高数据查询效率。\n索引的常见模型 索引的出现是为了提高查找效率，但是实现索引的方式有很多种，这里介绍三种常见的索引结构。\n哈希表 哈希表是一种以键 - 值（key-value）存储数据的结构，我们只要输入待查找的值即 key，就可以找到其对应的值即 value。哈希的思路很简单，把值放在数组里，用一个哈希函数把 key 换算成一个确定的位置，然后把 value 放在数组的这个位置。不可避免地，多个 key 值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表。\n和Java类似，当链表过长时会影响查询效率，Java中的HashMap首先是会进行数组的扩容，在一定阈值后将拉链表转换为红黑树。\n假设现在维护一个身份证信息和姓名的表，使用哈希索引的示意图如下：\n假设，这时候你要查 ID_card_n2 对应的名字是什么，处理步骤就是：首先，将 ID_card_n2 通过哈希函数算出 N；然后，按顺序遍历，找到 User2。 需要注意的是，图中四个 ID_card_n 的值并不是递增的，这样做的好处是增加新的 User 时速度会很快，只需要往后追加。但缺点是，因为不是有序的，所以哈希索引做区间查询的速度是很慢的。\n所以，哈希表这种结构适用于只有等值查询的场景，而有序数组在等值查询和范围查询的性能都较好。\n有序数组 依旧使用上面的例子，如果使用有序数组作为索引结构，示意图如下：\n这时候如果你要查 ID_card_n2 对应的名字，用二分法就可以快速得到，这个时间复杂度是 O(log(N))。这种索引结构同样支持范围查询，如果要查身份证号在 [ID_card_X, ID_card_Y] 区间的 User，可以先用二分法找到 ID_card_X（如果不存在 ID_card_X，就找到大于 ID_card_X 的第一个 User），然后向右遍历，直到查到第一个大于 ID_card_Y 的身份证号，退出循环。\n如果单纯看查询效率，有序数组是一个不错的存储结构，但当更新数据时，需要移动插入位置后的所有元素，成本较大。因此，有序数据索引结构只适合静态存储引擎\n二叉搜搜索树 二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子。这样如果你要查 ID_card_n2 的话，按照图中的搜索顺序就是按照 UserA -\u0026gt; UserC -\u0026gt; UserF -\u0026gt; User2 这个路径得到。这个时间复杂度是 O(log(N))。\n为了维持O(log(N))的查询复杂度，需要保持搜索树为平衡二叉树，因此更新元素的时间复杂度也为O(log(N))。\n更多情况下并不使用二叉搜索树作为索引。数据库索引不仅存在内存中，还要存储到磁盘里，不可避免的会涉及到读盘操作。二叉搜索树随着节点的增多，树高将不断增长，进行一次查询需要读取的数据块较多，就会导致查询效率降低。\nInnoDB的索引模型 在 InnoDB 中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表。InnoDB 使用了 B+ 树索引模型，所以数据都是存储在 B+ 树中的。每一个索引在InnoDB里面对应一棵B+树。\n假设有这么一个表，主键列为ID，表中有字段k，并且在k上有索引。建表语句如下：\ncreate table T ( id int primary key, k int not null, name varchar(16), index(k)), engine = InnoBD; 表中 R1~R5 的 (ID,k) 值分别为 (100,1)、(200,2)、(300,3)、(500,5) 和 (600,6)，两棵树的示例示意图如下：\nInnoDB索引组织结构 根据叶子节点的内容不同，索引类型分为 主键索引 和 非主键索引 。\n主键索引的叶子节点存储的是整行数据，在InnoDB里，主键索引也称为 聚簇索引 。 非主键索引的叶子节点存储的内容是主键的值，在InnoDB里，非主键索引也称为 二级索引 。 也因为这种存储结构，基于主键索引的查询和普通索引存在区别：\n如果查询语句是 select * from T where ID = 500，也就是主键查询的方式，那么只需要搜索ID这棵B+树。 如果查询语句是 select * from T where k = 5，也就是普通索引的方式，那么需要先搜索字段k这棵索引树，得到对应的主键ID值为500，再利用ID搜索ID索引树，这个过程也成为回表。 也就是说， 基于非主键索引的查询需要多扫描一棵索引树，所以如果可以的话尽量使用主键索引。\n索引维护 数据页 在讲述索引维护之前，先简单了解一下MySQL数据页的概念。 页（Pages）是 InnoDB 中管理数据的最小单元。Buffer Pool中存的就是一页一页的数据。再比如，当我们要查询的数据不在 Buffer Pool 中时，InnoDB 会将记录所在的页整个加载到 Buffer Pool 中去；同样的，将 Buffer Pool 中的脏页刷入磁盘时，也是按照页为单位刷入磁盘的。\n存储MySQL中的数据最后都是存储在页中的，在InnoDB的设计中，页与页之间是通过一个 双向链表连接起来的。如下图所示：\n而存储在页中的一行一行记录则是通过单链表连接起来的。如下图所示：\n维护 B+ 树为了维护索引有序性，在插入新值的时候需要做必要的维护。使用上面的例子，如果插入新的行 ID 值为 700，则只需要在 R5 的记录后面插入一个新记录。如果新插入的 ID 值为 400需要逻辑上挪动后面的数据，空出位置。 更糟的情况是，如果 R5 所在的数据页已经满了，根据 B+ 树的算法，这时候需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂。除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中，整体空间利用率会下降。有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。\n自增主键or其他字段 在很多建表规范中，要求建表语句中定义自增主键。也就是 NOT NULL PRIMARY KEY AUTO_INCREMENT。这时候插入新数据时，可以不指定ID的值，系统会获取当前ID最大值加1作为下一条记录的ID值。为什么使用自增主键可以从性能和存储两个方面考虑。\n使用自增主键，每次插入一条新记录，都是追加操作，不涉及挪动其他记录，也不会触发叶子节点的分裂。使用业务字段做主键，往往不容易保证有序插入，写入数据的成本较高。 从存储角度考虑，假如在表中有唯一字段身份证号（字符串类型），那么还需要使用自增主键吗？——因为每个非主键索引的叶子节点上存放的都是主键的值，因此如果使用身份证号作为主键，占用的空间显然是比自增主键要来的大的。 有没有什么场景适合用业务字段直接做主键的呢？还是有的。比如，有些业务的场景需求是这样的：\n只有一个索引； 该索引必须是唯一索引。 也就是典型的 KV 场景，由于没有其他索引，所以也就不用考虑其他索引的叶子节点大小的问题，直接将这个索引设置为主键，可以避免每次查询需要搜索两棵树。\n一个索引搜索的例子 有如下的一张表，若执行 select * from T where k between 3 and 5，需要经过几次树的搜索操作，会扫描多少行？表初始化语句如下：\ncreate table T ( ID int primary key, k int NOT NULL DEFAULT 0, s varchar(16) NOT NULL DEFAULT \u0026#39;\u0026#39;, index k(k)) engine=InnoDB; insert into T values(100,1, \u0026#39;aa\u0026#39;),(200,2,\u0026#39;bb\u0026#39;),(300,3,\u0026#39;cc\u0026#39;),(500,5,\u0026#39;ee\u0026#39;),(600,6,\u0026#39;ff\u0026#39;); 当前表的InnoDB索引组织结构 此时SQL语句执行的查询流程如下：\n在 k 索引树上找到 k=3 的记录，取得 ID = 300； 再到 ID 索引树查到 ID=300 对应的 R3； 在 k 索引树取下一个值 k=5，取得 ID=500； 再回到 ID 索引树查到 ID=500 对应的 R4； 在 k 索引树取下一个值 k=6，不满足条件，循环结束。 在这个过程中， 回到主键索引树搜索的过程，称为回表。这个查询过程读了 k 索引树的 3 条记录（步骤 1、3 和 5），回表了两次（步骤 2 和 4）。由于查询结果所需要的数据只在主键索引上有，所以不得不回表，这时候就可以考虑优化索引来避免回表。\n覆盖索引 如果执行的语句是 select ID from T where k between 3 and 5，因为ID的值已经在k索引树上，所以不需要回表。在这个查询里面，索引k已经“覆盖了”查询需求，因此称为覆盖索引。 覆盖索引可以减少树的搜索次数，提升查询性能，所以一个常用的优化手段就是使用覆盖索引。\n因此，如果有高频请求查询非主键字段，可以考虑建立联合索引来避免回表。\n令表中字段为a、b、c、d，其中a为主键。对于一个二级索引b，若查询条件为b，且查询内容不是主键，那么需要回表查询；若存在联合索引（b，c），当查询内容不是主键，但为c时，为覆盖索引，不需要回表。\n最左前缀原则 B+树这种索引结构，可以利用索引的“最左前缀”来定位记录。\n假设存在(name，age)的联合索引，其示意图如下：\n当你的逻辑需求是查到所有名字是“张三”的人时，可以快速定位到 ID4，然后向后遍历得到所有需要的结果。如果要查的是所有名字第一个字是“张”的人，你的 SQL 语句的条件是 where name like ‘张 %’。这时，也能够用上这个索引，查找到第一个符合条件的记录是 ID3，然后向后遍历，直到不满足条件为止。\n可以看到，只要满足最左前缀，就可以利用索引来加速检索。这个最左前缀可以是联合索引的最左 N 个字段，也可以是字符串索引的最左 M 个字符。\n在建立联合索引时，需要考虑的是 索引的复用能力以及 占用的空间：\n索引的复用能力，如果通过调整顺序可以少维护一个索引，那么这个顺序往往就是优先考虑采用的。 对于联合索引（a，b），当查询条件里面只有b时，是无法使用上联合索引的。此时可能需要再维护一个b的索引，因此这时候可以考虑a、b两个字段占用的空间大小来抉择。 索引下推 以一个联合索引（name，age）为例，如果有一个需求，检索出表中“名字第一个字是张，而且年龄是 10 岁的所有男孩”。SQL语句如下：\nselect * from t where name like \u0026#39;张 %\u0026#39; and age=10 and ismale=1; 由于存在前缀索引规则，这个语句会先使用“张”来搜索索引树，找到第一个满足条件的记录。在旧版本的MySQL中，由于没有索引下推的优化， 在匹配到第一条满足条件的记录后便会一条一条的回表查询，引入索引下推后，可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 两者的执行流程图如下：\n无索引下推 使用索引下推 两个流程的区别是，InnoDB 在 (name,age) 索引内部就判断了 age 是否等于 10，对于不等于 10 的记录，直接判断并跳过。在这个例子中，只需要对 ID4、ID5 这两条记录回表取数据判断，就只需要回表 2 次。如果没有索引下推， InnoDB 并不会去看 age 的值，只是按顺序把“name 第一个字是’张’”的记录一条条取出来回表。因此，需要回表 4 次。\n普通索引和唯一索引 查询过程 还是使用上文所述的身份证的例子，执行的查询语句为 select id from T where k=5 （k值是不重复的）。这个查询语句在索引树上查找的过程，先是通过 B+ 树从树根开始，按层搜索到叶子节点，然后可以认为数据页内部通过二分法来定位记录。\n对于普通索引来说，查找到满足条件的第一个记录 (5,500) 后，需要查找下一个记录，直到碰到第一个不满足 k=5 条件的记录。 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索。 由于InnoDB的数据是按数据页为单位来读写的。当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存（页大小默认为16KB）。对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算，如果 k=5 这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些。 因此，其实两类索引在这个案例上的性能差异几乎为零。\n更新过程 change buffer 当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB 会将这些更新操作缓存在 change buffer 中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行 change buffer 中与这个页有关的操作。\n将 change buffer 中的操作应用到原数据页，得到最新结果的过程称为 merge。除了访问这个数据页会触发 merge 外，系统有后台线程会定期 merge。在数据库正常关闭（shutdown）的过程中，也会执行 merge 操作。change buffer实际上也是可以持久化的，在内存中有拷贝也会写入磁盘。\nchange buffer的使用条件 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束。比如，要插入 (4,400) 这个记录，就要先判断现在表中是否已经存在 k=4 的记录，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用 change buffer 了。所以， 唯一索引的更新就不能使用 change buffer，实际上也只有普通索引可以使用。\n因此回到表中，如果要在表插入一条新的记录(4,400)，InnoDB的处理流程可以分为两种情况：\n这个记录 要更新的目标页在内存中。\n对于 唯一索引 来说，找到3和5之间的位置，判断有没有冲突，插入这个值，语句执行结束。 对于 普通索引 来说，找到3和5之间的位置，插入这个值，语句执行结束。 可以看出，这种情况下，两种索引之间的区别仅仅只是判断有没有冲突，因此性能差距不大。\n这个记录 要更新的目标页不在内存中。\n对于 唯一索引 来说，需要将数据页读入到内存中，判断有没有冲突，插入这个值，语句执行结束。 对于 普通索引 来说，则是将更新记录在change buffer，语句执行就结束了。 将数据从磁盘中读取内存涉及IO访问，成本较高，而change buffer就是避免了这一操作，对更新性能的提升是较为明显的。\nchange buffer使用场景 上面提到了change buffer适用于普通索引情况下，而不适用于唯一索引。那 是否所有的普通索引，使用change buffer都可以起到加速作用？\n因为 merge 的时候是真正进行数据更新的时刻，而 change buffer 的主要目的就是将记录的变更动作缓存下来，所以在一个数据页做 merge 之前，change buffer 记录的变更越多（也就是这个页面上要更新的次数越多），收益就越大。所以， 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时 change buffer 的使用效果最好。 这种业务模型常见的就是账单类、日志类的系统。 反过来，假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在 change buffer，但之后由于马上要访问这个数据页，会立即触发 merge 过程。这样随机访问 IO 的次数不会减少，反而增加了 change buffer 的维护代价。 所以，对于这种业务模式来说，change buffer 反而起到了副作用。\n从上面的描述可以看出，在这两种索引的选择上，尽量选择普通索引，如果数据的更新伴随着查询，那么就应该关闭change buffer。 普通索引+change buffer 在数据量大的表中其实就能够起到不错的优化效果了。\n至于MySQL怎么创建普通索引，如下所示（同时也列举了其他一些索引的创建方式）。\n# 直接在建立表的时候就创建索引 CREATE TABLE users ( id INT PRIMARY KEY, username VARCHAR(50), email VARCHAR(100), INDEX idx_username (username), -- 普通索引 UNIQUE INDEX idx_email (email), -- 唯一索引 PRIMARY KEY (id) -- 主键索引 ); # 创建完表之后再添加索引 -- 添加普通索引 ALTER TABLE users ADD INDEX idx_age (age); -- 添加唯一索引 ALTER TABLE users ADD UNIQUE INDEX idx_phone (phone); -- 添加复合索引 ALTER TABLE orders ADD INDEX idx_customer_date (customer_id, order_date); change buffer和redo log 假设要在表中执行以下的插入语句 insert into t(id, k) values(d1, k1), (id2, k2); 假设当前k1所在的数据页page1在内存中，k2所在的数据页page2不在内存中。其更新状态如下图所示：\n带change buffer的更新过程 这条更新语句涉及了四个部分：内存、redo log（ib_log_fileX）、 数据表空间（t.ibd）、系统表空间（ibdata1）\n这条更新语句做了如下几个操作：\nPage 1 在内存中，直接更新内存； Page 2 没有在内存中，就在内存的 change buffer 区域，记录下“要往 Page 2 插入一行”这个信息 将上述两个动作记入 redo log 中（图中 3 和 4）。 做完这些之后，事务就可以完成了。那么之后的读请求要如何处理呢？假设之后要执行 select * from t where k in (k1, k2); 如果内存中的数据都还在，那么读过程如下图所示：\n带change buffer的读过程 从图中可以看出：\n读 Page 1 的时候，直接从内存返回。\n这里也说明了WAL之后读数据并不一定要等待redo log将数据写入磁盘。只要内存中的数据是正确的，就可以直接从内存中读取。\n要读 Page 2 的时候，需要把 Page 2 从磁盘读入内存中，然后应用 change buffer 里面的操作日志，生成一个正确的版本并返回结果。\n总结来说就是， redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 这里再简单说明一下为什么redo log节省了随机写磁盘的IO消耗，因为将要写入磁盘的数据先顺序写入了内存中，从而避免了寻道。\n延伸问题 如果自增主键用完了怎么办？\n先简单回答一下，对于自增主键用完可以使用MySQL提供的线上修改方式或者第三方工具，修改字段类型，增加自增主键的数据范围，如改为BigInt。 但细想，其实这个问题既成立又不成立，对于一个int类型作为自增主键，其数据范围是0～2147483648，单表21亿条数据，就算考虑ID断序的问题，那么也有十几亿条数据，显然查询的效率不会太高。这时候就需要考虑到分库分表，一旦分库分表，就不能依赖每个表的自增主键来全局唯一标识数据了，这时候就需要类似雪花算法、UUID等来提供全局唯一ID的生成。\n参考 MySQL45讲——04 深入浅出索引（上） MySQL45讲——05 深入浅出索引（下） MySQL 页完全指南——浅入深出页的原理 - 知乎 MySQL中的自增主键用完了怎么办？ MySQL45讲——09 普通索引和唯一索引，应该怎么选择？ ","date":"2025-07-03T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/mysql%E5%AD%A6%E4%B9%A0%E5%9B%9B-%E7%B4%A2%E5%BC%95/","title":"MySQL学习（四）| 索引"},{"content":"前言 在和数据库打交道，事务总是绕不开的。简单说，事务就是要保证一组数据库操作要么全部成功，要么全部失败。 在 MySQL 中，事务支持是在引擎层实现的。MySQL 是一个支持多引擎的系统，但并不是所有的引擎都支持事务。比如 MySQL 原生的 MyISAM 引擎就不支持事务，这也是 MyISAM 被 InnoDB 取代的重要原因之一。\n隔离性与隔离级别 事务具有四大特性，也就是常说的ACID（Atomicity、Consistency、Isolation、Durability，即原子性、一致性、隔离性、持久性），这里打算说说隔离性。\n当数据库上有多个事务同时执行的时候，就可能出现脏读（dirty read）、不可重复读（non-repeatable read）、幻读（phantom read）的问题，为了解决这些问题，就有了“隔离级别”的概念。\n隔离级别越高，效率就会越低。 SQL的事务隔离级别包括：读未提交（read uncommitted）、读提交（read committed）、可重复读（repeatable read）和串行化（serializable ）。\n读未提交，一个事务还没提交时，它做出的变更就能被其他事务看到 读提交，一个事务提交之后，它做出的变更才能被其他事务看到 可重复读，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。在这个级别下，未提交的变更对其他事务也是不可见的。 串行化，对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 下面对这几种隔离级别进行简单解释。假设数据表T中只有一列，其中一行的值为1，下面是按照时间顺序执行的两个事务的行为。\n在不同的隔离级别下，图中V1、V2、V3会得到不同的值\n若隔离级别是“读未提交”， 则 V1 的值就是 2。这时候事务 B 虽然还没有提交，但是结果已经被 A 看到了。因此，V2、V3 也都是 2。 若隔离级别是“读提交”，则 V1 是 1，V2 的值是 2。事务 B 的更新在提交后才能被 A 看到。所以， V3 的值也是 2。 若隔离级别是“可重复读”，则 V1、V2 是 1，V3 是 2。之所以 V2 还是 1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的。 若隔离级别是“串行化”，则在事务 B 执行“将 1 改成 2”的时候，会被锁住。直到事务 A 提交后，事务 B 才可以继续执行。所以从 A 的角度看， V1、V2 值是 1，V3 的值是 2。 在实际实现上，数据库会创建一个视图，访问得到的就是视图的逻辑结果。在“可重复读”这个隔离级别下，视图是在事务启动时创建的，整个事务存在期间都是用这个视图；在“读提交”，视图是在每个SQL语句开始执行之后创建的；在“读未提交”这个隔离级别下，没有视图的概念，直接返回记录的最新值；在“串行化”隔离级别下，则是直接使用加锁的方式来避免并行访问\n事务隔离的实现 在 MySQL 中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从 1 被按顺序改成了 2、3、4，在 回滚日志（undo log） 里面就会有类似下面的记录。\n当前值为4，但是不同时刻启动的事务会有不同的read-view。在视图 A、B、C 里面，这一个记录的值分别是 1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。即使现在有另外一个事务正在将 4 改成 5，这个事务跟 read-view A、B、C 对应的事务是不会冲突的。\n但是回滚日志（undo log）不可能一直保留，系统会进行判断，在不需要的时候将回滚日志删除，当系统中没有比这个回滚日志更早的read view时，意味着这个回滚日志可以被删除了。（也就是没有事务使用比当前回滚日志更早的回滚日志了）\n因此，就衍生出一个问题，尽量不要使用长事务。 长事务意味着系统里会存在很多老的事务视图，由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间。\n事务的启动方式 MySQL的事务启动方式有如下几种：\n显式启动事务语句， begin 或 start transaction。配套的提交语句是 commit，回滚语句是 rollback。 set autocommit=0，这个命令会将这个线程的自动提交关掉。意味着如果你只执行一个 select 语句，这个事务就启动了，而且并不会自动提交。这个事务持续存在直到你主动执行 commit 或 rollback 语句，或者断开连接。 有些客户端连接框架会默认连接成功后先执行一个 set autocommit=0 的命令。这就导致接下来的查询都在事务中，如果是长连接，就导致了意外的长事务。所以建议使用set autocommit=1，并用显式语句来启动事务。\n也可以使用commit work and chain的语法，在 autocommit 为 1 的情况下，用 begin 显式启动的事务，如果执行 commit 则提交事务。如果执行 commit work and chain，则是提交事务并自动启动下一个事务。\n事务是隔离还是不隔离的？ 在阅读这部分之前，需要对行锁有相应的了解→小土地庙\n前面说过，在可重复读隔离级别下，事务T启动时会创建一个视图read-view。之后事务执行期间，即使有其他事务修改了数据，事务T看到的数据仍然和刚启动时一样。但在锁的篇章中说过，一个事务要更新一行，如果另一个事务拥有这行的行锁，此时事务就会被锁住进入等待状态。那么此时就有出现一个问题： 等这个事务获取到行锁要更新数据时，它读到的值又是什么？\n下面给出一张表和事务的执行顺序。\nCREATE TABLE `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, PRIMARY KEY (`id`) ) ENGINE=InnoDB; insert into t(id, k) values(1,1),(2,2); 事务A 事务B 事务C start transaction with consistent snapshot; start transaction with consistent snapshot; update t set k=k+1 where id=1 update t set k=k+1 where id=1;select k from t where id=1; select k from t where id=1;commit; commit; begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作 InnoDB 表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 这个命令。\n第一种启动方式，一致性视图是在第执行第一个快照读语句时创建的； 第二种启动方式，一致性视图是在执行 start transaction with consistent snapshot 时创建的。\n对于表中的事务C，语句本身就是一个事务，语句完成后就会自动提交。\n这个事务组的执行结果为： **事务B查到的k值为3，事务A查到的k值为1。**产生这种结果，是因为MySQL中存在两种“视图”的概念：\n一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 “快照”在MVCC中怎么工作的 在可重复读级别下，事务启动就“拍了个快照”，这个快照是基于整库的。下面来看这个快照是怎么实现的。\nInnoDB 里面每个事务有一个唯一的事务 ID，叫作 transaction id。它是在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且把 transaction id 赋值给这个数据版本的事务 ID，记为 row trx_id。同时，旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它。如图，就是数据被事务连续更新的状态。\n数据的行变更记录 图中虚线框里是同一行数据的 4 个版本，当前最新版本是 V4，k 的值是 22，它是被 transaction id 为 25 的事务更新的，因此它的 row trx_id 也是 25。图中的虚线箭头也就是前文提到的 回滚日志（undo log）。实际上，V1、V2、V3 并不是物理上真实存在的，而是每次需要的时候根据当前版本和 undo log 计算出来的。比如，需要 V2 的时候，就是通过 V4 依次执行 U3、U2 算出来。\n按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。在实现上，InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务 ID。“活跃”指的就是，启动了但还没提交。 这个视图数组和高水位，就组成了当前事务的一致性视图（read-view）。而数据版本的可见性规则，就是基于数据的 row trx_id 和这个一致性视图的对比结果得到的。\n这个视图数组把所有的 row trx_id 分成了几种不同的情况。\n数据版本的可见性规则 这样，对于事务启动瞬间来说，一个数据版本的row trx_id，有以下几种可能：\n如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 个人理解：\n事务开始时创建的是启动了还没提交的事务ID，低水位是数组里面事务ID最小值，高水位为系统已经创建过的事务ID的最大值+1。图中的黄色部分就是为每个事务创建的一致性视图。\n对于每个数据的版本来说，它携带有一个事务ID，而这个事务ID是有可能落在图中三个区间中的任何一个的，如果事务ID，小于低水位，说明事务提交了，肯定可见；如果大于高水位，说明事务未开始，肯定不可见；如果在两个水位之间，就要判断是否在数组之中，如果在数组之中，那么说明事务未提交，如果不在数组之中，说明事务在当前事务启动时已经提交了。\n对于上文数据行变更图中的数据来说，如果有一个事务，它的低水位是18，那么当它访问这一行数据时，就会从V4通过U3计算出V3，所以在它看来，这一行的值是11。\n案例分析 接下来，分析下一开始图中的事务A的语句返回的结果，为什么是k=1。\n这里，我们不妨做如下假设：\n事务A开始前，系统里面只有一个活跃事务ID是99； 事务A、B、C的版本号分别是100、101、102，且当前系统里只有这四个事务； 三个事务开始前，(1,1）这一行数据的row trx_id是90。 这样，事务A的视图数组就是[99,100], 事务B的视图数组是[99,100,101], 事务C的视图数组是[99,100,101,102]。去除其他干扰语句如下图：\n更新逻辑 这里可能会存在疑问，事务B的update语句，如果按照一致性读，好像结果不对哦？\n图中中，事务B的视图数组是先生成的，之后事务C才提交，不是应该看不见(1,2)吗，怎么能算出(1,3)来？\n是的，如果事务B在更新之前查询一次数据，这个查询返回的k的值确实是1。但是，当它要去更新数据的时候，就不能再在历史版本上更新了，否则事务C的更新就丢失了。因此，事务B此时的set k=k+1是在（1,2）的基础上进行的操作。\n所以，这里就用到了这样一条规则：更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。\n当遇到更新语句时，此时不管事务ID是否可见，都要使用最新版本的数据，但是\n因此，在更新的时候，当前读拿到的数据是(1,2)，更新后生成了新版本的数据(1,3)，这个新版本的row trx_id是101。所以，在执行事务B查询语句的时候，一看自己的版本号是101，最新数据的版本号也是101，是自己的更新，可以直接使用，所以查询得到的k的值是3。\n这里我们提到了一个概念，叫作当前读。其实，除了update语句外，select语句如果加锁，也是当前读。\n所以，如果把事务A的查询语句select * from t where id=1修改一下，加上lock in share mode 或 for update，也都可以读到版本号是101的数据，返回的k的值是3。下面这两个select语句，就是分别加了读锁（S锁，共享锁）和写锁（X锁，排他锁）。\n案例进阶 再往前一步，假设事务C不是马上提交的，而是变成了下面的事务C’，会怎么样呢？\n事务A、B、C执行流程 事务C’的不同是，更新后并没有马上提交，在它提交前，事务B的更新语句先发起了。前面说过了，虽然事务C’还没提交，但是(1,2)这个版本也已经生成了，并且是当前的最新版本。那么，事务B的更新语句会怎么处理呢？\n这时候，因为事务C’没提交，也就是说(1,2)这个版本上的写锁还没释放。而事务B是当前读，必须要读最新版本，而且必须加锁，因此就被锁住了，必须等到事务C’释放这个锁，才能继续它的当前读。\n事务B更新逻辑 到这里，我们把一致性读、当前读和行锁就串起来了。\n最后的总结 现在，我们再回到文章开头的问题：事务的可重复读的能力是怎么实现的？\n可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待。\n而读提交的逻辑和可重复读的逻辑类似，它们最主要的区别是：\n在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图； 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图。 那么，我们再看一下，在读提交隔离级别下，事务A和事务B的查询语句查到的k，分别应该是多少呢？\n这里需要说明一下，“start transaction with consistent snapshot; ”的意思是从这个语句开始，创建一个持续整个事务的一致性快照。所以，在读提交隔离级别下，这个用法就没意义了，等效于普通的start transaction。\n下面是读提交时的状态图，可以看到这两个查询语句的创建视图数组的时机发生了变化，就是图中的read view框。\n这时，事务A的查询语句的视图数组是在执行这个语句的时候创建的，时序上(1,2)、(1,3)的生成时间都在创建这个视图数组的时刻之前。但是，在这个时刻：\n(1,3)还没提交，属于情况1，不可见； (1,2)提交了，属于情况3，可见。 所以，这时候事务A查询语句返回的是k=2。\n显然地，事务B查询结果k=3。\n参考 MySQL实战45讲03——事务隔离：为什么你改了我还看不见? MySQL实战45讲08——事务到底是隔离还是不隔离的？ ","date":"2025-07-01T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/mysql%E5%AD%A6%E4%B9%A0%E4%B8%89-%E4%BA%8B%E5%8A%A1/","title":"MySQL学习（三）| 事务"},{"content":"前言 在了解了查询语句之后，再来了解一下更新语句的执行流程。\n对于如下这个表，有一个主键ID和整型字段c：\ncreate table T(ID int primary key, c int); 如果要将ID=2的行的字段 c 加一，那么SQL语句就会这么写\nupdate T set c = c + 1 where ID = 2; 在之前讲述过MySQL的整体架构，准确来说，查询语句经历的流程更新语句也会走一遍。首先执行前需要先连接数据库，又因为这是对表进行更新的语句，因此会将查询缓存清空，接下来分析器进行词法分析和语法分析确定这是一条更新语句。优化器决定使用ID这个索引，最后交由执行器执行。\n与查询流程不一样的是，更新流程涉及两个重要的日志模块，redo log（重做日志）和binlog（归档日志）。\nredo log 对于不断到来的修改记录，数据库自然需要对应的存储方式。如果记录不多，可以将记录存储在缓存中，当数据量大的后，日志系统不可避免的要存储到磁盘中。此时有两种存储方式\n一种是直接对对应的记录进行增减。 另一种直接记录当前的操作记录，待之后空闲再进行所有记录的核算。 在高并发的情况下，第二种方式无疑是较为合理的，避免了先在所有数据中找到对应的记录之后才能添加修改日志。\nMySQL就是是这样，如果每次更新都需要写入磁盘，磁盘也需要找到对应的那条记录再进行更新，整个过程的IO成本和查找成本都很高。\n具体说，当有一条记录需要更新的时候，InnoDB 引擎就会先把记录写到 redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB 引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，也就是WAL技术，全称是Write-Ahead-Logging，关键就是先写日志再写磁盘。\n为什么说redo log减少了磁盘随机写的IO性能消耗？\n如果没有redo log，那么每次写入都需要写入磁盘，而数据在磁盘中是随机存储的，会有寻道时间，如果是写入redo log，因为是顺序写入，不需要考虑寻道，所以性能就提升上来了。\nInnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么内存就可以记录 4GB 的操作。从头开始写，写到末尾就又回到开头循环写。如下图所示\nwrite pos 是当前记录的位置，一边写一边后移，写到第 3 号文件末尾后就回到 0 号文件开头。checkpoint 是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。\nwrite pos 和 checkpoint 之间的是还可以使用的内存大小，可以用来记录新的操作。如果 write pos 追上 checkpoint，表示内存满了，这时候不能再执行新的更新，得停下来先擦掉一些记录，把 checkpoint 推进一下。\n有了 redo log，InnoDB 就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个能力称为 crash-safe 。\nbinlog MySQL 整体来看，其实就两块：一块是 Server 层，它主要做的是 MySQL 功能层面的事情；还有一块是引擎层，负责存储相关的具体事宜。之前提到的redo log就是InnoDB引擎特有的日志，Server层也有自己的日志，即binlog（归档）日志。\n这里提一嘴，MySQL自带的引擎是MyISAM，但这个引擎并没有crash-safe的能力。因此InnoDB就使用了另一种日志系统来实现crash-safe能力。\n这两种日志有以下几点不同：\nredo log 是 InnoDB 引擎特有的；binlog 是 MySQL 的 Server 层实现的，所有引擎都可以使用。 redo log 是物理日志，记录的是“在某个数据页上做了什么修改”；binlog 是逻辑日志，记录的是这个语句的原始逻辑，比如“给 ID=2 这一行的 c 字段加 1 ”。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 执行器和InnoDB引擎在执行这个update语句时的内部流程如下：\n执行器先找引擎取 ID=2 这一行。ID 是主键，引擎直接用树搜索找到这一行。如果 ID=2 这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回。 执行器拿到引擎给的行数据，把这个值加上 1，比如原来是 N，现在就是 N+1，得到新的一行数据，再调用引擎接口写入这行新数据。 引擎将这行新数据更新到内存中，同时将这个更新操作记录到 redo log 里面，此时 redo log 处于 prepare 状态。然后告知执行器执行完成了，随时可以提交事务。 执行器生成这个操作的 binlog，并把 binlog 写入磁盘。 执行器调用引擎的提交事务接口，引擎把刚刚写入的 redo log 改成提交（commit）状态，更新完成。 如下图所示，深色框表示在执行器中执行的，浅色框表示在InnoDB中执行的。\nupdate语句执行流程 在流程的最后阶段，将redo log的写入拆为两个阶段，prepare和commit，这也就是MySQL的 两阶段提交。\n两阶段提交 “两阶段提交”是为了让两份日志的逻辑一致。\n前面说到，binlog会记录所有的逻辑操作，采用的是“追加写“的方式。一般来说，系统会保存最近一段时间的binlog日志，同时会定期做整库备份。当需要将数据库恢复到指定的某一秒时（以下称为“指定时间”），可以这么做：\n找到最近的一次全量备份，将这个备份恢复到临时库。 从备份的时间点开始，将备份的binlog依次取出来，恢复到指定的时间。 现在，临时库的数据就和之前的线上库一样的，只需要将临时库数据按需要恢复到线上库即可。 为什么要“两阶段提交”？ 这里其实可以探讨如果不用“两阶段提交”会有什么问题，还是使用前面提到的update的例子。\n先写redo log再写binlog。 假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。 但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。 然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。 先写binlog后写redo log。 如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。 这里我的理解是，redo log和binlog分别处理两种不同的恢复，一种是崩溃恢复，一种是临时库恢复。因此“两阶段提交”的目的是为了这两种恢复的一致性。\n这里还有一个关键点是，binlog日志并不仅仅是为了临时库恢复，当需要扩容（多搭建一些备库来增强MySQL的读能力的时候），常见的做法也是使用全量备份+binlog日志来实现的，此时不一致就会导致线上出现主从数据库不一致的情况；同时binlog日志在大数据场景中可能也会应用到MySQL增量数据读取上。\nundo log 这里稍微介绍一下undo log，较为具象的内容在事务篇章会涉及到。\n数据库事务四大特性中有一个是 原子性。 如果要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行 回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常，直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。\n同时，多版本并发控制（MVCC）也是通过undo log来实现的。\n总结 MySQL InnoDB 引擎使用 redo log（重做日志）保证事务的持久性，使用 undo log（回滚日志） 来保证事务的原子性。MySQL数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。\n延伸问题 MySQL为了提高性能，对redo log的操作一开始都是在内存中的，那么如果MySQL进程崩溃了，在内存中那部分redo log要怎么处理？\n查阅了相关资料后，稍微回答一下。在MySQL崩溃的时候，如果没有相关机制，确实会导致在内存中的redo log日志丢失，这里就要提到MySQL的 刷盘 机制，MySQL必须保证事务提交时产生的redo log最终被安全的写入磁盘中。负责写入磁盘的操作就是刷盘，而MySQL提供了不同的刷盘机制来决定数据何时落盘，也就是这样来解决这个问题的。（感觉这部分依旧是有很多内容，挖坑！）\n参考 MySQL45讲——日志系统：一条SQL更新语句是如何执行的？ 大厂基本功 | MySQL 三大日志 ( binlog、redo log 和 undo log ) 的作用？ - 知乎 ","date":"2025-06-30T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/mysql%E5%AD%A6%E4%B9%A0%E4%BA%8C-%E6%97%A5%E5%BF%97%E7%B3%BB%E7%BB%9F/","title":"MySQL学习（二）| 日志系统"},{"content":"已经许久没有更新过博客了，曾经的热情似乎早已不再。在经历过一些人和事之后，觉得还是有必要重启这个博客，回归打算就从MySQL开始吧，经好友推荐了解到了《MySQL45讲》，之前的数据库基础实在是有点薄弱，打算趁此机会正式学习一下这个数据库。\nMySQL基础架构 MySQL逻辑架构 如图所示，是MySQL的基础架构，大体可以分为server层和存储引擎层两部分。Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现。存储引擎层负责数据的存储和提取。其架构模式是插件式的，最常用的存储引擎是InnoDB，也是MySQL的默认存储引擎。当然，我们也可以在建表时使用 engine=MyISAM 来更改存储引擎。\n值得一提是，MySQL8.0取消了查询缓存这个模块，原因自然是查询缓存存在弊端和局限性。同时由于现代缓存redis发展的成熟，将缓存置于客户端也有更多的好处。这些内容有机会再另开一篇博客稍作解释吧。\n连接器 第一步，在连接到数据库之后，首先通过的就是连接器。连接器负责跟客户端建立连接、获取权限、维持和管理连接。连接命令如下：\nmysql -h$ip -P$port -u$user -p$password 在经过 TCP握手 之后，连接器就会开始认证你的身份。\n如果用户名或密码不对，你就会收到一个”Access denied for user”的错误，然后客户端程序结束执行。 如果用户名密码认证通过，连接器会到权限表里面查出你拥有的权限。之后，这个连接里面的权限判断逻辑，都将依赖于此时读到的权限。 一个用户成功建立连接后，即使你用管理员账号对这个用户的权限做了修改，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。\n如果连接断开之后（如果客户端长期处于空闲，MySQL也会将连接中断，由参数 wait_timeout 控制），客户端再次发送请求，就会发生错误，此时需要进行重连。这里区分了 长连接 和 短连接 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。\n建立连接的过程较为复杂，实际使用中要减少建立连接的动作，因此尽量使用长连接。\n但是全部使用长连接后，MySQL占用内存涨得特别快，这是因为MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（OOM），从现象看就是MySQL异常重启了。\n如何解决这种问题？\n定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存 因为新版的MySQL已经舍弃了这一部分，所以不作过多介绍。只需要知道MySQL拿到一个查询请求后，会先到查询缓存看看，之前是不是执行过这条语句。之前执行过的语句及其结果可能会以键值对的形式，被直接缓存在内存中。key是查询的语句，value是查询的结果。如果你的查询能够直接在这个缓存中找到key，那么这个value就会被直接返回给客户端。\n但是查询缓存的失效十分频繁，只要有针对表的更新，那么关于这个表的查询缓存就会被清空。\n分析器 分析器首先会进行“词法分析”，识别出里面的字符串分别是什么，代表什么。之后进行“语法分析”，根据词法分析的结果，语法分析器会根据语法规则，判断你输入的这个SQL语句是否满足MySQL语法。\n如果查询的字段在表中不存在，那么在分析器阶段就会报错\n优化器 经过分析器，MySQL就知道要做什么了，在执行之前还需要经过优化器处理。优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。比如你执行下面这样的语句，这个语句是执行两个表的join：\nmysql\u0026gt; select * from t1 join t2 using(ID) where t1.c=10 and t2.d=20; 此时优化器要决定先取出t1表的数据还是t2表的数据再进行关联。虽然是一样的逻辑，但可能有不一样的执行效率。\n执行器 到了执行器之后就开始正式执行语句。开始执行的时候，要先判断一下你对这个表 T 有没有执行查询的权限，如果没有，就会返回没有权限的错误。对于如下的这个语句，若ID字段没有索引，那么执行流程如下\nselect * from T where ID = 10; 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 参考 MySQL45讲——01 基础架构：一条SQL查询语句是如何执行的？ ","date":"2025-06-27T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/mysql%E5%AD%A6%E4%B9%A0%E4%B8%80-%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84/","title":"MySQL学习（一）| 基础架构"},{"content":"想来既然决定了走Java这条路，那么一定是绕不开redis了，redis的使用以及其原理也算是一门艺术了，希望通过博文把一些常见的redis问题和操作记录下来。\n前言 在当前的数据库体系中有两大类型的数据库，一类是关系型数据库，另一类为非关系型数据库。关系型数据库典型的数据结构是表，是由二维表及其之间的联系所组成的数据组。非关系型数据库严格上来说不是一种数据库，而是一种 数据结构化存储方法的集合，可以是文档或者键值对等。 两者的优缺点如下\n关系型数据库 优点 缺点 使用方便，SQL语言较为通用，可以用于复杂查询 高并发读写性能较差，在海量数据的读写场景中性能较差，硬盘的IO是一个无法避免的瓶颈 易于维护，都是使用表结构，格式一致 灵活度较低，表结构固定，DDL修改对业务影响较大 复杂操作，可用于一个表以及多个表之间的复杂查询 支持事务控制 非关系型数据库 优点 缺点 读写速度快，可以存储在内存中，不依赖于硬盘 不支持join等复杂连接操作 数据格式灵活，可以是键值对，文档、图片等，扩展性强 事务处理能力弱 缺乏数据完整性约束 不提供SQL支持 redis简介 Redis（Remote Dictionary Server），即远程字典服务，是一个用C语言编写可基于内存亦可以持久化的日志型，Key-Value数据库。是非关系型数据库的一种解决方案，也是目前业界主流的缓存解决方案组件。\nwhy redis redis性能优秀，能够支持每秒大量的读写操作，还支持集群，分布式、主从同步等配置。同时支持一定的事务能力，保证了高并发场景下的数据的安全和一致性。还有一点，redis的社区十分活跃。其优点如下：\nredis优点 通常情况下，redis作为MySQL等数据库的缓存层使用。为什么要有缓存？如果在这样一个场景，当大量的数据请求访问MySQL，过多的请求可能会导致MySQL服务器压力过大，甚至会因为过量的请求将数据库击穿，数据服务也会因此中断。此时如果有缓存，那么数据访问请求将会先通过缓存再到达数据库，一旦请求在缓存中得到响应，将不会再查询数据库，这会很大的减少数据库的压力。简单描述如下图：\n什么数据可以放在redis 这个问题其实一直萦绕在脑海里很久，在网上翻阅了一段时间也没有找到描述得较为详细的，更多的是应用的具体场景，但在我看来都不够抽象。看来这个问题也只有不断的在实践中去寻找答案了。这里就稍微简单描述一下\n不需要实时更新但是又极其消耗数据库的数据。例如网上的商品销售排行榜，这种数据只需要每隔一段时间统计即可，其实时性关注度并不高。 更新频率不高，但是访问比较频繁的数据。这类数据如果放置于缓存能够一定程度上减少数据库的访问压力。如用户个人资料，设置完成后并不会频繁更新，但是为了个性化服务可能会频繁访问。 需要实时更新，但是更新频率不高的数据。比如一个用户的订单列表，用户的订单显然是需要实时呈现的，但是频繁下单的情况又比较少。 在某个时刻访问量极大而且更新也很频繁的数据。种数据有一个很典型的例子就是秒杀，在秒杀那一刻，可能有N倍于平时的流量进来，系统压力会很大。但是这种数据使用的缓存不能和普通缓存一样，这种缓存必须保证不丢失，否则会出现一致性等问题。 redis缓存问题 缓存雪崩 定义 大量或全部缓存数据突然失效或消失，导致所有请求都直接打到数据库上，数据库在巨大的压力下响应缓慢或宕机，应用性能急剧下降，就像雪崩一样。\n触发原因 同步过期。如果你将大量缓存设置为在同一时间过期。突然间，所有数据都需要重新加载到缓存中，这时候所有的请求都会转到数据库上，导致瞬间流量激增。 系统重启。有时系统维护或意外的服务重启会导致所有缓存失效。当服务再次上线，所有的请求都会涌向空无一物的缓存，然后转向数据库。 Redis服务宕机。硬件故障、网络问题或配置错误都可能导致Redis服务不可用。此时所有的请求都会打向数据库。 热点key消失。在某些情况下，特定的热点key（被大量频繁访问的key）如果失效或被删除，也会导致相应的大量请求直接落到数据库上，造成局部的雪崩效应。 解决方案 过期策略改进 随机过期时间。给缓存项设置随机的过期时间可以防止它们同时失效。例如，希望缓存大约在1小时后过期，可以设置过期时间为60±10分钟。 细粒度过期。对于一些热点数据，可以使用更细粒度的过期时间。如使用不同的过期时间策略针对不同类型或频率访问。 预防措施 合理设置缓存失效时间。根据应用的具体情况合理设置缓存的失效时间，避免大量缓存同时过期。对于不同的数据和业务场景，失效时间应该有所不同。 持久化策略。利用Redis的RDB或AOF持久化机制，确保在系统重启后缓存可以被恢复，减少对数据库的压力。 备份机制。确保有备份和灾难恢复计划，当缓存服务器出现问题时，可以快速恢复或切换到备份系统。 热点数据处理 识别热点数据。监控和识别访问频率特别高的数据。这些数据是潜在的热点，需要特别关注。 分布式锁。对于热点key的更新操作，可以使用分布式锁来确保同一时间只有一个请求去构建新的缓存，避免大量请求同时击中数据库。 使用队列。对于高频更新的热点数据，可以使用消息队列来缓冲和序列化处理请求。 降级和限流 服务降级。在缓存雪崩或其他系统异常时，可以暂时关闭一些非核心功能，保证核心功能的正常运作。 请求限流。通过算法（如令牌桶、漏桶等）限制访问频率，确保系统在承受范围内。 缓存穿透 定义 当请求查询的数据在缓存中不存在时（也不存在于数据库中），请求便会“穿透”缓存层直接查询数据库。在正常情况下，缓存系统会减轻对数据库的访问压力，但在缓存穿透的情况下，大量的无效请求会直接落在数据库上，导致数据库负载激增，甚至可能导致服务瘫痪。\n触发原因 恶意攻击\n攻击者可能会故意请求缓存中不存在的数据。这种攻击通常旨在使应用程序变慢或崩溃，从而达到拒绝服务的效果。\n系统缺陷\n设计缺陷。如果系统没有妥善处理不存在的数据请求，例如未设置合理的默认行为或缓存策略，那么即使是正常的用户行为也可能导致缓存穿透。 数据不一致。在有些情况下，缓存和数据库之间的数据不同步也可能导致缓存穿透。 错误的用户输入\n无效请求。用户错误的输入，如错误的ID或查询参数，如果没有妥善处理，也可能导致请求直接查询数据库。 缺乏验证。系统未能验证输入的有效性，也可能导致大量无效查询穿透缓存层。 解决方案 布隆过滤器\n原理\n如何工作： 布隆过滤器是一种空间效率很高的数据结构，它可以告诉你某个元素是否在一个集合中。它使用多个哈希函数将元素映射到位数组中的几个点。如果检查时所有点都是1，那么元素可能存在；如果任何一个点是0，则元素一定不存在。 适用场景： 对于缓存穿透问题，布隆过滤器可以作为请求的第一道防线，用来检查请求的数据是否有可能存在于数据库中。 应用\n设置过程： 将所有可能的有效数据的标识（例如ID）添加到布隆过滤器中。然后，每次缓存查询前先查询布隆过滤器。 效果： 如果布隆过滤器说数据一定不存在，就可以直接拒绝请求，避免查询数据库。 空值缓存\n策略\n核心思想： 即使一个查询的结果是空（即数据在数据库中不存在），也将这个“空”结果缓存起来。这样，下次相同的查询来时，直接从缓存中返回空结果，而不需要再次查询数据库。 注意： 空结果设置一个较短的过期时间，避免长时间内有效数据被错误地判定为不存在。 优势与考量\n减轻数据库压力。这种方法可以显著减少对数据库的无效查询。 考量。过多的空结果缓存可能会占用大量缓存空间，应根据实际情况和业务需求适度使用。 缓存击穿 定义 指一个热门的缓存键在缓存中过期或不存在的情况下，大量请求同时访问该键所对应的数据，导致这些请求直接绕过缓存，直接访问数据库。\n触发原因 缓存失效\n当一个热门的缓存键对应的数据在缓存中过期或者不存在时，如果此时有大量请求访问这个缓存键，就会导致缓存击穿。缓存失效可能是由于缓存策略设置的过期时间到期，或者手动删除缓存数据引起的。\n大量并发请求\n缓存击穿通常不是由单一请求引起的，而是由大量并发请求集中在某个特定的热门数据上。这可能是由于系统设计的瓶颈、缓存数据的热度高、某个功能或数据点引起了极大的用户兴趣等原因。当大量请求同时访问一个缓存失效或者不存在的热门数据时，它们都会绕过缓存，直接访问数据库。\n解决方案 热点数据预加载。在数据即将过期之前，提前异步加载新的数据到缓存中。通过定期或异步地预加载热门数据，可以避免缓存失效时大量请求同时访问。 互斥锁机制。在获取缓存数据之前，先尝试获取锁，只有一个线程能够从底层存储系统中加载数据，其他线程需要等待锁释放。这样可以避免多个线程同时访问存储系统，减轻了缓存击穿的可能性。 设置合理的缓存失效时间。缓存的过期时间应该设置得既不会导致数据过于陈旧，也不会过于频繁地触发缓存失效。合理的过期时间有助于平衡缓存的新鲜度和系统性能。 使用缓存穿透保护机制。在缓存中存储空对象或者特殊标记，当缓存中的值是空时，不再继续访问底层存储系统，而是直接返回空结果，从而防止大量请求穿透到存储系统。 分布式锁。在分布式系统中，使用分布式锁可以确保在集群环境中只有一个节点能够执行缓存失效时的数据加载操作，防止多个节点同时加载相同数据。 redis序列化问题 在使用Spring提供的Spring Data Redis操作redis必然需要使用Spring提供的模板类RedisTemplate。我们进入RedisTemplate类的源码中，如下图。红框框起来的部分即为RedisTemplate序列化器相关属性，都是为空的。这四个序列化属性主要是对key，value，hashkey，hashvalue进行序列化。我们经常需要将POJO对象存储到redis中，通常会使用JSON的方式序列化为字符串存储到redis中。\n如果不设置这四个序列化器，那么RedisTemplate将会采用默认的JDK序列化。我们可以来看看JDK序列化的效果。我们有如下这样一个测试类和用户类，目的是为了向redis中写入对象user。得到的结果如图：\n/*----User类----*/ @Data @NoArgsConstructor @AllArgsConstructor @JsonIgnoreProperties(ignoreUnknown = true) public class User implements Serializable { private String name; private String password; } /*----测试类----*/ @SpringBootTest @RunWith(SpringRunner.class) public class SpringTestClass { @Resource private RedisTemplate redisTemplate; @Test public void testDefaultSerializer() { User user = new User(\u0026#34;Tom\u0026#34;, \u0026#34;123456\u0026#34;); redisTemplate.opsForValue().set(\u0026#34;user\u0026#34;, user); redisTemplate.opsForValue().set(\u0026#34;string\u0026#34;, \u0026#34;abcd\u0026#34;); } } 使用默认序列化器插入对象 可以看到，在redis的key不仅有我们指定的key，user。还有前导的一些类似16进制的数字，并且存入的 value内容也不够简洁，附带了一些不需要的属性。我们可以再来试试原始的字符串类型。结果如图。可以观察到，如果不知道你原始的值是什么，看着这些乱码根本无法识别。\n使用默认序列化器插入字符串 为了解决这些问题，我们需要实现自己的序列化器，来达到简便且辨识度高的特点。自定义序列化器可以这样定义\n@Configuration(\u0026#34;redisConfig\u0026#34;) public class RedisConfig\u0026lt;V\u0026gt; { @Bean public RedisTemplate\u0026lt;String, V\u0026gt; redisTemplate(RedisConnectionFactory factory) { RedisTemplate\u0026lt;String, V\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); template.setConnectionFactory(factory); // 设置key的序列化方式 template.setKeySerializer(RedisSerializer.string()); // 设置value的序列化方式 template.setValueSerializer(RedisSerializer.json()); // 设置hash的key的序列化方式 template.setHashKeySerializer(RedisSerializer.string()); // 设置hash的value的序列化方式 template.setHashValueSerializer(RedisSerializer.json()); template.afterPropertiesSet(); return template; } } 我们将原来默认的对象序列化方式改为了JSON的序列化方式，我们可以再来看看效果。如下图\n自定义序列化器的字符串结果 自定义序列化器的类对象结果 至此，为什么要进行redis的自定义序列化就很明白了，自定义序列化的数据不仅存储的冗余减少了，也不再有乱码出现。在存储的key中也没有了乱码。既节省了空间，也让数据的表示更清楚了，方便排错。\n写在最后 这篇文章简单介绍了一下redis，并对redis中会出现的三种缓存问题作了详细说明（也是因为易混淆，便于日后翻起来看看），至于实际操作和应用，有待日后精进了再实现一番。最后再简要介绍了redis自定义序列化器的问题，说明了为什么要使用自定义序列化。\n算是给redis开了个头，之后嘛，应该是想参悟什么再写什么了。\n参考 看完这篇大总结，彻底掌握Redis的使用！ - 知乎 (zhihu.com)\n什么时候使用Redis缓存_什么时候本地缓存什么时候用redis-CSDN博客\n防弹防线：彻底击败Redis缓存穿透问题【redis问题 一】-CSDN博客\nRedis缓存雪崩：预防、应对和解决方案【redis问题 二】-CSDN博客\nRedis缓存保卫战：拒绝缓存击穿的进攻【redis问题 三】-CSDN博客\n","date":"2024-03-30T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/redis%E4%BA%8C%E4%B8%89%E4%BA%8B%E4%B8%80/","title":"Redis二三事（一）"},{"content":"如题所述，本篇博文就是使用AOP方法实现参数校验，同时利用了自定义注解。第一次见到这个做法的时候觉得甚是巧妙，故此记录。\nAOP 首先解释一下什么是AOP，AOP（Aspect Orient Programming）是spring框架中的一个重要部分。直译过来就是面向切面编程，这是一种编程思想，作为面向对象的一种补充。其本质就是在不改变源代码的情况下给程序或一组程序动态统一的添加额外功能。\n用一个具体的例子来讲，假如我们有如下这么个类，用来实现简单的四则运算。\n@NoArgsConstructor @AllArgsConstructor public class Operation { private int num1; private int num2; public int add() { return num1 + num2; } public int sub() { return num1 - num2; } public int mul() { return num1 * num2; } public int div() { return num1 / num2; } } 如果我们希望在每个方法运行前打印出一行日志信息，很容易想到的就是在操作类中的每个方法返回之前加上 logger.info()，虽然很简单，但是当源代码无法修改或者十分繁杂的时候，工程量将会呈几何倍数增加。此时会更希望将这个日志方法抽象为一个类，当每个方法执行时，能够自动的在方法返回之前打印出日志。如下图，AOP其实就是由切面对象和目标对象组成的代理对象。\nAOP示意图 自定义注解实现参数校验 假定有以下这个用户类（ User），它有两个字段 name和 password，希望实现对用户名的非空判断和对密码的长度判断。\n@Data @NoArgsConstructor public class User { private String name; private String password; } 直观的做法就是在控制器方法上，对获得的用户对象或者是相应的请求参数进行if的条件判断。诚然，这十分有效，但如果换个角度，需要进行参数校验的类不止有User，且控制器方法也不止一个。那么还要一个一个的去添加if条件吗？甚至当控制器方法无法修改时呢？于是，使用一个类或者是一个注解抽象出这个功能就显得一劳永逸了。这也就是AOP强大的解耦能力。\n注解的定义 顺着上面的思路，需要拦截的就是控制器方法，首先需要的就是一个用于标识方法的注解，当方法有这个注解时进入拦截器中执行参数校验过程。这个注解可以这么定义。\n@Retention(RetentionPolicy.RUNTIME) @Target({ElementType.METHOD, ElementType.TYPE}) public @interface GlobalInterceptor() { boolean checkParam() default false; // 校验参数，默认为false } 再思考这样的问题，对于控制器方法的每个参数，校验的规则一样吗？每个参数都需要校验吗？显然不是，因此需要再定义一个参数注解来实现更加自适应的功能。这个注解可以根据实际需要来实现。在这里实现了长短判断和是否必须的功能。如下：\n@Retention(RetentionPolicy.RUNTIME) @Target({ElementType.FIELD, ElementType.PARAMETER}) public @interface VerifyParam { int min() default -1; // 最大长度 int max() default -1; // 最小长度 boolean required() default false; // 是否必须 } 切面类的实现 在写完了注解之后，接下来就该实现切面类来使用这个注解了。我们希望能对拥有这个注解的方法实现拦截，自然可以想到切点就是这个注解。而参数校验应该要在方法执行之前实现，因此可以定义如下一个切面类：\npublic class GlobalOperationAspect { @PointCut(\u0026#34;@annotation(com.example.annotation.GlobalInterceptor)\u0026#34;) // 注解的全类名 private void requestInterceptor() {} @Before(\u0026#34;requestInterceptor()\u0026#34;) public void interceptorDo(JoinPoint point) throws Exception { // TODO } } 之后需要自然是要获取方法的参数类型，参数值，并考虑参数是否具有之前定义的 VerifyParam注解来考虑对应的判断规则。但是从切面类中，我们只有一个连接点，此时要获取这些信息只能通过反射。\n我们利用 point反射获取控制器对象的实例类，被拦截方法的参数值，方法名等。之后再考虑对被拦截的方法的参数依次校验。于是我们可以完善interceptorDo方法。\n@Before(\u0026#34;requestInterceptor()\u0026#34;) public void interceptorDo(JoinPoint point) throws Exception { try { Object target = point.getTarget(); // 获取被代理的目标对象 Object[] args = point.getArgs(); // 获取被拦截方法的参数值 String methodName = point.getSignature().getName(); // 获取方法名 Class\u0026lt;?\u0026gt;[] parameterTypes = ((MethodSignature) point.getSignature()).getMethod().getParameterTypes(); Method method = target.getClass().getMethod(methodName, parameterTypes); GlobalInterceptor interceptor = method.getAnnotation(GlobalInterceptor.class); if (interceptor == null) { // 此判断也可不做 return; } if (interceptor.checkParam()) { // 如果需要校验，才进行校验，否则可能添加了注解但是不校验 validateParam(method, args); } } catch (Exception e) { throw e; } } 接下来讲讲 validateParam方法。这个方法是实现被拦截方法参数校验的，自然而然我们需要得到参数值以及更为重要的参数注解 （从注解中才能获得校验规则）。依旧是通过反射来实现。\nprivate void validateParam(Method method, Object[] args) throws Exception { Parameter[] parameters = method.getParameters(); // 参数列表信息 for (int i = 0; i \u0026lt; parameters.length; ++i) { Parameter parameter = parameters[i]; Object value = args[i]; // 参数值 VerifyParam verifyParam = parameter.getAnnotation(VerifyParam.class); if (null == verifyParam) { continue; } checkValue(value, verifyParam); } } private void checkValue(Object value, VerifyParam verifyParam) { boolean isEmpty = value == null || value.toString().length() == 0; int length = value == null ? 0 : value.toString().length(); // 校验是否为空 if (isEmpty \u0026amp;\u0026amp; verifyParam.required()) { throw new IllegalArgumentException(); } // 校验长度 if (!isEmpty \u0026amp;\u0026amp; (verifyParam.min() != -1 \u0026amp;\u0026amp; length \u0026lt; verifyParam.min() || verifyParam.max() != -1 \u0026amp;\u0026amp; length \u0026gt; verifyParam.max())) { throw new IllegalArgumentException(); } } 至此，切面类的实现也就完成了。\n效果 最后当然是要试试效果了，附上简单的控制器方法。\n@RequestMapping(\u0026#34;/test\u0026#34;) @GlobalInterceptor(checkParam = true) public ResponseVO\u0026lt;User\u0026gt; access(@VerifyParam(required = true) String name, @VerifyParam(required = true, min = 8, max = 18) String password) { ResponseVO\u0026lt;User\u0026gt; responseVO = new ResponseVO\u0026lt;\u0026gt;(); User user = new User(); user.setName(name); user.setPassword(password); responseVO.setStatus(\u0026#34;success\u0026#34;); responseVO.setInfo(\u0026#34;校验通过\u0026#34;); responseVO.setData(user); return responseVO; } 当我们的控制器方法不加自定义的注解时，发送请求不符合规则请求的结果如下，\n不拦截结果 可以看见，请求参数被正常接收了，当我们加上参数时\n拦截结果 响应发生了变化，不再接收不符合规则的参数了。（由于只抛出了异常，因此响应比较简单）。\n写在最后 本文只是简单介绍了使用自定义注解和AOP功能来实现参数校验和拦截的功能，整体的思路如图：\n自定义注解拦截大致过程 在这个思路的基础上，可以实现更为复杂的校验，以及更为完善的响应。\n写下这篇博文旨在能够加强一些印象，也是这种方法带来的惊艳所驱使。\n","date":"2024-03-26T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8aop%E5%AE%9E%E7%8E%B0%E5%8F%82%E6%95%B0%E6%A0%A1%E9%AA%8C/","title":"优雅的使用AOP实现参数校验"},{"content":"虽然总是隔了很久才会更新一篇博客，但是一旦更新便会有图片到底放哪的烦恼，用过一段时间的在线图床，但终究不是个长久之计。也便有了这篇博文的诞生。\n搭建过程 首先在GitHub上创建一个新的仓库，至于怎么创建这里就不赘述了。大家自行bing即可\n仓库创建好之后就是我们之后上传图片的地方了，也就是图床。之后要考虑的是如何上传图片，这里用到了一个PicGo这个软件，可以通过这个篝火到达它的官网，之后根据需要下载相应的版本即可。之后按照以下步骤即可\n从GitHub中创建一个token，打开路径为 setting-\u0026gt;Developer settings-\u0026gt;Personal access tokens，这里我们选用 Tokens(classic)就行。（其实是我不知道另外一个是干啥用的）。\n在生成token界面中，Note用来表示你对这个token使用的描述。主要的还是红框部分，由于这个token会长期使用，而且是私有的这里设置为无限期也问题不大。然后再勾选repo，最后就可以点击generate了。 要注意的是token生成之后只会显示一次，因此要及时复制，否则你就只能再来一次了\n生成token界面 配置PicGo，依次打开 图床设置 -\u0026gt; GitHub，如下图。 进行仓库相应配置。在图床配置名上写个名字，仓库名的设置为 GitHub用户名/作为图床的仓库名，分支名使用 main即可，token就是上文刚刚得到的那一串字符。剩下的就不是必选项了。 配置项 之后你只要在软件首页选择相应的图床就能够上传图片了。 最后 由于GitHub访问具有一定限制，这里选择使用jsdelivr对图床的访问进行加速。方法很简单，只需要对博客的图片访问路径替换为 https://\u0026lt;jsDelivr加速域名\u0026gt;/gh/\u0026lt;用户\u0026gt;/\u0026lt;项目\u0026gt;@\u0026lt;版本\u0026gt;/\u0026lt;资源路径\u0026gt;，例如我的一个图片路径就可以写为 https://cdn.jsdelivr.net/gh/MUNLELEE/image-hosting/blog/20240325213310.png。\n同时可以在PicGo上面写上自定义域名 https://cdn.jsdelivr.net/gh/GitHub用户名/仓库名，这样，上传图片后PicGo就会在剪贴板上写上jsdelivr加速的路径了。如图\n参考 如何利用 Github 搭建自己的免费图床？ - 知乎 (zhihu.com)\n通过jsDelivr实现Github图床CDN加速 | XieJava\u0026rsquo;s blog (ishareread.com)\n折腾完这些之后发现，好像用git命令上传图片也不是不行。\n算了，聊胜于无吧。\n","date":"2024-01-05T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E5%88%A9%E7%94%A8github%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E5%9B%BE%E5%BA%8A/","title":"利用GitHub搭建一个简易图床"},{"content":"前言 近日有了解社区发现相关内容以及实现这个比较古老的算法的需要，索性就了解一下社区发现，并尝试着复现这个算法。\n社区发现概述 什么是社区 在现代人们的日常生活中，每个人都会进行社交，这在无形中就形成了一个社交网络。在这个网络中，每个人可以视为一个点，而用户之间的点赞、关注以及其他行为形成了边。在这样的一个网络中，有的用户之间联系较为紧密，有的用户之间联系较为稀疏。联系较为紧密的几个用户可以形成一个社区，在社区内，每个节点的链接都较为紧密，而两个社区之间的联系就较为稀疏。整体的结构可以被称为社团结构。如下图，圈起来的部分即为社区。\n社团中的边结构其实是一种逻辑抽象，并不是一种空间位置上的关系，而是节点之间的共有关系，假如节点代表的是消费者，那么节点间的边则可能代表消费者购买了共同一类商品，边的权重则可以代表购买物品的数量。\n社区发现的目的及简单思路 社区发现的目的就是为了在图中找到具有一定共同关系或者潜在特定关系的组织，也就是社区。社区发现也就是为了寻找图网络中联系较为紧密的社区也称为块（cluster）。基于这种想法，我们可以想到如果要进行社区发现那么就要寻找一个联系较为紧密的块，我们可以将其解释为具有较大的密度，之后便是考虑用什么方法去评估这种密度，类似于神经网络中的损失函数。\nCPM算法 CPM（派系过滤算法）算法是最早的重叠社区发现算法，它的思想是基于团渗透的，这个算法认为社区是具有共享节点的全连通子集集合，并通过一种团过滤算法来识别网络中的社区结构。在这个算法中遵循以下两个概念\n在图网络中视为团的部分是任意两个节点都存在一条边相连的，也就是完全子图 所有彼此连通的k-团（拥有k个节点）构成一个k-团社区，当两个k-团之间存在$k-1$个节点共享那么认为这两个k-团连通 算法步骤 算法首先需要找到网络中的所有团，并构建一个用来表示团团重叠情况的矩阵（matrix），在这个矩阵中matrix[i][j]表示网络中第i个团和第j个团之间的公共节点数。 根据给定的参数k，将矩阵（matrix）中对角线上小于$k$的元素和非对角线上的小于$k-1$的元素置零，其他元素置为一，这样所有对角线为1的即为k-团，而非对角线为1的即为两个相邻的k-团。 将相邻的k-团合并为一个较大的团社区，并可以使用模块度进行评价。 从上面可以得知算法中首先需要做到是得到k-团，因此为了提高效率，在这里需要使用Born_Kerbosch算法来寻找图中的团。在这里也简单介绍一下这个算法。\nBorn_Kerbosch算法 算法的初始化包括了以下三个集合\nR集合：记录当前极大团中已经加入的点 P集合：记录可能可以继续加入极大团中的点（这些点应该与R集合中的所有点都相连） X集合：记录已经加入过极大团的点（用于判重，因为会从每个节点开始，枚举所有的团） 简单流程：\n对于每一个在集合P中的点v，将v加入R集合中，之后更新P集合，确保集合中的节点与v相连。 进行回溯时，将v节点从P集合中取出，并加入X集合，表示包含v节点的极大团已经寻找完毕了 当R集合满足为极大团时，P集合和X集合必须为空。因为P集合中包含的点是可能加入R集合中的点，同样的X集合中的点也与R集合中的点都相邻，因此也属于可能称为R集合中极大团点的情况。 算法缺点 CPM算法较为简单，但存在不能为单节点分配社团以及比较使用于完全子图较多的网络中的问题，也即边密集网络中，在稀疏网络中算法的效率较低。\n模块度 模块度是评估一个社区网络划分好坏度量方法，其含义为社区内节点的边数与随机情况下的边数之差。定义如下：\n$$Q=\\frac{1}{2m} \\sum_{i, j}[A_{i j}-\\frac{k_i k_j}{2m}]\\delta(c_i, c_j) \\qquad \\delta(u, v)=\\begin{cases}1,\u0026amp;u==v \\newline 0, \u0026amp;else \\end{cases}$$\n其中$A_{i j}$是节点$i$和节点$j$之间边的权重，当图网络不带权时，可以将其视为1；$k_i=\\sum_j{A_{ij}}$表示所有与节点$i$相连的边的权重之和；$c_i$表示节点$i$所属的社区；$m={1\\over2}\\sum_{ij}A_{ij}$表示所有边的权重之和。$\\frac{k_i k_j}{2m}$表示随机情况下节点$i$与节点$j$之间产生的边。\n对这个公式做进一步简化可以得到如下的公式：\n$$Q=\\frac{1}{2m}\\sum_c{[\\sum{in}-\\frac{{(\\sum{tot})}^2}{2m}]}$$\n其中$\\sum{in}$表示社区$c$内边的权重之和，$\\sum{tot}$表示与社区$c$节点相连的边的权重之和。\n可以这么简单的理解模块度，即社区内部边的权重和减去社区外部与社区内部相连的边的权重和。\n算法实现 在这部分将讲述算法实现的一些较为重要的地方，完整的代码可以点此，需要的朋友可以直接前往。\n实现算法要先实现寻找极大团，这里使用Born_Kerbosch算法，这个算法有两个版本，但我这里采用了经过一些剪枝处理的版本，在递归之前需要先确定需要的枢纽元素，数据结构方面采用集合（自动去重属实是太好用了^_^） /** * 使用BornKerbosch算法寻找最大团 * @param R 存在于极大团中的点 * @param P 可能可以加入极大团的点 * @param X 用于判重的点集合 * @param ans 将最后寻找到的所有极大团保存 * @param neighbor 存储每个节点的邻居节点，是一个map */ def bornKerbosch(R: Set[VertexId], P: Set[VertexId], X: Set[VertexId], ans: Set[Set[VertexId]], neighbor: Map[VertexId, Set[VertexId]]): Unit = { if (P.toList.isEmpty \u0026amp;\u0026amp; X.toList.isEmpty) { // 递归退出的条件 ans.add(R) } else { val pivot: VertexId = P.union(X).head // 枢纽元素 val nu: mutable.Set[VertexId] = neighbor(pivot) for (v \u0026lt;- P.diff(nu)) { // 取一个顶点 val vNu: mutable.Set[VertexId] = neighbor(v) // 所取顶点的邻居节点集合 bornKerbosch(R + v, P.intersect(vNu), X.intersect(vNu), ans, neighbor) P.remove(v) X.add(v) } } } 之后我觉得较为关键的是合并社团，直观上合并并不难，只需要创建出CPM算法所需要的矩阵并按照矩阵数值和矩阵索引进行合并即可。比较难的过程可能是如何用数据结构将这些过程保存下来。在算法实现中我使用了二维Set来保存最后的合并的社区顶点。创建矩阵时采用数组嵌套Set的方式保存每个社区应该和哪些索引的社区合并。 /** * 记录应该合并的团，也就是得到cpm算法的矩阵 * @param community 传入的所有的社团集合 * @return 得到的是与社团集合相对应的集合，其中每个存放的是相应社团与哪些索引社团应该合并 */ def cpmMatrix(community: Set[Set[VertexId]]): Array[Set[Int]] = { // 创建矩阵，因为如果是真正的矩阵空间占用较大，这里选择创建集合型矩阵 val matrix: Array[mutable.Set[Int]] = new Array[mutable.Set[Int]](community.size) var i: Int = 0 community.foreach(item =\u0026gt; { var j: Int = 0 matrix(i) = Set() community.foreach(item2 =\u0026gt; { // 两者之间交集 val lap: Int = item.intersect(item2).size if (i == j \u0026amp;\u0026amp; lap \u0026gt;= config.kVal) { // 表示对角线，如果是对角线要大于k matrix(i).add(j) } if (i != j \u0026amp;\u0026amp; lap \u0026gt;= config.kVal - 1) { // 表示非对角线，需要大于k-1 matrix(i).add(j) } j += 1 }) i += 1 }) matrix } /** * 进行合并操作，将需要合并的社团所有顶点放在集合中 * @param community 使用BK算法划分好的极大团集合 * @param idxStore 存储每个社团和哪些社团（索引）合并 * @return 返回所有合并后的社团，每个社团的顶点放在一个集合里 */ def mergeAll(community: Set[Set[VertexId]], idxStore: Array[Set[Int]]): Set[Set[VertexId]] = { val tmpSeq: Seq[mutable.Set[VertexId]] = community.toSeq // 转换为可索引 val res: Set[Set[VertexId]] = Set() // 返回结果集合 val lenS: Int = idxStore.length for (i \u0026lt;- 0 until lenS) { var tmpSet: Set[VertexId] = Set() // idx变量是此集合和索引的哪个集合并集 idxStore(i).foreach(idx =\u0026gt; tmpSet = tmpSet.union(tmpSeq(idx))) res.add(tmpSet) } res } 遗憾的是没有实现相关的评估手段，待日后想起或再次需要的时候再进行修改吧。\n","date":"2023-02-01T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E7%A4%BE%E5%8C%BA%E5%8F%91%E7%8E%B0-cpm%E7%AE%97%E6%B3%95/","title":"社区发现 | CPM算法及其实现"},{"content":"这篇博文应该是这个应用的最后一个部分，主要是实现调用本地麦克风录音的前后端。\n前端页面的实现 在这个页面中，需要实现录音功能，而录音功能在参考了网上的相关资料之后，决定采用recorder.js的现行库（因为如果要造轮子的话需要自己去了解语音采样等过程），这是库的链接。\n对于我这种前端小白本来是没看懂要怎么使用的，后来才发现在其项目目录下有dist/recorder.js文件，如果你使用是Flask框架只需要在static文件夹中导入recorder.js以及recorder.js.map两个文件并采用如下方法引入\n\u0026lt;script type=\u0026#34;text/javascript\u0026#34; src=\u0026#34;{{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;recorder.js\u0026#39;) }}\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 通过recorder我决定设置录音相关的六个按钮分别是开始、暂停、继续、结束、上传、播放。于是就有了以下的HTML代码\n\u0026lt;body style=\u0026#34;background-size: 100%; background-image:url({{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;cool-2.png\u0026#39;) }})\u0026#34;\u0026gt; \u0026lt;div id=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;麦克风录制\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;开始录制\u0026#34; id=\u0026#34;record_btn\u0026#34; class=\u0026#34;item_btn_left\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;暂停录制\u0026#34; id=\u0026#34;stop_btn\u0026#34; class=\u0026#34;item_btn_left\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;继续录制\u0026#34; id=\u0026#34;resume_btn\u0026#34; class=\u0026#34;item_btn\u0026#34;\u0026gt; \u0026lt;br\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;结束录制\u0026#34; id=\u0026#34;end_btn\u0026#34; class=\u0026#34;item_btn_left\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;播放录音\u0026#34; id=\u0026#34;play_btn\u0026#34; class=\u0026#34;item_btn_left\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;上传录音\u0026#34; id=\u0026#34;submit_btn\u0026#34; class=\u0026#34;item_btn\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div\u0026gt; \u0026lt;p\u0026gt;识别文本\u0026lt;/p\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;res_text\u0026#34; disabled=\u0026#34;disabled\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; value=\u0026#34;返回主页\u0026#34; onclick=\u0026#34;back()\u0026#34; id=\u0026#34;back_btn\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; 前端CSS样式 没有什么CSS基础又想让界面好看一点的我，在这个页面的按钮样式上参考了其他大佬的所写的CSS代码样式，并修改了其大小和变化时间长短，所以这里并没有太多可以讲述的东西。直接附上相关链接以及我修改后的CSS代码\n.item_btn { display: flex; align-items: center; justify-content: center; height: 50px; width: 100px; } .item_btn_left { display: flex; align-items: center; justify-content: center; height: 50px; width: 100px; float: left; margin-right: 55px; } #back_btn { display: flex; align-items: center; justify-content: center; height: 50px; width: 100px; left: 75%; margin-top: 100px; } #back_btn, .item_btn, .item_btn_left { border: 0; border-radius: 10px; background: #2ec4b6; text-transform: uppercase; color: white; font-size: 16px; font-weight: bold; padding: 15px 30px; outline: none; position: relative; transition: border-radius 1.5s; -webkit-transition: border-radius 1.5s; } #back_btn:hover, .item_btn:hover, .item_btn_left:hover { border-bottom-right-radius: 50px; border-top-left-radius: 50px; border-bottom-left-radius: 10px; border-top-right-radius: 10px; } 通过以上的CSS可以实现如下的鼠标悬停效果\n按钮悬停效果 前端功能实现 实现基础的录音功能 参阅recorder.jsAPI可以发现这个库提供基础的录音功能，且实现较为简单。以下为使用API的jQuery代码，用来实现相关的功能。这里需要注意的是当使用了jQuery中的on函数来绑定相关的事件时，就不需要在HTML元素中使用属性onclick来绑定相关函数了，这个问题一度折磨了我许久，最后还是感谢好友的修改。\n\u0026lt;script\u0026gt; window.onload = function() { let recorder; $(\u0026#34;#record_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function (e) { console.log(\u0026#34;开始录制\u0026#34;); recorder = new Recorder({ sampleBits: 16, sampleRate: 16000, numChannels: 1 }); recorder.start().then( () =\u0026gt; {// 开始录音 }, (error) =\u0026gt; {// 出错了 console.log(`${error.name} : ${error.message}`); }); }); $(\u0026#34;#stop_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function () { console.log(\u0026#34;暂停录制\u0026#34;); recorder.pause(); }); $(\u0026#34;#resume_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function () { console.log(\u0026#34;继续录制\u0026#34;); recorder.resume(); }); $(\u0026#34;#end_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function () { console.log(\u0026#34;结束录制\u0026#34;); recorder.stop(); }); $(\u0026#34;#play_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function () { console.log(\u0026#34;播放\u0026#34;); recorder.play(); }); \u0026lt;/script\u0026gt; 实现录音上传的功能 要实现录音上传首先我们要先获取录音文件，recorder.js库提供了相应的API（getWAVBlob）得到录音文件的.wav形式的文件。\n要上传这个文件就需要通过AJAX技术来上传，此时需要创建一个FormData来提交这个文件，通过append函数来接收这个文件，并将其命名为audio.wav。最后创建一个AJAX对象来上传这个文件。\n在这个AJAX对象中，我们需要将提交方法设置为POST，并指定一个url来作为提交的目的。在这里有一个参数processData默认值为true，表示在上传文件时，jQuery会对数据进行序列化处理，在使用true为参数值时，产生了提交文件的错误，所以这里建议将参数设置为false。\n最后通过recorder.js的API将录音文件删除，以便下一次调用。JS代码如下\n$(\u0026#34;#submit_btn\u0026#34;).on(\u0026#39;click\u0026#39;, function () { let wav_blob = recorder.getWAVBlob(); let formData = new FormData(); formData.append(\u0026#39;file\u0026#39;, wav_blob, \u0026#39;audio.wav\u0026#39;); {# 添加文件到表单中 #} $.ajax({ type: \u0026#34;POST\u0026#34;, url: \u0026#34;/micro_upload\u0026#34;, data: formData, cache: false, processData: false, contentType: false, success: function (data) { if (data.msg === \u0026#39;success\u0026#39;) { recorder.destroy().then(function () { recorder = null; }); } else { console.log(data.msg); recorder.destroy().then(function () { recorder = null; }); } } }) }) 后端接收 在后端接收中，我们需要将上述对应的url利用装饰器映射到相应的接收函数中，在这个函数中可以参考之前的接收文件函数，提交的文件将保存在Flask中的request.file中。\n利用secure_filename函数对文件名进行保留，最后保存至相应的工程目录中。相关代码如下\n@app.route(\u0026#39;/micro_upload\u0026#39;, methods=[\u0026#39;post\u0026#39;]) def micro_upload(): audio_input = request.files[\u0026#39;file\u0026#39;] filename = secure_filename(audio_input.filename) audio_input.save(os.path.join(app.config[\u0026#39;UPLOAD_PATH\u0026#39;], filename)) return 页面实现效果 页面效果 小结 至此，此应用的框架基本完成，最后的识别部分需要等到算法实现后再行融合。\n","date":"2022-04-11T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/flask%E5%89%8D%E5%90%8E%E7%AB%AF%E5%BA%94%E7%94%A8%E4%B8%89/","title":"Flask实现Web应用（三）"},{"content":"书接上回，这篇博文主要是要讲一讲前端页面以及相应的css的变化，虽然是一个简单的web应用，但是在一定程度上还是需要符合人的审美。\n基础页面 由于我需要实现两个功能，调用本地麦克风录制和上传录音文件，最开始我是想把这两个功能放在同一个页面，但是最后觉得为了美观和页面逻辑，我就决定设计一个主页面，包含了这两个功能的可选项，之后再通过选择重定向至另一个页面。\n因此在基础页面中就有了如下的HTML代码\n\u0026lt;body style=\u0026#34;background-size: 100%; background-image:url({{ url_for(\u0026#39;static\u0026#39;, filename=\u0026#39;cool-background.png\u0026#39;) }})\u0026#34;\u0026gt; \u0026lt;div class=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;p\u0026gt;选择语音识别方式\u0026lt;/p\u0026gt; \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; id=\u0026#34;choose_btn1\u0026#34; value=\u0026#34;上传文件\u0026#34; class=\u0026#34;choose_btn\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;item\u0026#34;\u0026gt; \u0026lt;input type=\u0026#34;button\u0026#34; id=\u0026#34;choose_btn2\u0026#34; value=\u0026#34;麦克风录音\u0026#34; class=\u0026#34;choose_btn\u0026#34;\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 当然一个网页好看的背景是不可缺少的，在这里可以给各位有前端需求的朋友推荐一个网站炫酷背景，这个网站可以在线生成一些简单的可以用作网页背景的图片并提供免费下载。\n当你使用Flask框架并打算引用外部文件时，你只能将文件放在static文件夹下，并采用url_for函数来作为引用的链接。\n按钮链接 可以看见在这个基础页面中有两个按钮，通过这两个按钮的点击重定向至其他页面。这时候需要用到jQuery来判断被点击的是哪个按钮（最开始想用JS实现，但是发现略微麻烦）。\n我们可以通过attr来获取相应HTML标签的属性值，通过属性值实现判断和重定向。判断条件满足时使用window.location.href将url导向至其他页面。并将这个函数绑定到按钮的点击事件上。\nwindow.onload = function () { $(\u0026#34;.choose_btn\u0026#34;).click(function () { const text = $(this).attr(\u0026#34;value\u0026#34;); if (text === \u0026#34;上传文件\u0026#34;) { window.location.href = \u0026#39;upload\u0026#39;; } else { window.location.href = \u0026#39;microphone\u0026#39;; } }); } 通过上面的JS，当点击某个按钮，浏览器的url将会改变，因此我们需要将将相应的url在Flask后端进行渲染。\n@app.route(\u0026#39;/upload\u0026#39;) def upload(): return render_template(\u0026#39;upload.html\u0026#39;) @app.route(\u0026#39;/microphone\u0026#39;) def microphone(): return render_template(\u0026#39;microphone.html\u0026#39;) 背景CSS样式 段落字体 这一块其实没什么好说的，就是将网站的默认字体修改了一下，设置了字体间距和文字居中\np { color: white; font-family: \u0026#34;Microsoft YaHei UI Light\u0026#34;; font-size: 20px; letter-spacing: 3px; /*文字间间距*/ text-align: center; /* 居中 */ } 卡片样式 我希望能够将两个功能的选择放在一个卡片上，这样不会显得一个页面上的元素过少，只有两个按钮元素毕竟难以撑起整个页面。同时修改了卡片的样式。\n通过background: rgba(0, 0, 0, .5)修改卡片的背景 利用属性border-radius来改变卡片四周的角 利用box-shadow属性来使卡片显得更立体 通过transform属性来让卡片位于居中位置 相应的CSS文件属性如下\n.items { position: absolute; top: 50%; left: 50%; width: 400px; height: 300px; padding: 40px; transform: translate(-50%, -50%); background: rgba(0, 0, 0, .5); box-sizing: border-box; box-shadow: 0 15px 25px rgba(0, 0, 0, .6); border-radius: 20px; } 卡片动画效果 通过以上的CSS打开网页时，卡片将会直接出现在页面上。这样显然是不是很美观的，在好友的帮助下实现了卡片的淡出效果。首先我们需要在CSS上设置opacity属性为透明，并设置相关的transition过渡。最后就是通过jQuery来修改网页加载后的CSS属性（之前不知到jQuery能操作CSS一直觉得没有逻辑可以实现这样的动态效果）。\n相关的CSS和JS的修改如下\n.items { position: absolute; top: 50%; left: 50%; width: 400px; height: 300px; padding: 40px; transform: translate(-50%, -50%); background: rgba(0, 0, 0, .5); box-sizing: border-box; box-shadow: 0 15px 25px rgba(0, 0, 0, .6); border-radius: 20px; opacity: 0; transition: opacity 1.5s ease-in; } window.onload = function () { $(\u0026#34;.items\u0026#34;).css(\u0026#39;opacity\u0026#39;, \u0026#39;1\u0026#39;); $(\u0026#34;.choose_btn\u0026#34;).click(function () { const text = $(this).attr(\u0026#34;value\u0026#34;); if (text === \u0026#34;上传文件\u0026#34;) { window.location.href = \u0026#39;upload\u0026#39;; } else { window.location.href = \u0026#39;microphone\u0026#39;; } }); } 当使用了以上的组合之后就会有打开网页之后卡片淡入的效果了。\n按钮悬停样式 为了能够让按钮在鼠标悬停的时候有一定的反应，就在按钮上加上一些鼠标悬停时的动态效果。\n悬停效果其实和上面的卡片动画效果异曲同工，只不过动画的最后效果采用CSS中的hover伪元素进行修改，而非采用JS进行修改。\n相关代码如下\n.item:before { content: \u0026#39;\u0026#39;; height: 4px; background-color: #BADFED; width: 100%; position: absolute; left: 0; bottom: 0; transform: scaleX(0); transition: .3s; } .item:hover:before { transform: scaleX(1); } 最后实现的效果如下：\n选择按钮动效 基础页面效果 页面效果 ","date":"2022-04-10T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/flask%E5%89%8D%E5%90%8E%E7%AB%AF%E5%BA%94%E7%94%A8%E4%BA%8C/","title":"Flask实现Web应用（二）"},{"content":"K近邻算法 K近邻是一种有监督学习算法。因为没有对数据进行训练，而是通过新数据与旧数据的比较得到相应的结果。因此是一种隐式的学习过程和训练过程。K近邻算法可以用来解决分类问题，也可以用来解决回归问题。\n步骤 对未知类别的属性的数据集中的每个点依次执行以下操作：\n计算已知类别数据集中点与当前点之间的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在的类别出现的频率 返回前k个点出现频率最高的类别作为当前点的预测分类 在确定样本和当前点的距离时，通常采用的是欧式距离公式$$d=\\sqrt{(x_1-x_0)+(y_1-y_0)}$$ 当公式中的指数变化时，随之也会得到相应的不同的距离公式。\n如下图所示的例子中\n1、当采用实线的圆作为k近邻的范围，也就是$k=3$时，此时与绿点距离更近的三个点中，三角形出现的频率更大，因此将绿点归为三角形一类\n2、当采用虚线的圆作为k近邻的范围，也就是$k=5$，时，此时与绿点距离更近的五个点中，正方形的频率更大，因此将绿点归为正方形一类。\n以下采用鸢尾花作为例子进行KNN测试\n代码 import numpy as np import pandas as pd from sklearn.datasets import load_iris import matplotlib.pyplot as plt iris = load_iris() df = pd.DataFrame(iris.data, columns=iris.feature_names) # print(df) df[\u0026#39;label\u0026#39;] = iris.target # print(len(df)) 150 # 绘散点图 # print(df.info()) # 显示数据类型 # 前两个特征 Colors = [] for i in range(df.shape[0]): item = df.iloc[i, -1] # 定位到标签 if item == 0: Colors.append(\u0026#39;black\u0026#39;) if item == 1: Colors.append(\u0026#39;red\u0026#39;) if item == 2: Colors.append(\u0026#39;orange\u0026#39;) plt.rcParams[\u0026#39;font.sans-serif\u0026#39;] = [\u0026#39;Simhei\u0026#39;] bgplt = plt.figure(figsize=(12, 8)) fig1 = bgplt.add_subplot(221) plt.scatter(df.iloc[:, 0], df.iloc[:, 1], marker=\u0026#39;.\u0026#39;, c=Colors) plt.xlabel(\u0026#39;sepal length\u0026#39;) plt.ylabel(\u0026#39;sepal width\u0026#39;) # 13两个特征 fig2 = bgplt.add_subplot(222) plt.scatter(df.iloc[:, 0], df.iloc[:, 2], marker=\u0026#39;.\u0026#39;, c=Colors) plt.xlabel(\u0026#39;sepal length\u0026#39;) plt.ylabel(\u0026#39;petal length\u0026#39;) # 34两个特征 fig3 = bgplt.add_subplot(223) plt.scatter(df.iloc[:, 2], df.iloc[:, 3], marker=\u0026#39;.\u0026#39;, c=Colors) plt.xlabel(\u0026#39;petal length\u0026#39;) plt.ylabel(\u0026#39;petal width\u0026#39;) plt.show() new_data = [5.7, 3.3, 6.2, 0.7] def KnnAlgorithm(data, target, k=3): tmp_list = list((((data.iloc[:150, 0:4] - target) ** 2).sum(1)) ** 0.5) dist_l = pd.DataFrame({\u0026#39;dist\u0026#39;: tmp_list, \u0026#39;label\u0026#39;: (data.iloc[:150, -1])}) # 加上标签 dist_sort = dist_l.sort_values(by=\u0026#39;dist\u0026#39;)[: k] # print(dist_sort) res = dist_sort.loc[:, \u0026#39;label\u0026#39;].value_counts() return res.index[0] print(KnnAlgorithm(df, new_data, k=4)) ","date":"2022-04-06T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/k%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95/","title":"机器学习算法 | K近邻"},{"content":"在DFS这一块一直是比较弱的，因此就去看了DFS相关的一些题目，并找到了岛屿的一系列问题。\n岛屿数量 岛屿的最大面积 最大人工岛 封闭岛屿的数目 岛屿的周长 图类DFS方法 图通常是由方格组成，通过方格中的元素来对图的DFS进行限制。岛屿问题便是其中经典的一类。在岛屿问题中，通常由$1$表示陆地，由$2$表示海洋，当每个方格都相邻时，组成的一个全$1$方格域即为一个岛屿。（这里不包含对角相邻）\nDFS框架 DFS也是一种意义上的递归，因此在一个DFS程序中首先要做的便是设定递归出口。类似于树结构可以利用指针为空等条件，在岛屿问题中，我们需要判断边界，也需要判断当前方格是否是陆地。因此在递归出口设置中就需要两个条件进行限制。\n之后DFS就要考虑下一步搜索与前一轮搜索之间的区别。岛屿问题中最主要的区别便是遍历方格的四个邻格，其余的区别便需要依题目而定。因此我们可以得到以下的岛屿类问题的DFS框架\nclass Solution { private: bool inRange(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } void dfs(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int pos_x, int pos_y) { if (!inRange(grid, pos_x, pos_y)) return; if (grid[pos_x][pos_y] != 1) return; dfs(grid, pos_x - 1, pos_y); dfs(grid, pos_x + 1, pos_y); dfs(grid, pos_x, pos_y + 1); dfs(grid, pos_x, pos_y - 1); } } 避免重复遍历 在DFS中一个重要的问题就是要避免重复遍历，不然可能会造成程序原地打转的现象。在岛屿一类的问题中，可以将已经遍历过的方块修改其元素为$0$或者为非题中给出元素。因此我们可以修改上述的DFS模板如下\nclass Solution { private: bool inRange(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } void dfs(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int pos_x, int pos_y) { if (!inRange(grid, pos_x, pos_y)) return; if (grid[pos_x][pos_y] != 1) return; grid[pos_x][pos_y] = 2; dfs(grid, pos_x - 1, pos_y); dfs(grid, pos_x + 1, pos_y); dfs(grid, pos_x, pos_y + 1); dfs(grid, pos_x, pos_y - 1); } }; 岛屿问题解法 岛屿数量问题 其实粗看题目，很容易能想到并查集能够解决这类问题，但是DFS应该较为常见的解法，这道题也是最为简单的DFS。\n我们可以这么想，当你使用DFS时，每次遍历到的都是相邻的方格，因此在一次DFS过程中，遍历过的所有陆地应该是属于同一块岛屿的，而在遍历的过程中我们应当将已走过的陆地更改为不可走。这样一来就会使之前的岛屿失效。\n那么如何寻找下一个岛屿呢？答案也就是寻找下一个在地图中尚未使用过的$1$，之后在对这个$1$进行DFS，也就可以得到另一个岛屿了。\n因此要想获得岛屿的数量，其实就是在整个地图执行DFS算法的次数。\nclass Solution { private: bool inRange(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } void dfs(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid, int pos_x, int pos_y) { if (!inRange(grid, pos_x, pos_y)) return; if (grid[pos_x][pos_y] != \u0026#39;1\u0026#39;) return; grid[pos_x][pos_y] = \u0026#39;2\u0026#39;; dfs(grid, pos_x - 1, pos_y); dfs(grid, pos_x + 1, pos_y); dfs(grid, pos_x, pos_y + 1); dfs(grid, pos_x, pos_y - 1); } public: int numIslands(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; grid) { int row = grid.size(); int col = grid[0].size(); int res = 0; for (int i = 0; i \u0026lt; row; ++i) { for (int j = 0; j \u0026lt; col; ++j) { if (grid[i][j] == \u0026#39;1\u0026#39;) { ++res; dfs(grid, i, j); } } } return res; } }; 岛屿的最大面积 这个问题和上一个问题其实本质上没什么区别，你都需要在整个地图中去遍历出所有的岛屿，无非需要的就是在DFS的过程中，你需要统计已走过的陆地数量。当遍历完所有的岛屿之后，取这些值中的最大值即可。\n因此在这个问题中，我们只需要在每次进入DFS函数中，如果走过的是陆地，那么就对其面积（area）$+1$即可。（使用C++时，可以传递引用避免无法改变形参的问题）。\nclass Solution { private: bool inRange(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } void dfs(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int pos_x, int pos_y, int\u0026amp; area) { if (!inRange(grid, pos_x, pos_y)) return; if (grid[pos_x][pos_y] != 1) return; ++area; grid[pos_x][pos_y] = 2; dfs(grid, pos_x - 1, pos_y, area); dfs(grid, pos_x + 1, pos_y, area); dfs(grid, pos_x, pos_y + 1, area); dfs(grid, pos_x, pos_y - 1, area); } public: int maxAreaOfIsland(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { int row = grid.size(); int col = grid[0].size(); int res = 0; for (int i = 0; i \u0026lt; row; ++i) { for (int j = 0; j \u0026lt; col; ++j) { if (grid[i][j] == 1) { int area = 0; dfs(grid, i, j, area); res = max(res, area); } } } return res; } }; 岛屿的周长 这道题要求的是岛屿的周长，这道题最终是在题解的启发下才做出来的。\n对于一张地图，如果我们遍历到一个海洋结点（$0$），这时候表明岛屿有一条边与其相接，而当我们遍历出地图时，也表明有一条边与岛屿相接，相反，如果是陆地，则对岛屿的周长没有影响。\n所以我们只需要在DFS中分以上三种情况进行讨论，将最后的结果相加即可得到岛屿的周长。\nclass Solution { private: bool inRange(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } int dfs(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int pos_x, int pos_y) { if (!inRange(grid, pos_x, pos_y)) return 1; if (grid[pos_x][pos_y] == 0) return 1; if (grid[pos_x][pos_y] != 1) return 0; // 说明这个方格已经是遍历过的陆地 grid[pos_x][pos_y] = 2; return dfs(grid, pos_x - 1, pos_y) + dfs(grid, pos_x + 1, pos_y) + dfs(grid, pos_x, pos_y - 1) + dfs(grid, pos_x, pos_y + 1); } public: int islandPerimeter(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { int row = grid.size(); int col = grid[0].size(); int res = 0; for (int i = 0; i \u0026lt; row; ++i) { for (int j = 0; j \u0026lt; col; ++j) { if (grid[i][j] == 1) { res += dfs(grid, i, j); } } } return res; } }; 封闭岛屿的数目 这道题与求岛屿的数目有点像，稍有不同的是，当岛屿以地图边界为边时，此时这个岛屿不算是封闭的。\n因此我们可以自然的想到，利用DFS将不算封闭的边界岛屿找出来，而在这个算法执行的过程中就会将这些岛屿修改为海洋，因此剩下的岛屿就全为封闭岛屿，计算这些剩下的岛屿即可。\nclass Solution { private: constexpr static int dx[4] = {0, -1, 0, 1}; constexpr static int dy[4] = {1, 0, -1, 0}; bool inRange(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int x, int y) { if (x \u0026gt;= 0 \u0026amp;\u0026amp; x \u0026lt; grid.size() \u0026amp;\u0026amp; y \u0026gt;= 0 \u0026amp;\u0026amp; y \u0026lt; grid[0].size()) { return true; } return false; } void dfs(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid, int pox, int poy) { if (!inRange(grid, pox, poy)) return; if (grid[pox][poy] != 0) return; grid[pox][poy] = 1; for (int i = 0; i \u0026lt; 4; ++i) { dfs(grid, pox + dx[i], poy + dy[i]); } } public: int closedIsland(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grid) { int res = 0; int row = grid.size(), col = grid[0].size(); // 优先处理边界，将边界岛屿剔除 for (int i = 0; i \u0026lt; row; ++i) { dfs(grid, i, 0); dfs(grid, i, col - 1); } for (int j = 0; j \u0026lt; col; ++j) { dfs(grid, 0, j); dfs(grid, row - 1, j); } for (int i = 0; i \u0026lt; row; ++i) { for (int j = 0; j \u0026lt; col; ++j) { if (grid[i][j] == 0) { ++res; dfs(grid, i, j); } } } return res; } }; ","date":"2022-04-02T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E5%B2%9B%E5%B1%BF%E9%97%AE%E9%A2%98/","title":"岛屿问题 | DFS框架"},{"content":"为了让之后的项目能够专注于算法，于是打算先在近几日解决一定的应用层面的内容。因此也就诞生了这篇文章。\n背景 这个Web应用说来其实应该是很简单的，但是由于没有前后端开发经验，对我来说也算是一个挑战。这个应用旨在实现能够通过前端上传录音文件，通过后端的处理将文本返回给前端页面以及能够调用本地的麦克风进行录音最后通过这段录音能够返回文本内容。项目不大，也应该不会太过华丽，所以选用了Flask框架处理。\n基本文件上传表单 从客户端的角度来讲，上传文件和提交表单数据一样，因此我们需要定义一个包含文件字段的HTML表单。一个简单的HTML表单如下。这个表单可以接收一个文件。\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;title\u0026gt;语音识别站点\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;录音文件上传\u0026lt;/h1\u0026gt; \u0026lt;form method=\u0026#34;post\u0026#34; action=\u0026#34;\u0026#34; enctype=\u0026#34;multipart/form-data\u0026#34;\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=\u0026#34;file\u0026#34; name=\u0026#34;file\u0026#34; accept=\u0026#34;.wav\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;\u0026lt;input type=\u0026#34;submit\u0026#34; value=\u0026#34;提交\u0026#34;\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/form\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 这里需要注意的是，\u0026lt;form\u0026gt;元素的method属性可以是get或post。使用get时，数据将在请求URL的查询字符串中提交，而使用post时，数据将进入请求主体。当一个HTML表单中包含文件时，必须使用post方法，因为我们不可能在请求URL字符串中处理提交的文件数据。采用multipart/form-data字段则是因为这个字段可以适用于提交文件。accept属性则是可以限制表单可以接受的文件类型。如上使用了.wav进行限制，当点击选择文件时，系统的资源管理器会帮你过滤类型不匹配的文件。\n使用Flask进行文件接收 对于常规表单，Flask提供了对提交表单字段的访问。其中文件字段就包含在 request.files字典中，可以通过键值 file来访问文件列表，从中得出文件名，再用save保存文件。\n@app.route(\u0026#39;/upload\u0026#39;, methods=[\u0026#39;post\u0026#39;]) def upload_file(): upload_file = request.files[\u0026#39;file\u0026#39;] if upload_file.filename != \u0026#34;\u0026#34;: upload_file.save(upload_file.filename) return redirect(url_for(\u0026#39;upload_file\u0026#39;)) 这样，每当提交一次文件时，就会将文件保存到你的工程目录中。如下图所示\n文件上传效果 添加一些限制 文件大小限制 为了防止上传十分大的文件，我们在后端中添加对文件大小进行限制的配置。\napp.config[\u0026#39;MAX_CONTENT_LENGTH\u0026#39;] = 1024 * 1024 添加了如上的配置之后，文件大小将会被限制在1MB，超过此大小的文件将会被拒收。这样在一定程度上也可以防止客户端采用上传大文件的方式过多的占用服务器资源以及，通过大容量文件对服务器进行攻击。\n可接收文件类型限制 在前面的HTML表单中提到了accept属性，通过这个属性系统资源管理器会对文件的扩展名进行筛选。在发现了Flask框架可以对文件类型进行限制的时候我决定转到后台进行文件类型的限制。\napp.config[\u0026#39;UPLOAD_EXTENSIONS\u0026#39;] = [\u0026#39;.wav\u0026#39;] 上传路径的限制 由于之后需要使用到这个录音文件，所以我将这个录音文件保存在相应的工程路径中，便于之后的获取。据此添加了相关配置\napp.config[\u0026#39;UPLOAD_PATH\u0026#39;] = \u0026#39;uploads\u0026#39; 这样当文件上传之后，你就可以看到文件在uploads的文件夹下出现\n文件上传路径限制 综合以上几点限制之后我们可以得到后端接收录音文件的完整代码如下。\n@app.route(\u0026#39;/upload\u0026#39;, methods=[\u0026#39;post\u0026#39;]) def upload_file(): upload_file = request.files[\u0026#39;file\u0026#39;] filename = secure_filename(upload_file.filename) if filename != \u0026#34;\u0026#34;: file_ext = os.path.splitext(filename)[1] if file_ext not in app.config[\u0026#39;UPLOAD_EXTENSIONS\u0026#39;]: abort(400) upload_file.save(os.path.join(app.config[\u0026#39;UPLOAD_PATH\u0026#39;], filename)) return redirect(url_for(\u0026#39;upload_file\u0026#39;)) 使用dropzone的css框架 当采用HTML自带的表单文件上传时，生成的页面显示的更像是一个登录的文本框，在我看来如果要提交一个文件的话这样的文本框未免太小气了，因此在查阅相关资料后，发现了dropzone这个css框架，通过这个框架可以得到一个范围较大且可拖拽上传的文本框以及一个上传动画（虽然HTML表单也可以拖拽）。\n拖拽上传 要使用这个框架我们只需要在前端页面中添加如下的代码即可\n\u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/dropzone/5.7.1/min/dropzone.min.css\u0026#34;\u0026gt; \u0026lt;form action=\u0026#34;{{ url_for(\u0026#39;upload_file\u0026#39;) }}\u0026#34; class=\u0026#34;dropzone\u0026#34;\u0026gt;\u0026lt;/form\u0026gt; \u0026lt;script src=\u0026#34;https://cdnjs.cloudflare.com/ajax/libs/dropzone/5.7.1/min/dropzone.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 此处的action属性即是Flask后端中用来接收上传文件的处理函数。这样一个简单的录音文件上传的前后端应用就已经初具模样了。\n最后的页面效果 页面效果 ","date":"2022-03-15T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/flask%E5%89%8D%E5%90%8E%E7%AB%AF%E5%BA%94%E7%94%A8%E4%B8%80/","title":"Flask实现Web应用（一）"},{"content":"想来虽然学了计网，但却没有真正的用程序来实现一下计网的理论，于是就学习了一下socket的通信，并进行复现。至于为什么是Linux，是发现网上好像没有太多系统的关于Windows的网络编程，索性就直接放弃了。\nsocket socket即为套接字，是对网络中不同主机上的应用进程之间进行双向通信的端点的抽象。提供了应用进程利用网络协议交换数据的机制，上联应用程序，下联网络协议栈。既是应用协议通过网络协议进行通信的接口，也是应用程序与网络协议栈进行交互的接口。\n因此socket将复杂的网络协议族（也就是TCP/IP），隐藏起来，使其对应用程序透明，利用socket可以实现两个程序之间的通信。\n对于网络层的两大协议TCP和UDP，socket也分为了流和数据报的两种通信形式\n流：基于TCP协议，因此有序，可靠。可以实现可靠传输。 数据报：基于UDP，不需要建立和维持连接，不可靠，但是通信速度较快。 C/S模式 在计算机网络中，两个程序的通信模式是，客户/服务器模式，双方借助socket建立连接后便能实现通信。\n服务器工作流程 1、创建服务器socket 在这个阶段我们可以使用socket函数来创建一个服务器的套接字。需要注意的是在socket编程中只能指定协议族为AF_INET，即表示利用IPv4进行通信。这个函数的返回值为 文件描述符 （类型为整型），当失败时则会返回-1如下\nint listenfd; listenfd = socket(AF_INET, SOCK_STREAM, 0) 2、将服务器用于通信的地址和端口绑定到socket上 在Linux中，地址信息被封装在了结构体sockaddr中，如下\nstruct sockaddr { sa_family_t sin_family; //地址族 char sa_data[14]; //14字节，包含套接字中的目标地址和端口信息 }; 想必可以很明显的看到，在这个结构体中目标地址和端口信息是存放在一个数组中的，因此不便于区分，所以我们使用另外一个结构体sockaddr_in，这个结构体如下\nstruct sockaddr_in { sa_family_t sin_family; uint16_t sin_port; // 端口号 struct in_addr sin_addr; // IP地址 char sin_zero; } 但如果你进入到Linux的底层代码会发现，sockaddr_in还有一个参数为sockaddr的构造函数。因此使用这个结构体可以让操作更简单，如下\nstruct sockaddr_in servaddr; memset(\u0026amp;servaddr, 0, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); // 指定为任意IP地址 servaddr.sin_port = htons(atoi(argv[1])); // 将主机字节顺序转换为网络字节顺序 之后将端口绑定。\nbind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr); 3、将socket设置为监听模式 listen(listenfd, 5); // 5表示准备接受5个连接 4、等待并接受客户端连接 使用accept函数来连接客户端\nint clientfd; clientfd = accept(listenfd, (struct sockaddr*)\u0026amp;clientaddr, (socklen_t*)\u0026amp;socklen); 这里需要解释的是，服务器端其实建立了两个socket，第一个socket为被动的socket用来监听，当经过监听阶段后，Linux内核拥有accept函数借助监听的socket创建出连接客户端的socket\n5、与客户端通信，接受客户端发送的信息，并返回响应 通过recv和send函数进行通信\nwhile (true) { int iret; memset(buffer, 0, sizeof(buffer)); if ((iret = recv(clientfd, buffer, sizeof(buffer), 0)) \u0026lt;= 0) { cout \u0026lt;\u0026lt; \u0026#34;iret = \u0026#34; \u0026lt;\u0026lt; iret \u0026lt;\u0026lt; endl; break; } cout \u0026lt;\u0026lt; \u0026#34;receive \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; strcpy(buffer, \u0026#34;ACCEPT\u0026#34;); if ((iret = send(clientfd, buffer, strlen(buffer), 0)) \u0026lt;= 0) { perror(\u0026#34;send\u0026#34;); break; } cout \u0026lt;\u0026lt; \u0026#34;SEND: \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; } 6、重复（5）中操作，直到客户端断开连接 7、释放socket连接 close(listenfd); close(clientfd); // 释放监听socket和连接socket 代码 #include\u0026lt;iostream\u0026gt; #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; #include\u0026lt;unistd.h\u0026gt; #include\u0026lt;cstdlib\u0026gt; #include\u0026lt;netdb.h\u0026gt; #include\u0026lt;sys/types.h\u0026gt; #include\u0026lt;sys/socket.h\u0026gt; #include\u0026lt;arpa/inet.h\u0026gt; using namespace std; int main(int argc, char *argv[]) { if (argc != 2) { cout \u0026lt;\u0026lt; \u0026#34;Using server port\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;Example: ./server 5005\u0026#34; \u0026lt;\u0026lt; endl; return -1; } // 1 int listenfd; if ((listenfd = socket(AF_INET, SOCK_STREAM, 0)) == -1) { perror(\u0026#34;socket\u0026#34;); return -1; } // 2 struct sockaddr_in servaddr; memset(\u0026amp;servaddr, 0, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_addr.s_addr = htonl(INADDR_ANY); //servaddr.sin_addr.s_addr = inet_addr(\u0026#34;192.168.190.134\u0026#34;); // 手动指定IP地址 servaddr.sin_port = htons(atoi(argv[1])); // 3 if (bind(listenfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) != 0) { perror(\u0026#34;bind\u0026#34;); close(listenfd); return -1; } if (listen(listenfd, 5) != 0) { perror(\u0026#34;listen\u0026#34;); close(listenfd); return -1; } // 4 int clientfd; int socklen = sizeof(struct sockaddr_in); struct sockaddr_in clientaddr; clientfd = accept(listenfd, (struct sockaddr*)\u0026amp;clientaddr, (socklen_t*)\u0026amp;socklen); cout \u0026lt;\u0026lt; \u0026#34;clinet has connected\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;IP is: \u0026#34; \u0026lt;\u0026lt; inet_ntoa(clientaddr.sin_addr) \u0026lt;\u0026lt; endl; // 5 char buffer[1024]; while (true) { int iret; memset(buffer, 0, sizeof(buffer)); // if ((iret = recv(clientfd, buffer, sizeof(buffer), 0)) \u0026lt;= 0) { cout \u0026lt;\u0026lt; \u0026#34;iret = \u0026#34; \u0026lt;\u0026lt; iret \u0026lt;\u0026lt; endl; break; } cout \u0026lt;\u0026lt; \u0026#34;receive \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; strcpy(buffer, \u0026#34;ACCEPT\u0026#34;); if ((iret = send(clientfd, buffer, strlen(buffer), 0)) \u0026lt;= 0) { perror(\u0026#34;send\u0026#34;); break; } cout \u0026lt;\u0026lt; \u0026#34;SEND: \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; } close(listenfd); close(clientfd); } 客户端工作流程 1、创建客户端socket 同样采用socket函数创建套接字，用来主动连接，与服务器的创建方式一样。\n2、向服务器发起连接请求 使用gethostbyname函数将IP地址转换为hostent结构体所表示的格式。hostent结构体如下\nstruct hostent { char *h_name; //正式主机名 char **h_aliases; //主机别名 int h_addrtype; //主机IP地址类型：IPV4-AF_INET int h_length; //主机IP地址字节长度，对于IPv4是四字节，即32位 char **h_addr_list; //主机的IP地址列表 }; 建立连接的过程如下\nint sockfd; struct sockaddr_in servaddr; memset(\u0026amp;servaddr, 0, sizeof(servaddr)); servaddr.sin_family = AF_INET; // 手动指定相关参数 servaddr.sin_port = htons(atoi(argv[2])); memcpy(\u0026amp;servaddr.sin_addr, hostcon-\u0026gt;h_addr, hostcon-\u0026gt;h_length); connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr) // 连接 3、与服务器通信，将消息通过socket发送给服务器，每发送一个消息等待回复 同样使用recv和send函数\nchar buffer[1024]; for (int i = 0; i \u0026lt; 3; ++i) { int iret; memset(buffer, 0, sizeof(buffer)); sprintf(buffer, \u0026#34;this is %d data\u0026#34;, i + 1); if ((iret = send(sockfd, buffer, strlen(buffer), 0)) \u0026lt;= 0) { perror(\u0026#34;send\u0026#34;); break; } cout \u0026lt;\u0026lt; \u0026#34;send: \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; memset(buffer, 0, sizeof(buffer)); if ((iret = recv(sockfd, buffer, sizeof(buffer), 0)) \u0026lt;= 0) { cout \u0026lt;\u0026lt; \u0026#34;iret = \u0026#34; \u0026lt;\u0026lt; iret \u0026lt;\u0026lt; endl; break; } cout \u0026lt;\u0026lt; \u0026#34;receive \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; } 4、重复操作（3），直至数据发送完毕 5、释放socket close(sockfd); 代码 #include\u0026lt;cstdio\u0026gt; #include\u0026lt;cstring\u0026gt; #include\u0026lt;unistd.h\u0026gt; #include\u0026lt;netdb.h\u0026gt; #include\u0026lt;cstdlib\u0026gt; #include\u0026lt;sys/types.h\u0026gt; #include\u0026lt;sys/socket.h\u0026gt; #include\u0026lt;arpa/inet.h\u0026gt; #include\u0026lt;iostream\u0026gt; using namespace std; int main(int argc, char *argv[]) { if (argc != 3) { cout \u0026lt;\u0026lt; \u0026#34;Using client port\u0026#34; \u0026lt;\u0026lt; endl; cout \u0026lt;\u0026lt; \u0026#34;Example: ./client 127.0.0.1 5005\u0026#34; \u0026lt;\u0026lt; endl; return -1; } // 1 int sockfd; if ((sockfd = socket(AF_INET, SOCK_STREAM, 0)) == -1) { perror(\u0026#34;socket\u0026#34;); return -1; } // 2 struct hostent* hostcon; if ((hostcon = gethostbyname(argv[1])) == 0) { cout \u0026lt;\u0026lt; \u0026#34;gethostbyname failed\u0026#34; \u0026lt;\u0026lt; endl; close(sockfd); return -1; } struct sockaddr_in servaddr; memset(\u0026amp;servaddr, 0, sizeof(servaddr)); servaddr.sin_family = AF_INET; servaddr.sin_port = htons(atoi(argv[2])); memcpy(\u0026amp;servaddr.sin_addr, hostcon-\u0026gt;h_addr, hostcon-\u0026gt;h_length); // 3 if (connect(sockfd, (struct sockaddr*)\u0026amp;servaddr, sizeof(servaddr)) != 0) { perror(\u0026#34;connect\u0026#34;); close(sockfd); return -1; } // 4 char buffer[1024]; for (int i = 0; i \u0026lt; 3; ++i) { int iret; memset(buffer, 0, sizeof(buffer)); sprintf(buffer, \u0026#34;this is %d data\u0026#34;, i + 1); if ((iret = send(sockfd, buffer, strlen(buffer), 0)) \u0026lt;= 0) { perror(\u0026#34;send\u0026#34;); break; } cout \u0026lt;\u0026lt; \u0026#34;send: \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; memset(buffer, 0, sizeof(buffer)); if ((iret = recv(sockfd, buffer, sizeof(buffer), 0)) \u0026lt;= 0) { cout \u0026lt;\u0026lt; \u0026#34;iret = \u0026#34; \u0026lt;\u0026lt; iret \u0026lt;\u0026lt; endl; break; } cout \u0026lt;\u0026lt; \u0026#34;receive \u0026#34; \u0026lt;\u0026lt; buffer \u0026lt;\u0026lt; endl; } // 5 close(sockfd); } 运行结果 在工程目录中打开两个终端，在编译后通过命令./server 5005启动服务器，之后在另一个终端通过命令./client 127.0.0.1 5005运行客户端，最终结果呈现如下。\n参考 [1] 网络通信基础socket\n[2] hostent实例讲解\n[3] sockaddr和sockaddr_in详解\n","date":"2022-02-28T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/socket%E7%BC%96%E7%A8%8B/","title":"Linux下的socket网络通信"},{"content":"HTTP协议概述 概况 Web的应用层协议是超文本传输协议（HTTP）。HTTP协议由两个程序实现：一个客户程序和一个服务器程序，通过交换HTTP报文进行会话。\nHTTP定义了Web页面的方式，以及服务器向客户传送Web页面的方式。并 使用TCP作为它的支撑运输协议 。HTTP客户首先发起一个与服务器的TCP链接。连接建立，彼此之间就可以通过 套接字接口 访问TCP。从而利用套接字接口接受和发送HTTP报文。通信过程如下图所示\n由于TCP为HTTP提供可靠数据传输服务，因此每个报文都能完整地到达服务器或客户，HTTP协议不关心TCP从网络种如何处理报文的各种问题，也不用担心数据丢失。但又由于服务器只为客户服务，并不存储任何关于客户的状态信息，因此HTTP协议是一个 无状态协议 ，如果需要保存客户的登录信息，则 需要引入相关技术来记录状态，如Cookie 。\n持续和非持续连接 在实际情况中，客户可能会发出一系列请求并且服务器会对每个请求进行响应。而这种客户-服务器的交互是经过TCP进行的，因此这些请求是经过单独的TCP连接还是经过相同的TCP连接就是这种交互方式的关键问题。也因此诞生了持续和非持续的HTTP连接。\n非持续连接： 使用非持续连接，每个TCP连接在服务器 发送一个对象后就关闭 （对象即HTML文件以及网页图像等），每个TCP连接只传输一个请求报文和一个响应报文。\n非持续连接有以下缺点：\n必须为每个请求的对象建立和维护一个全新的连接。对于这样的连接要分配TCP缓冲区和保持TCP变量。造成服务器负担。 每个对象经受两倍的RTT交付时间，即一个RTT用于创建，一个RTT用于请求和接受. 持续连接： 服务器在发送响应后保持该TCP连接打开。在相同的客户与服务器之间，后续的请求和响应报文能够通过相同的连接进行传送，而如果一条连接经过一定时间间隔（配置好的超时间隔）仍未被使用，HTTP服务器就应该关闭这个连接。\nHTTP连接流程 （1）、 客户端连接到Web服务器。 一个HTTP客户端，通常是浏览器，与Web服务器的HTTP端口（默认为80端口），建立一个TCP套接字连接。\n（2）、 发送HTTP请求。 通过TCP套接字，客户端向HTTP服务器发送一个文本的请求报文。\n（3）、 服务器接受请求并返回HTTP响应。 Web服务器解析请求，定位请求资源。服务器将资源副本写到TCP套接字，由客户端读取。\n（4）、 释放TCP连接。 若connection模式为close，则服务器主动关闭TCP连接，客户端被动关闭TCP连接并释放。反之若为keepalive，则TCP连接会保持一段时间，在这段时间内可以继续接受请求和响应。\n（5）、 客户端浏览器解析HTML内容。 客户端浏览器首先解析状态行，查看表明请求是否成功的状态代码。然后解析每一个响应头，响应头告知以下为若干字节的HTML文档和文档的字符集。客户端浏览器读取相应的HTML，根据HTML语法对其进行格式化，并在浏览器窗口显示。\nHTTP报文 HTTP请求报文 请求报文格式如下图。其中第一行称为 请求行 ，其后继的行称为 首部行 。\n请求行有三个字段：方法字段、URL字段和HTTP版本字段。方法字段可以区包括GET、POST、HEAD、PUT和DELETE。这些请求方法将会在下文讲述。\n首部行则会包含Host字段，Connection字段以及User-agent字段等。分别指明了对象所在的主机、是否使用持续连接以及用户代理（发送请求的浏览器类型）。其中Host首部行则是Web代理高速缓存所要求的。\nHTTP响应报文 响应报文的格式如下图。分为 状态行 ， 首部行 以及 实体体 三个部分。实体体为报文的主要部分，为所请求的对象本身。同样在首部行会包含一些字段。例如：Date：指示发送该报文的日期和时间；Server：指示发送的服务器；Last-Modified：最后修改的日期和时间等。\nHTTP状态码 由上述HTTP响应报文可以得出，在状态行中包含状态码字段，这个字段将指示请求的结果。可以将状态码分为以下几类：\n1xx： 指示信息 ，表示请求已接收，继续处理 2xx： 成功 ，表示请求已被成功接收、理解、接受 3xx： 重定向 ，要完成请求必须进行进一步的操作 4xx： 客户端错误 ，请求有语法错误或请求无法实现 5xx： 服务器端错误 ，服务器未能实现合法请求 常见的状态码如下：\n200 OK：请求成功，信息在返回的响应报文中。 301 Moved Permanently：请求的对象已经被永久转移了，新的URL定义在响应报文的Location：首部行中。客户软件将自动获取新的URL 400 Bad Request：一个通用的差错代码，指示该请求不能被服务器理解。 404 Not Found：被请求的文档不在服务器上。 505 HTTP Version Not Supported：服务器不支持请求报文使用的HTTP协议版本。 更多的状态码及其对应短语可以参考此网址。\nHTTP请求方法 HTTP1.0定义了三种请求方法：GET、POST和HEAD。\nHTTP1.1新增了五种请求方法：OPTIONS、PUT、DELETE、TRACE和CONNECT方法\n每种方法的特点如下：\nGET。 请求指定的页面信息，并返回实体主体。 HEAD。 类似于GET请求，只不过返回的响应中没有主体内容，用于获取报头。 POST。 向指定资源提交提交数据进行处理请求（例如提交表单或者上传文件）。数据被包含在请求体中。POST请求可能会导致新的资源的建立和/或已有资源的修改。 PUT。 从客户端向服务器传送的数据取代指定的文档的内容。 DELETE。 请求服务器删除指定的页面。 CONNECT。 HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器。 OPTIONS。 允许客户端查看服务器的性能。 TRACE。 回显服务器收到的请求，主要用于测试或诊断。 GET和POST请求的区别 （1）、 GET提交： 请求的数据会附在URL之后（就是把数据放置在HTTP协议头中），以?分割URL和传输数据，多个参数用\u0026amp;连接；例如：login.action?name=hyddd\u0026amp;password=idontknow\u0026amp;verify=%E4%BD%A0%E5%A5%BD。如果数据是英文字母/数字，原样发送，如果是空格，转换为+，如果是中文/其他字符，则直接把字符串用BASE64加密，得出如： %E4%BD%A0%E5%A5%BD，其中％XX中的XX为该符号以16进制表示的ASCII。 POST提交： 把提交的数据放置在是HTTP包的包体中。\n也即 GET提交的数据会在地址栏中显示出来，而POST提交，地址栏不会改变\n（2）、传输数据的大小：首先 HTTP协议没有对传输的数据大小进行限制，HTTP协议规范也没有对URL长度进行限制 。\n而在实际的开发中：\nGET： 特定浏览器和服务器对URL长度有限制，例如：IE对URL长度的限制是2083字节(2K+35)。对于其他浏览器，如Netscape、FireFox等，理论上没有长度限制，其限制取决于操作系统的支持。因此对于GET提交时，传输数据就会受到URL长度的限制。\nPOST： 由于不是通过URL传值，理论上数据不受限。但实际各个WEB服务器会规定对POST提交数据大小进行限制，Apache、IIS6都有各自的配置。\n（3）、安全性\nPOST的安全性要比GET的安全性高。比如：通过GET提交数据，用户名和密码将明文出现在URL上，因为登录页面有可能被浏览器缓存；其他人查看浏览器的历史纪录，那么别人就可以拿到你的账号和密码了，除此之外，使用GET提交数据还可能会造成Cross-site request forgery攻击\nHTTPS HTTP的不足 通信使用明文（不加密），内容可能会被窃听 不验证通信方的身份，因此有可能遭遇伪装 无法证明报文的完整性，所以有可能已遭篡改 HTTPS介绍 HTTP协议中没有加密机制，但可以通过和SSL（Secure Socket Layer， 安全套接层 ）或TLS（Transport Layer Security， 安全传输协议 ）的组合使用，加密HTTP通信内容。属于通信加密，即在整个通信线路中加密。如图\nHTTPS采用 共享密钥加密（对称） 和 公开密钥加密（非对称） 两者并用的混合加密机制，若密钥能够实现安全交换，那么有可能仅考虑使用公开密钥来加密通信。但是公开密钥加密相对于共享密钥加密处理速度较慢。因此可以在交换密钥阶段使用 公开密钥 加密方式，之后建立通信交换报文阶段则使用 共享密钥 加密方式。\nHTTPS握手流程简单描述如下：\n（1）、浏览器将自己支持的一套加密规则发送给网站 （服务器获得浏览器公钥）\n（2）、网站从中选出一组加密算法与HASH算法，并将自己的身份信息以证书的形式发回给浏览器。证书里面包含了网站地址，加密公钥等信息 （浏览器获得服务器公钥）\n（3）、获得网站证书后浏览器需要进行以下操作\n（a）、验证证书的合法性（颁发证书的机构是否合法，证书中包含的网站地址是否与正在访问的地址一致等），如果证书受信任，则浏览器栏里面会显示一个小锁头，否则会给出证书不受信的提示。\n（b）、如果证书受信任，或者是用户接受了不受信的证书，浏览器会生成一串随机数的密码（接下来通信的密钥），并用证书中提供的公钥加密（共享密钥加密）。\n（c）、使用约定好的HASH计算握手消息，并使用生成的随机数对消息进行加密，最后将之前生成的所有信息发送给网站。\n（4）、网站接受浏览器发来的数据之后要做以下的操作：\n（a）、使用自己的私钥将信息解密取出密码，使用密码解密浏览器发来的握手消息，并验证HASH是否与浏览器发来的一致。\n（b）、使用密码加密一段握手消息，发送给浏览器。\n","date":"2022-02-14T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E5%BA%94%E7%94%A8%E5%B1%82%E5%8D%8F%E8%AE%AE-http/","title":"应用层协议——HTTP"},{"content":"题目描述 输入一棵二叉搜索树，将该二叉搜索树转换成一个排序的循环双向链表。要求不能创建任何新的节点，只能调整树中节点指针的指向。\n为了让您更好地理解问题，以下面的二叉搜索树为例：\n我们希望将这个二叉搜索树转化为双向循环链表。链表中的每个节点都有一个前驱和后继指针。对于双向循环链表，第一个节点的前驱是最后一个节点，最后一个节点的后继是第一个节点。\n下图展示了上面的二叉搜索树转化成的链表。“head” 表示指向链表中有最小元素的节点\n特别地，我们希望可以就地完成转换操作。当转化完成以后，树中节点的左指针需要指向前驱，树中节点的右指针需要指向后继。还需要返回链表中的第一个节点的指针。\n解题想法 题目给出的是一棵二叉搜索树，而要我们整理成一个升序的双向链表，可以很自然的想到二叉搜索树的中序遍历即为升序序列。面对双向链表，我们还需要建立相邻结点之间的关系，因此需要设置前驱结点pre和当前结点cur两个指针来组织结点之间的关系。（具体即为pre-\u0026gt;right = cur和cur-\u0026gt;left = pre），最后则是连接头结点和尾结点。\n中序遍历流程 令recur为中序遍历函数\n1、终止条件：当cur结点为空时，直接返回，说明此时已经越过了叶节点\n2、递归左子树，recur（cur-\u0026gt;left）\n3、构建相邻结点之间的关系\n（a）、如果pre指针为空，说明当前访问的结点为中序遍历的第一个结点，也就是双向链表的头结点，此时将当前结点cur赋值给head指针。\n（b）、如果pre指针非空，需要建立两个结点之间的关系，即pre-\u0026gt;right = cur和cur-\u0026gt;left = pre\n（c）、更新前驱结点pre，即pre = cur\n4、递归右子树，recur（cur-\u0026gt;right）\n最后在执行函数中，需要利用head和pre指针建立双向链表的头尾关系，即head-\u0026gt;left = pre，pre-\u0026gt;right = head，因为当递归完二叉搜索树时，pre会指向中序的最后一个元素。\n代码 /* // Definition for a Node. class Node { public: int val; Node* left; Node* right; Node() {} Node(int _val) { val = _val; left = NULL; right = NULL; } Node(int _val, Node* _left, Node* _right) { val = _val; left = _left; right = _right; } }; */ class Solution { private: Node *head, *pre; void recur(Node *root) { if (root == NULL) return; recur(root-\u0026gt;left); // 如果pre为空，说明当前访问的是头结点 if (pre) pre-\u0026gt;right = root; else head = root; root-\u0026gt;left = pre; // 修改当前的结点为前驱结点 pre = root; recur(root-\u0026gt;right); } public: Node* treeToDoublyList(Node* root) { if (root == NULL) return NULL; recur(root); head-\u0026gt;left = pre; pre-\u0026gt;right = head; return head; } }; ","date":"2022-01-31T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E4%BA%8C%E5%8F%89%E6%A0%91%E4%B8%8E%E5%8F%8C%E5%90%91%E9%93%BE%E8%A1%A8/","title":"LeetCode 二叉搜索树与双向链表"},{"content":"题目描述 给定一个 m x n 二维字符网格 board 和一个字符串单词 word 。如果 word 存在于网格中，返回 true ；否则，返回 false 。\n单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。\n例如，在下面的 3×4 的矩阵中包含单词 \u0026ldquo;ABCCED\u0026rdquo;（单词中的字母已标出）。\n示例：\n输入：board = [[\u0026#34;A\u0026#34;,\u0026#34;B\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;E\u0026#34;],[\u0026#34;S\u0026#34;,\u0026#34;F\u0026#34;,\u0026#34;C\u0026#34;,\u0026#34;S\u0026#34;],[\u0026#34;A\u0026#34;,\u0026#34;D\u0026#34;,\u0026#34;E\u0026#34;,\u0026#34;E\u0026#34;]], word = \u0026#34;ABCCED\u0026#34;\r输出：true 输入：board = [[\u0026#34;a\u0026#34;,\u0026#34;b\u0026#34;],[\u0026#34;c\u0026#34;,\u0026#34;d\u0026#34;]], word = \u0026#34;abcd\u0026#34;\r输出：false 解题想法 这道题一开始想到的是使用BFS，但是细想发现BFS好像很难处理回溯的问题，应该采用DFS。然后，就是递归的痛了，也借这道题学习一下DFS的写法吧。\n**DFS：**即是暴力遍历矩阵的所有元素，来搜索一条可行的路径，通过递归，可以在一条路径中搜索到底，最后回溯到之前已经匹配的节点。\n**剪枝：**在DFS过程中如果当前的矩阵字符与字符串字符不等，可以直接回溯。或者路径已经访问过，可以直接跳过。\nDFS解析 1、递归终止条件：\n（a）、返回true，当匹配到字符串的最后一个字符时，可以直接返回true（至于为什么可以这样返回，参看代码注释）\n（b）、返回false，如果矩阵的索引越界，当前矩阵的字符和字符串的字符不匹配，当前元素已经访问过了直接返回false\n2、递归过程\n（a）、选定当前元素，将当前元素标记为空字符，表明当前元素已经访问过，防止走回头路。\n（b）、搜索下一个元素，向四个方向分别匹配字符，如果有一个方向可以匹配就继续递归这个方向的DFS算法，并返回true，否则返回false\n（c）、还原当前元素，需要在DFS算法退出之前，将空字符还原为原来的字符，用于回溯时重新寻找路径，否则回溯将找不到正确路径。\n代码 class Solution { public: bool exist(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt;\u0026amp; board, string word) { rows = board.size(); cols = board[0].size(); for (int i = 0; i \u0026lt; rows; ++i) { for (int j = 0; j \u0026lt; cols; ++j) { if (dfs(board, word, i, j, 0)) return true; } } return false; } private: int rows, cols; // k为单词索引 bool dfs(vector\u0026lt;vector\u0026lt;char\u0026gt;\u0026gt; \u0026amp;board, string word, int i, int j, int k) { // 如果越界或者字符不相等，就停止递归 if (i \u0026gt;= rows || i \u0026lt; 0 || j \u0026gt;= cols || j \u0026lt; 0 || board[i][j] != word[k]) return false; // 当进入这个判断条件时，会先判断是否相等，因此只要走到最后一个字符就可以返回true if (k == word.size() - 1) return true; // 将访问过的字符设为一个特殊字符，防止走回头路 board[i][j] = \u0026#39;\\0\u0026#39;; array\u0026lt;int, 4\u0026gt; dx{0, 1, 0, -1}; array\u0026lt;int, 4\u0026gt; dy{1, 0, -1, 0}; for (int idx = 0; idx \u0026lt; 4; ++idx) { if (dfs(board, word, i + dx[idx], j + dy[idx], k + 1)) { return true; } } // 在退出dfs之前要恢复board，否则回溯将无法找到正确路径 board[i][j] = word[k]; return false; } }; ","date":"2022-01-29T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E7%9F%A9%E9%98%B5%E4%B8%AD%E7%9A%84%E8%B7%AF%E5%BE%84/","title":"LeetCode 矩阵中的路径"},{"content":"题目描述 请从字符串中找出一个最长的不包含重复字符的子字符串，计算该最长子字符串的长度。\n示例\n输入: \u0026#34;abcabcbb\u0026#34;\r输出: 3 解释: 因为无重复字符的最长子串是 \u0026#34;abc\u0026#34;，所以其长度为 3。 输入: \u0026#34;bbbbb\u0026#34;\r输出: 1\r解释: 因为无重复字符的最长子串是 \u0026#34;b\u0026#34;，所以其长度为 1。 输入: \u0026#34;pwwkew\u0026#34;\r输出: 3\r解释: 因为无重复字符的最长子串是 \u0026#34;wke\u0026#34;，所以其长度为 3。\r请注意，你的答案必须是 子串 的长度，\u0026#34;pwke\u0026#34; 是一个子序列，不是子串。 解题想法 这道题想通了发现也是一道比较常规的动态规划题，但是开始时没有考虑好的细节确实是折磨了我一阵。以及没有想到用哈希表处理属实比较蠢。\n动态规划 1、状态的定义，本题依旧需要一个一维的动态规划数组，dp[i]则表示以第i个字符为结尾的子字符串可以得到的最长的含不重复字符的子字符串长度。\n2、状态转移方程，首先我们需要固定右边界i，也就是遍历时的索引，设与s[i]相同且距离最近的字符为s[pos]（pos初始化为-1。此时不重复字符子串长度应该是i - pos，但是题目所求为最长，因此根据动态规划的思想就有以下的情况\n（a）、当$pos\u0026lt;0$时，说明在s[i]的左侧没有与s[i]相同的字符，那么dp[i] = dp[i - 1] + 1（即最长长度等于遍历到前一字符的长度 + 1）\n（b）、当$dp[i - 1]\u0026lt;i - pos$时，说明重复的字符应该在当前最长非重复字符子串的区间外，即在这个子串的左边，此时dp[i] = dp[i - 1] + 1，也就是之前的子串再加上当前的字符。\n（c）、当$dp[i - 1]\u0026gt;=i-pos$​时，说明此时重复的字符应该在当前最长的非重复字符子串的区间内，因此dp[i] = i - pos，也就是在之前的子串中截取出非重复的部分。\n3、需要一个哈希表用来记录曾经出现过的字符的相对应的索引，每当遍历到一个重复字符时，便要将哈希表中的相应表项更新为当前的索引。\n由以上的分析可以看出，dp[i]只由dp[i - 1]决定，所以可以省略动态规划的数组，采用一个变量进行迭代即可，并求取此变量在这个过程中的最大值。\n双指针法（滑动窗口） 由于滑动窗口的代码写得比较少，当看到题解有滑动窗口时，便又学习了一下。\n首先要初始化left = -1以及right = 0两个指针，利用right指针进行遍历，并根据是否有重复的字符，以及是否为最长的子字符串来更改left指针的位置。这种做法依旧需要一个哈希表来存储字符对应的索引。这样就会有以下两种情况。\n（a）、当right指针对应的字符在哈希表中不存在时，可以直接增加right指针的值，且子字符串长度为right - left\n（b）、若right指针对应的字符在哈希表中存在，那么需要将left更改为当前的left指针与哈希表中对应索引的最大值（这里很重要，如果没有取两者之间的最大值那么虽然可以避免在左右两个端点处重复，但可能会导致在字符串内存在重复字符）。\n在每次更新最长字符串的值之前都要更新哈希表中的相应表项。\n代码 /*动态规划*/ class Solution { public: int lengthOfLongestSubstring(string s) { if (!s.size()) return 0; if (s.size() == 1) return 1; // 优先判断两种特殊情况 int res = 1, len = s.size(); unordered_map\u0026lt;char, int\u0026gt; um; // 用于存储字符的索引位置 int temp = 0; // temp用来存储上一次的dp值 for (int i = 0; i \u0026lt; len; ++i) { int pos; // 用来获取是否有当前字符的索引 if (um.count(s[i])) pos = um[s[i]]; else pos = -1; um[s[i]] = i; // 更新哈希表的索引 temp = (i - pos) \u0026gt; temp ? temp + 1 : (i - pos); res = max(res, temp); } return res; } }; /*滑动窗口*/ class Solution { public: int lengthOfLongestSubstring(string s) { if (!s.size()) return 0; if (s.size() == 1) return 1; // 优先判断两种特殊情况 unordered_map\u0026lt;char, int\u0026gt; um; int left = -1, len = s.size(); int res = 1; for (int right = 0; right \u0026lt; len; ++right) { if (um.count(s[right])) { // 表明这时候已经出现的重复的字符 // 这时候需要移动左指针 left = max(left, um[s[right]]); // 如果不取两者最大，那么会可能会导致字符串内重复 } um[s[right]] = right; // 更新索引 res = max(res, right - left); } return res; } }; ","date":"2022-01-24T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E6%9C%80%E9%95%BF%E4%B8%8D%E5%90%AB%E9%87%8D%E5%A4%8D%E5%AD%97%E7%AC%A6%E7%9A%84%E5%AD%90%E5%AD%97%E7%AC%A6%E4%B8%B2/","title":"LeetCode 最长不含重复字符的子字符串"},{"content":"题目描述 假设把某股票的价格按照时间先后顺序存储在数组中，请问买卖该股票一次可能获得的最大利润是多少？\n示例：\n输入: [7,1,5,3,6,4]\r输出: 5\r解释: 在第 2 天（股票价格 = 1）的时候买入，在第 5 天（股票价格 = 6）的时候卖出，最大利润 = 6-1 = 5 。\r注意利润不能是 7-1 = 6, 因为卖出价格需要大于买入价格。 输入: [7,6,4,3,1]\r输出: 0\r解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。 解题想法 这道题首先可以采用暴力的求解方法，只要计算在某一天中，其后的每一天卖出的价钱所得的利润最大，依次遍历每一天，最终就可以求得最大利润，这样可以得到的时间复杂度为$$(n-1)+(n-2)+\u0026hellip;+2+1=n(n-1)/2$$也就是$O(n^2)$，不出所料，在这种解法中，你需要将一些冗余的操作去除，才能通过力扣的判题机。于是就有了下面的解法，也就是动态规划。\n动态规划思路 1、状态的定义，首先需要声明一个动态规划数组dp，其中dp[i]表示以第i天为最后一天的子数组所能获得的最大利润。\n2、状态转移方程，由于股票只能买卖一次，因此dp[i]（也就是前i天的最大利润），应该是前i-1天的最大利润与第i天卖出股票的利润的最大值，于是就有了$前i天最大利润=max（前i-1天最大利润，第i天价格-前i天最低价格）$即为dp[i] = max(dp[i - 1], prices[i] - min(prices[0:i]))，最终返回dp[n-1]为所求。\n优化 我们还可以进行空间和时间上的优化，时间上，可以借助一个变量cost来维护最小值，这样每次进行状态转移时只需要dp[i] = max(dp[i - 1], prices[i] - min(cost, prices[i]))，空间上，由于本道题的状态转移只需要前一次的状态，因此可以采用一个变量maxprofit来代替整个动态规划列表，这样状态转移就简化为了maxprofit = max(maxprofit, prices[i] - min(cost, prices[i]))\n代码 /*未优化*/ class Solution { public: int maxProfit(vector\u0026lt;int\u0026gt;\u0026amp; prices) { if (prices.empty()) { return 0; } array\u0026lt;int, 100004\u0026gt; dp {}; int cost = prices[0]; dp[0] = 0; for (int i = 1; i \u0026lt; prices.size(); ++i) { if (prices[i] \u0026lt; cost) { cost = prices[i]; } dp[i] = max(dp[i - 1], prices[i] - cost); } return dp[prices.size() - 1]; } }; /*优化后*/ class Solution { public: int maxProfit(vector\u0026lt;int\u0026gt;\u0026amp; prices) { if (prices.empty()) return 0; int cost = prices[0], maxprofit = 0; for (int i = 1; i \u0026lt; prices.size(); ++i) { if (prices[i] \u0026lt; cost) { cost = prices[i]; } maxprofit = max(maxprofit, prices[i] - cost); } return maxprofit; } }; ","date":"2022-01-22T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E8%82%A1%E7%A5%A8%E7%9A%84%E6%9C%80%E5%A4%A7%E5%88%A9%E6%B6%A6/","title":"LeetCode 股票的最大利润"},{"content":"题目描述 输入两棵二叉树A和B，判断B是不是A的子结构。(约定空树不是任意一个树的子结构)\nB是A的子结构， 即 A中有出现和B相同的结构和节点值。\n例如: 给定的树 A:\n3\r/ \\\r4 5\r/ \\\r1 2 给定的树 B：\n4\r/\r1 返回 true，因为 B 与 A 的一个子树拥有相同的结构和节点值。\n解题想法 树的题目是真的让人头疼。在经过几次暴力解的尝试之后我还是放弃了，最后几组数据总是通过不了。还是去翻了题解，发现又是我最烦的递归（永远的痛）\n1、首先子结构要么是其本身要么在树的左子树或者右子树里，因此在isSubStructure函数中需要判断是本身匹配还是左子树或者是右子树匹配，这是最外层递归。最外层递归中，如果B树（也就是子结构树）为空，那么直接返回false，同理如果A树（要从这棵树中寻找子结构）为空也可以直接返回false\n2、对于子结构是否匹配的判断也需要通过递归实现（也就是isContain函数），但具有以下几种情况，其中包含退出递归的条件\n（a）、如果当前的B树为空，且At树（也就是从A树种截取出的子树）非空或者空，此时表明B树是A树的一个子结构，因为可以将B树匹配完。\n（b）、如果当前的B树非空，但At树为空，说明此时已经遍历过了A树的叶节点，那么B树一定与At树不匹配，可以直接返回false\n（c）、如果此时At树和B树都不为空，且当前匹配中的结点的值不相等，那么B树一定与At树不匹配，可以直接返回false\n（d）、如果At树和B树都不为空，且当前匹配中的结点的值相等，那么就继续匹配两棵树当前结点的左结点和右结点（也就是内层的递归）\n代码 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: bool isContain(TreeNode *A, TreeNode *B) { // 递归判断是否包含 // 如果此时B为空树，说明已经递归完了B树，因此是包含的 if (B == NULL) return true; // 如果大树为空，匹配树不是空，那么说明不包含 if (A == NULL) return false; // 如果当前匹配的结点值不相等，那么也是不包含 if (A-\u0026gt;val != B-\u0026gt;val) return false; // 如果以上条件都不满足就继续判断子树 return isContain(A-\u0026gt;left, B-\u0026gt;left) \u0026amp;\u0026amp; isContain(A-\u0026gt;right, B-\u0026gt;right); } bool isSubStructure(TreeNode* A, TreeNode* B) { // B树为空，则为false if (B == NULL) return false; // A树为空，也为false if (A == NULL) return false; return isContain(A, B) || isSubStructure(A-\u0026gt;left, B) || isSubStructure(A-\u0026gt;right, B); } }; ","date":"2022-01-21T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/","title":"LeetCode 树的子结构"},{"content":"题目描述 从上到下按层打印二叉树，同一层的节点按从左到右的顺序打印，每一层打印到一行。\n例如: 给定二叉树: [3,9,20,null,null,15,7],\n3\r/ \\\r9 20\r/ \\\r15 7 返回其层次遍历结果：\n[\r[3],\r[9,20],\r[15,7]\r] 解题想法 这道题本身并不是什么难题，也很明显可以看出是一个层序遍历的变种，解决问题的关键就在于如何区分当前层和下一层，最开始是打算使用pair将每个结点与其相应的层序号对应起来。但在实际写代码时发现有点困难，最后还是去翻了题解，发现了利用空指针作为层与层之间分隔的方法。具体做法就是在第一次入队时再入队一个空指针，之后利用队头元素不为空指针作为内循环的判断，如果队头为空，则表明了已经循环完了一层。而由于二叉树的特点，在使用层序遍历时，遍历完一层也即代表着下一层的非空结点已经完全入队，这时便可以再入队一个空指针作为下一个分隔结点。利用这样的循环就能做到以层为序来打印层序遍历。\n代码 /** * Definition for a binary tree node. * struct TreeNode { * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) {} * }; */ class Solution { public: vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; levelOrder(TreeNode* root) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; res; if (root == NULL) { return res; } queue\u0026lt;TreeNode*\u0026gt; que; // 用于层序遍历 que.push(root); que.push(NULL); // 利用空指针作为每层之间的分隔 while (que.size()) { vector\u0026lt;int\u0026gt; vtemp; // 队列不为空时，就申请空间存储本层数据 while (que.front()) { // 用这个条件判断是否属于同一层数据 TreeNode *temp = que.front(); vtemp.push_back(temp-\u0026gt;val); que.pop(); if (temp-\u0026gt;left) que.push(temp-\u0026gt;left); if (temp-\u0026gt;right) que.push(temp-\u0026gt;right); } // 退出上面的循环表示已经到了层与层之间的界线 que.pop(); if (que.size()) { // 如果此时队列非空，则表明不是最后一层 // 遍历过一层即表示队列中已经保存了下一层，因此需要加入空指针分割 que.push(NULL); } res.push_back(vtemp); } return res; } }; ","date":"2022-01-20T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E4%BB%8E%E4%B8%8A%E5%88%B0%E4%B8%8B%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/","title":"LeetCode 从上到下打印链表"},{"content":" 如题的这两部动漫都是我认为接近于国漫巅峰的动漫，当然我并没有看过多少国漫，因此这一判断也是相当主观的。但确实的是其在我内心中是“前无古人”的。这两部动漫也是将我这一早已不看动漫的人又带回了国创动漫。\n动漫情节叙述 由于两部动漫都只做到了第一季，虽能体现一定的设定及世界观，但是很多剧情依旧是在迷雾之中的。这里也就只能讲讲已有的剧情。\n《灵笼》的设定之前也是有过，但是灵笼剧情的饱满程度让末日题材的这种设定能够更好的展现出来。开篇便是猎荒者（即在末日中探索旧世界的可利用的东西，为幸存下来的人服务）在探索一篇废墟，并遭遇了噬极兽（因为这种怪兽因而认为地上是无法生存的）。当猎荒者凯旋归来时，之后便迎来了另一个比较重要的情节，也就是城主的更替。当然这里要先说说这个城主的问题，现在的人们生活在一个叫做“灯塔”的建筑上，而现任成主的儿子则是在灯塔上成立了一个类似于教会的“光影会”。这里就权且称这位会长为“黄毛”。城主更希望由猎荒者队长马克来接任下一任城主，但是心胸狭隘的黄毛却利用一些手段导致马克成为了噬极兽并因此被流放。（这里其实省略了很多的情节，但我觉得第一季的主要目的应该就是仅仅为了引出世界观以及《灵笼》整个庞大故事的开端）。最后第一季也是在马克与在地面上的人类相遇而结束。（这么讲其实还是草率了，如果有时间我还是希望能够去看《灵笼》的剧情）。\n反观《时光代理人》却是以一个较为新颖的角度作为设定，也即时间。（具体便是主角团能够以自己的特殊能力进入照片，而在照片里的一言一行都有可能对未来造成影响）。时光照相馆是主角团的“基地”，也是通过这个照相馆接取委托人的任务从而帮助委托人实现一些过去或希望从过去得到线索的事情。由于这部动漫是通过一个个的小故事，因此在这里也不好叙述，依旧是推荐能够去观看动漫本身。\n谈动漫本身 很明显两部动漫都具有自己的特点，但是我却依旧将他们放在一起谈，是因为它们给我带来足够的震撼，纵使在这之前亦有《百妖谱》等，却没能给我带来那种共鸣与一种仿佛自己置身于其中希望改变世界的情感。\n先谈《灵笼》，灵笼首先在建模方面就足够优秀，虽然采用了我不是很喜欢的建模方式，但是却依旧能够吸引我（对于我这种比较叼的人来说，这个方面就已经足够碾压国产大部分动漫了）不仅如此，人物的表情以及动作都十分逼真，其实到这里就已经可以体现制作者的用心了。当然这只是建模这个我比较关注的方面。接下来应该是剧情方面，《灵笼》的设定是动漫中比较不常见的，本以为这样的题材并不能够很好的把握，但事实是艺画开天以一种更为庞大的世界观来描述这个末日世界，这是在之前的各种影视剧中没有出现的。不仅如此，涉及的领域也十分广泛，大部分动漫可能在主题上会展现的只会是比较重要的一部分，例如青春或者爱情、友情等等。但《灵笼》光第一季就像是想要把自己的所有元素都展现出来，其中的光影会代表着类似教会一般的人，没有什么实质性的作用却控制着一部分人的思想，而会长也借助这些力量来实现自己的目标，甚至几乎摧毁了猎荒者这一强大的对抗力量。这如果影射到现实，便是思想、武装与权力之间的关系。除了这个，人性的描述也十分到位，通过对尘民4068的刻画，既展现了末日阶级统治的一种希望成为更高阶级的渴望，也让观众因为这位尘民的所作所为更加拥有共鸣。最终也会更投入到作品本身。\n当然，一部好的的作品我觉得感情线应该是必不可少的。在《灵笼》中，并没有婚恋的自由，甚至拥有“三大法则”制约每一个人的情感。所谓的“爱情”更像是统治阶级给予的枷锁，而作为猎荒者队长的马克，本应遵守这些规则，却仍然在第一季的最后体会到了什么是爱，并为自己所爱的人大闹灯塔。虽然并没有成功的将灯塔上的人的思想扭转过来，但也为作品之后奠定了一个基调。第一季的最后则是利用地上的生存的人来表明这仅仅是《灵笼》这个庞大世界观的开始。还有一个吸引我的点是，在第一季末的一个全季最精彩的打斗场面中，背景音乐采用了纯唢呐，这种打破常规的做法也是取得了一个意想不到的效果，导致了各种二创都不及原版。\n再看《时光代理人》，这动漫则是一开始便就从一个故事入手，直入主题，直接为观众呈现了这部作品在之后的表现方式。最开始我以为这只会是一种加强版的泡面番，通过一个个的故事来组成一部动漫，但渐渐的我发现我错了，在短短十几集的动漫中，却在最后的几集给你中给你串连起之前的故事。瞬间让你觉得这不是一个个故事那么简单。这也反应了作品中一直在重复的一句话“无论过去，不问将来”。故事也不是那种偏离现实的故事，而是充满人情味，让你觉得这都是可能发生在你身边的故事。其中的一个描述汶川地震的故事，借用了亲情和爱情令人动容。每个故事又都在恰到好处的地方停了下来。（因此我建议如果要看，就干脆一次性看完）\n通过这整个主角团在第一季遭遇的整个故事，很精确的展现了主角团三人的性格特点，沉稳缜密的陆光，正义但却有点冲动的程小时，以及开朗能够带动气氛的乔苓。让人多多少少有点羡慕这样的三人组。第一季的最后程小时为了挽救自己之前在照片里所做过的一些错事，却发现最后仍旧没能改变，这也引出了这部动漫的反派（不止有人能够像他们那样穿越时空回到过去）。也因为这个反派最终主角团三人全部都遭到了不同程度的不幸。第一季也是在这样的一种紧张的气氛中结束了。\n给我的感受 看完这两部动漫已经很久了，但是仍然给我一种意犹未尽的感觉，最为明显的情感就是让我觉得在中国，在动漫的国创领域，仍然有这一批热爱动漫，能够写出好故事，能够通过讲好故事带给我们不一样的感受，带给我们动漫魅力的这样一群人。国创的在建模和画风上也在寻求创新以及更适合年轻人观感的改变。故事也具有较为厚重的情感，不像以前的日漫虽有情感，却给我一种较为单薄站不住脚的感觉（当然也不能否认有情感浓重的日漫）。《灵笼》带来的是宏大的世界观和巨大的创作空间。《时光代理人》带来的则是新颖的设定和令人出乎意料的编剧创作。两部动漫都以自己独特的一部分吸引着观众，也引起了在B站的极大热度。当然期待越高，不可避免的便是可能会带来失望。高开低走的动漫已经不少，我只希望拥有这么好的创作背景和创作团队，这两部动漫能够好好的珍惜，能够用尽心思来为观众呈现国漫的另一番风景，至少不要让观众对这两部动漫的呼声成为空喊，也不要让推荐这两部动漫的人最后感到太大的失望。\n在最后还是希望国创能够走出一条更加新奇的道路，也希望这两部动漫都能够继续自己第一季的辉煌。\n我喜欢的《时光代理人》画风 ","date":"2022-01-18T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/%E8%A7%82%E6%97%B6%E5%85%89%E4%BB%A3%E7%90%86%E4%BA%BA%E7%81%B5%E7%AC%BC%E6%89%80%E6%84%9F/","title":"观《时光代理人》、《灵笼》所感"},{"content":"本来是不想用这篇博文作为新博客的第一篇的，但无奈修改这个主题以便让我能够看得舒服确实花费了我很多精力，因此想写下这一篇博文来记录这一历程，当然这次的修改也只是建博客时对于我来说最难以忍受的痛点，如果在使用的时候觉得还有值得修改的地方，那说不定就有第二站。不废话，直接开冲！\n自定义的项目 TO-DO · 修改文章的字体为思源宋体（其实我喜欢行楷，但苦于还没有找到解决方法。。）\n· 修改站点图标\n· 修改站点名称并使其居中，同时具有一定的间隔。\n· 修改选中的样式\n· 站点头像居中（虽然本来应该就是居中了。。）\n· 添加主页按钮点击不会高亮的问题\n· 主页样式的修改\n· 修改左边栏头像大小\n· 返回顶部功能的添加\n· 修改Toc的样式\n· 修改分类的标签样式\n· 修改滚动条样式\n· 使文章支持数学公式编辑\n具体修改 修改全站的字体 其实修改字体的方案主题的原作者已经给出，有需要的可以参考修改字体，作为一个中国人，好康的中文永远比好康的英文重要。 具体做法是在站点根目录中的layouts/partials/head/custom.html中添加如下代码：\n\u0026lt;style\u0026gt; :root { --article-font-family: \u0026#34;Noto Serif SC\u0026#34;, var(--base-font-family); } \u0026lt;/style\u0026gt; \u0026lt;script\u0026gt; // 正文自重300，标题字重700 (function () { const customFont = document.createElement(\u0026#39;link\u0026#39;); customFont.href = \u0026#34;https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;700\u0026amp;display=swap\u0026#34;; customFont.type = \u0026#34;text/css\u0026#34;; customFont.rel = \u0026#34;stylesheet\u0026#34;; document.head.appendChild(customFont); }()); \u0026lt;/script\u0026gt; 以上是作者给出的，我只是在字重上做了修改。除此之外我还修改了hugo-theme-stack/assets/scss/variable.scss文件，具体就是调整了--zh-font-family和--base-font-family的顺序，代码如下：\n--zh-font-family: \u0026#34;Noto Serif SC\u0026#34;, \u0026#34;Hiragino Sans GB\u0026#34;, \u0026#34;Droid Sans Fallback\u0026#34;, \u0026#34;Microsoft YaHei\u0026#34;; --base-font-family: \u0026#34;Noto Serif SC\u0026#34;, var(--sys-font-family), var(--zh-font-family), sans-serif; 修改站点的图标 这个问题起先困扰了我很久（虽然期间可能已经修改好了，但因为网站cookie的原因导致修改没有及时显示），具体做法如下，先将你需要的图片转换为.ico文件（其它类型的图片文件应该也行），如需转换可以借助这个网站。之后将转换后的文件保存在网站根目录的static文件夹中。之后修改主题的配置文件config.yaml。\nparams: mainSections: - post featuredImageField: image rssFullContent: true favicon: favicon.ico 修改站点名称 站点名称对大家来说应该不是什么难事，这里就说说居中以及间隔。这里需要修改主题根目录的assets/scss/partials/siderbar.scss文件。如下：\n.site-name { color: var(--accent-color); margin: 0; font-size: 1.8rem; text-align: center; // 站点名称文本居中 @include respond(2xl) { font-size: 2rem; } } .site-description { color: var(--body-text-color); font-weight: normal; margin: 10px 0; font-size: 1.2rem; text-align: center; // 站点描述居中 @include respond(2xl) { font-size: 1.6rem; } } 当然如果站点名称之间具有一定的间隔的话，应该是会更美观一点的。方法是在layouts/partials/head/custom.html文件上加入以下代码\n\u0026lt;style\u0026gt; h1 { letter-spacing: 6px; } \u0026lt;/style\u0026gt; 这个代码将会对全站的h1标题进行修改，因此业也会导致其它h1标签也发生改变，但目前只注意到进入一个分类页面时，分类的字距变大了，因此我们需要在custom.scss文件中追加如下修改：\n.section-term { font-size: 1.8rem; letter-spacing:1px; } 修改选中的样式 原来作者的选中样式和当前的大多数网站是一样的，给我一种十年前互联网的感觉，于是喜欢深色系的我修改了选中样式。需要在assets/scss/custom.scss文件中加入如下代码：\n::selection { color: #fff; background: #557697; } 站点头像居中 这个可能只是心理作用。。 同样是在assets/scss/partials/siderbar.scss文件中加入修改以下代码\n.site-avatar { position: relative; margin-left: auto; // 站点头像居中 margin-right: auto; // 站点头像居中 width: var(--sidebar-avatar-size); height: var(--sidebar-avatar-size); margin-bottom: var(--sidebar-element-separation); } 主页按钮高亮问题 这个虽然不算是什么大问题吧，但是主页不高亮我就觉得很难受。。于是就找到了layouts/partials/sidebar/left.html这个文件。我们可以看到一下代码：\n{{ $currentPage := . }} {{ range .Site.Menus.main }} {{ $active := or (eq $currentPage.Title .Name) (or ($currentPage.HasMenuCurrent \u0026#34;main\u0026#34; .)($currentPage.IsMenuCurrent \u0026#34;main\u0026#34; .)) }} 这段代码大概的逻辑就是如果当前的页面是菜单中的页面之一就设定为激活状态，而如果要让主页高亮的话需要加入以下代码：\n{{ $siteTitle := .Site.Title }} {{ $active := or ($active) (and (eq $currentPage.Title $siteTitle) (eq .Identifier \u0026#34;home\u0026#34;)) }} 这样就会使若当前的网站的标题等于网站名称就会令其为激活状态，而之后的Identifier则是为了区分主页页面，否则就会出现点击主页所有页面都是高亮的情况。\n主页样式的修改 第一次看到这个主题时就对它的主页布局感到不舒服，尤其是在笔记本电脑上挤满了整个屏幕令人难受，于是希望左右两边能够有一定的留白，这样可以美观一些。\n从开发者工具中我们可以得知主页的class是container，经过查找可以知道其样式是在assets/scss/grid.css中定义的。于是可以发现如下这段代码\n.container { margin-left: auto; margin-right: auto; .left-sidebar { max-width: var(--left-sidebar-max-width); margin-right: 1%; } .right-sidebar { max-width: var(--right-sidebar-max-width); /// Display right sidebar when min-width: lg @include respond(lg) { display: block; } } \u0026amp;.extended { @include respond(md) { max-width: 1024px; --left-sidebar-max-width: 25%; --right-sidebar-max-width: 30%; } @include respond(lg) { max-width: 1280px; --left-sidebar-max-width: 20%; --right-sidebar-max-width: 30%; } @include respond(xl) { max-width: 1536px; --left-sidebar-max-width: 15%; --right-sidebar-max-width: 25%; } } } 这段代码对左边栏和右边栏进行了最大宽度的限制，这也就是我们要修改的地方。但是这其中的各个@include函数又代表什么呢？我们可以观察asset/scss/breakpoints.scss这个文件，如下：\n$breakpoints: ( sm: 640px, md: 768px, lg: 1024px, xl: 1280px, 2xl: 1536px, ); @mixin respond($breakpoint) { @if not map-has-key($breakpoints, $breakpoint) { @warn \u0026#34;\u0026#39;#{$breakpoint}\u0026#39; is not a valid breakpoint\u0026#34;; } @else { @media (min-width: map-get($breakpoints, $breakpoint)) { @content; } } } 没有学过css的我也就只能大致看出这是封装了各种屏幕尺寸类似于map，而在笔记本电脑上，屏幕宽度是1024px以上的，因此我们只需要修改@include respond(lg)函数下的样式就可以了。在asset/scss/custom.scss下添加如下代码：\n.container { margin-left: auto; margin-right: auto; \u0026amp;.extended { /* range: 768-1024 */ @include respond(md) { max-width: 1024px; --left-sidebar-max-width: 25%; --right-sidebar-max-width: 30%; } /* range: 1024-1280 */ @include respond(lg) { // 修改主页的三个组件的显示比例 max-width: 1280px; --left-sidebar-max-width: 25%; --right-sidebar-max-width: 22%; } } \u0026amp;.compact { @include respond(md) { --left-sidebar-max-width: 25%; max-width: 768px; } @include respond(lg) { max-width: 1024px; --left-sidebar-max-width: 20%; } @include respond(xl) { max-width: 1280px; } } } 至此，主页的修改也告一段落了。以上都是从这位大佬那里学来。\n修改左侧边栏头像的大小 左侧边栏的元素太多，显得实在有点挤，于是索性修改了一下博客的头像大小。需要在主题根目录下的assets/scss/partials/sidebar.scss文件。\n@include respond(2xl) { --sidebar-avatar-size: 125px; // 修改头像大小的代码 --sidebar-element-separation: 10px; } 返回顶部按钮的添加 在参考过其它使用这个主题的博主之后发现，如果一篇文章太长，没有返回顶部这个按钮可能会在某些情况下造成体验的不舒适，于是决定加上这个功能。最开始是希望能够在一个页面的右下角添加这个按钮，但是无法解决页面缩放时显示位置与主页面之间的关系的问题，因此决定借助目录放置于目录的下方。\n首先是按钮的函数，参考了slim主题，具体是在layouts/partials/head/script.html中加入如下代码：\n\u0026lt;script src=\u0026#34;https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script\u0026gt; // Check to see if the window is top if not then display button $(window).scroll(function() { if ($(this).scrollTop()) { $(\u0026#39;#back-to-top\u0026#39;).fadeIn(); } else { $(\u0026#39;#back-to-top\u0026#39;).fadeOut(); } }); // Click event to scroll to top $(\u0026#39;#back-to-top\u0026#39;).click(function() { $(\u0026#39;html, body\u0026#39;).animate({scrollTop: 0}, 1000); return false; }); \u0026lt;/script\u0026gt; 这段代码可以监听窗口抖动，从而控制按钮的显示和隐藏。\n然后是添加返回顶部的按钮，由于这个按钮需要和文章目录保持一个相对固定的位置，因此需要修改layouts/_default/single.html，在aside标签下加入如下代码：\n{{ $topImg := resources.Get (\u0026#34;img/top.png\u0026#34;) }} {{ $topImg := $topImg.Resize \u0026#34;40x\u0026#34; }} \u0026lt;a id=\u0026#34;back-to-top\u0026#34; href=\u0026#34;#\u0026#34;\u0026gt; \u0026lt;img src=\u0026#34;{{ $topImg.RelPermalink }}\u0026#34; /\u0026gt; \u0026lt;/a\u0026gt; 利用一张图片作为返回顶部的链接。最后我们需要通过css来为这个按钮添加样式。首先我们需要找到之前提过的custom.scss文件，在文件中加入如下代码：\n#go-top { bottom: -15px; right: 2px; display: none; position: absolute; border: 0; \u0026amp;:hover { filter: opacity(60%); // 悬停淡化 } 其中bottom值是我觉得可以避免当目录过长时会几乎占满整个屏幕，导致按钮没有地方放的问题。\n修改Toc样式 由于Toc每个标题之间的行间距有点大，于是就缩了缩，在custom.scss文件中又追加了如下的修改：\n.widget--toc { background-color: var(--card-background); border-radius: var(--card-border-radius); display: flex; flex-direction: column; color: var(--card-text-color-main); overflow: hidden; // display:inline-block; font-size: 16px; #TableOfContents { overflow-x: auto; ol, ul { margin: 10; padding: 0; } ol { list-style-type: none; counter-reset: item; li:before { counter-increment: item; content: counters(item, \u0026#34;.\u0026#34;) \u0026#34;. \u0026#34;; font-weight: bold; margin-right: 0px; } } \u0026amp; \u0026gt; ul { padding: 0 1em; } li { margin: 5px 20px; padding: 6px; \u0026amp; \u0026gt; ol, \u0026amp; \u0026gt; ul { margin-top: 10px; padding-left: 6px; margin-bottom: -5px; \u0026amp; \u0026gt; li:last-child { margin-bottom: 0; } } } } } 追加了一些修改，在使用了一段时间后发现，每个目录项前面的小点在一定程度上占了空间，挤压了目录字，导致每个目录项容易换行，以及在margin和padding上还可以有缩减的空间，于是在custom.scss文件上做了以下修改（删了一些无用代码，注释了小点，并修改了一些间距）\n.widget--toc { // display:inline-block; font-size: 16px; #TableOfContents { overflow-x: auto; max-height: 70vh; ol, ul { list-style-type: none; margin: 0; padding: 0; } // \u0026amp; \u0026gt; ul { // padding: 0 1em; // } li { margin: 5px 10px; padding: 6px; \u0026amp; \u0026gt; ol, \u0026amp; \u0026gt; ul { margin-top: 10px; padding-left: 8px; margin-bottom: -5px; \u0026amp; \u0026gt; li:last-child { margin-bottom: 0; } } } } } 修改分类标签样式 这个stack主题属实是有点迷，一样的主题推送到GitHub居然能产生不同的效果，无奈的我只能又去修改原先又大又丑的分类标签，根据我个人的喜好，将标签修改为小巧一点，放弃了原来的图片修饰，相关的代码写于custom.scss文件中：\n.subsection-list { margin-bottom: var(--section-separation); overflow-x: auto; .article-list--tile { display: flex; padding-bottom: 15px; article { width: 200px; // 修改分类的标签大小 height: 50px; margin-right: 5px; flex-shrink: 0; // box-shadow: var(--shadow-l2); .article-title { margin: 0; font-size: 1.5rem; text-align: center; // 保证字居中美观一点 } .article-details { padding: 20px; justify-content: center; } } } } 修改相关文章图标 样式 在第一次推送到GitHub之后发现，相关文章的推荐标签有点与当前屏幕不相符，于是决定将相关文章的标签的大小调整到与分类的大小差不多大，于是我们依旧在custom.scss文件中追加修改样式的代码：\n.related-contents--wrapper { .related-contents { article { width: 200px; height: 85px; .article-title { font-size: 1.6rem; } } } } 修改滚动条样式 当文章的目录太长时便会产生滚动条，而默认的滚动条实在是丑得不想形容。由于不是很想再调试了，于是再次参考大佬的操作，而且既然想改滚动条了，索性一步到位直接全改，我们需要在custom.scss文件中添加如下代码：\nhtml { ::-webkit-scrollbar { width: 20px; } ::-webkit-scrollbar-track { background-color: transparent; } ::-webkit-scrollbar-thumb { background-color: #d6dee1; border-radius: 20px; border: 6px solid transparent; background-clip: content-box; } ::-webkit-scrollbar-thumb:hover { background-color: #a8bbbf; } } 支持数学公式编辑 在一次使用了markdown内联的$LaTeX$公式发现部署上去之后没有反应时，便一直想使文章能够支持数学公式的编辑。于是在时隔多日之后终于有了解决办法。\n我们需要在主题根目录的/layouts/partials创建math.html文件，之后在这个文件中加入如下代码\n{{ if or .Params.math .Site.Params.math }} {{ partial \u0026#34;math.html\u0026#34; . }} {{ end }} \u0026lt;link rel=\u0026#34;stylesheet\u0026#34; href=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.css\u0026#34; integrity=\u0026#34;sha384-MlJdn/WNKDGXveldHDdyRP1R4CTHr3FeuDNfhsLPYrq2t0UBkUdK2jyTnXPEK1NQ\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/katex.min.js\u0026#34; integrity=\u0026#34;sha384-VQ8d8WVFw0yHhCk5E8I86oOhv48xLpnDZx5T9GogA/Y84DcCKWXDmSDfn13bzFZY\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script defer src=\u0026#34;https://cdn.jsdelivr.net/npm/katex@0.15.2/dist/contrib/auto-render.min.js\u0026#34; integrity=\u0026#34;sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; onload=\u0026#34;renderMathInElement(document.body);\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; 这段代码能够自动渲染数学公式，当然这样还是不够的，我们还需要在每篇文章的markdown的header加上math=\u0026quot;true\u0026quot;的选择字段。\n","date":"2022-01-15T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/hugo%E5%8D%9A%E5%AE%A2-stack%E4%B8%BB%E9%A2%98%E4%BF%AE%E6%94%B9%E7%AC%AC%E4%B8%80%E7%AB%99/","title":"Hugo博客 | stack主题修改第一站"},{"content":"题目描述 请实现 copyRandomList 函数，复制一个复杂链表。在复杂链表中，每个节点除了有一个 next 指针指向下一个节点，还有一个 random 指针指向链表中的任意节点或者 null。\n示例 输入：head = [[7,null],[13,0],[11,4],[10,2],[1,0]]\r输出：[[7,null],[13,0],[11,4],[10,2],[1,0]] 解题 题目的意思很简单，就是返回一个一模一样的链表头结点。由于随机指针的存在，链表的复制不能够像正常链表那样直接遍历，也因此没想明白要怎么做，最后翻了题解，才发现又是我最烦的递归。\n这道题采用的递归解法其实本质就是遇到问题再解决问题，在还没有开始复制之前，所有的复制结点都是虚无的，要让这些结点和已知的结点一一对应起来就需要一个map数据结构。用来对应新旧两个链表的结点。这样从第一个结点入手，当这个结点不在map中时，就立刻创建这个结点并于原链表中的相应结点建立对应关系。之后的每一个结点都可以根据这样的逻辑进行创建，而因为有map这个数据结构的存在，这样每个结点都不是虚空存在的，而是可以在map中找到与之对应的结点。可以解决随机结点创建的问题。具体代码如下：\nclass Solution { public: unordered_map\u0026lt;Node*, Node*\u0026gt; um; // 用来存储两个链表，结点之间一一对应 Node* copyRandomList(Node* head) { if (head == NULL) { // 如果链表本就为空，那么直接返回空 return NULL; } // 遵循边遍历边创建的原则 if (!um.count(head)) { // 如果此时哈希表中没有这个结点，就直接创建 Node* headNew = new Node(head-\u0026gt;val); // 初始化一个相同的结点 um.insert(make_pair(head, headNew)); // 将这两个位置一样的结点对应起来 headNew-\u0026gt;next = copyRandomList(head-\u0026gt;next); // 同理下一个结点也应该这样拷贝，依旧遵循上面的原则 headNew-\u0026gt;random = copyRandomList(head-\u0026gt;random); } // 如果这个结点已经在哈希表中存在，那么就直接返回 return um[head]; } }; ","date":"2022-01-10T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-%E5%A4%8D%E6%9D%82%E9%93%BE%E8%A1%A8%E7%9A%84%E5%A4%8D%E5%88%B6/","title":"LeetCode 复杂链表的复制"},{"content":"题目描述 给定一个链表，判断链表中是否有环。\n如果链表中有某个节点，可以通过连续跟踪 next 指针再次到达，则链表中存在环。 为了表示给定链表中的环，我们使用整数 pos 来表示链表尾连接到链表中的位置（索引从 0 开始）。 如果 pos 是 -1，则在该链表中没有环。注意：pos 不作为参数进行传递，仅仅是为了标识链表的实际情况。\n如果链表中存在环，则返回 true 。 否则，返回 false 。\n示例 在这里插入图片描述 输入：head = [3,2,0,-4], pos = 1\r输出：true\r解释：链表中有一个环，其尾部连接到第二个节点。 在这里插入图片描述 输入：head = [1,2], pos = 0\r输出：true\r解释：链表中有一个环，其尾部连接到第一个节点。 在这里插入图片描述 输入：head = [1], pos = -1\r输出：false\r解释：链表中没有环。 提示 链表中节点的数目范围是 [0, 104]\r-105 \u0026lt;= Node.val \u0026lt;= 105\rpos 为 -1 或者链表中的一个 有效索引 。 解题想法 这道题在leetcode上属于简单题，但由于第一次遇见这种解法（也由于第一次想错了，根本不是正解）所以就想记录一下。 很明显题目要判断链表中是否有环，于是可以想到如果在链表中有两个指针分别向前跑，当两个指针指向的结点相同的时候便证明链表中有环存在。这个时候就需要一个循环来判断两个指针是否相等。此时条件应该是first != second因此如果我们初始化两个指针在同一个位置，那么将无法进入循环，因此需要将两个指针分别初始化在头结点以及头结点的下一个结点，还要注意的是，first指针的速度应该要快于second指针的速度，这样，当链表中有环时，first指针会先进入环中，并一直在环中循环，而当second指针进入环时，由于first指针速度快，将会在某个循环时刻追上second指针，这样当两个指针重合后就能够判断出链表存在环，下面附上C++代码\n/** * Definition for singly-linked list. * struct ListNode { * int val; * ListNode *next; * ListNode(int x) : val(x), next(NULL) {} * }; */ class Solution { public: bool hasCycle(ListNode *head) { if (head == nullptr || head-\u0026gt;next == nullptr) { return false; } ListNode *first = head-\u0026gt;next; ListNode *second = head; while (first != second) { if (first == nullptr || first-\u0026gt;next == nullptr) { return false; } second = second-\u0026gt;next; first = first-\u0026gt;next-\u0026gt;next; } return true; } }; ","date":"2021-01-18T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-141-%E7%8E%AF%E5%BD%A2%E9%93%BE%E8%A1%A8/","title":"LeetCode 141 环形链表"},{"content":"题目描述 返回 A 的最短的非空连续子数组的长度，该子数组的和至少为 K 。\n如果没有和至少为 K 的非空子数组，返回 -1\n示例 输入：A = [1], K = 1\r输出：1\r输入：A = [1,2], K = 4\r输出：-1\r输入：A = [2,-1,2], K = 3\r输出：3 提示 1. 1 \u0026lt;= A.length \u0026lt;= 50000\r2. -10 ^ 5 \u0026lt;= A[i] \u0026lt;= 10 ^ 5\r3. 1 \u0026lt;= K \u0026lt;= 10 ^ 9 首先对于我这样一个不怎么有基础的人来说，这道题着实是令我作呕。。解题也是通过了其它题解的启发。下面就开始吧。\n解题想法 由于题目中已经明确表示了数组的长度为50000，因此如果要使用暴力接法，势必造成O(n^2)的时间复杂度。因此就要想办法让i， j两个循环变量只跑一遍。而对于这道题因为绕不开要求取数组中的区间和。所以可以采用的一个方法是前缀和思想。大致的意思就是sum[i] = array[0] + array[1] + \u0026hellip; + array[i - 1]利用这个前缀和数组就可以较为方便的得到一个数组的区间和，例如要知道区间[1, 3]的和，我们可以利用前缀和数组得到sum[4] - sum[1]（这里需要注意的是，为了便于计算前缀和数组的第零个元素默认为零）\n解决了前缀和之后就要解决更加关键的问题，如何能够得到最短长度的和最小序列。首先我们不妨先这么想，如果我们当前的索引为i， 此时若区间[0, i]的和比区间[0, i - 1]的和要小，那么就不需要再考虑i - 1这个位置的索引了（因为这个时候有sum[A.length] - sum[i] \u0026gt; sum[A.length] - sum[i - 1] 且前者的区间长度要小于后者）而这个时候就要原先的存储的i - 1这个索引弹出，再将更优的i索引加入。这个时候可以利用队列这样的数据结构来保存。有了这个存删的机制就可以保证每次能够得到的长度都是最短且和最大的。之后就是获得长度的问题了。如果这个队列中还有元素的话，就从当前遍历到的索引i的前缀和减去队列头部保存的索引，即（sum[i] - sum[queue.front()]）这个条件需要满足题目中给出的K。由此可以看出这个队列需要两端都能出，因此我们需要维护一个单调递增的双端队列。下面附上C++代码\nclass Solution { public: int shortestSubarray(vector\u0026lt;int\u0026gt;\u0026amp; vec, int threshold) { int len = vec.size(); int minLength = 5e4 + 11; deque\u0026lt;int\u0026gt; dque; vector\u0026lt;int\u0026gt; sum(len + 1, 0); // 初始化前缀和数组 for (int i = 1; i \u0026lt;= len; ++i) { sum[i] = sum[i - 1] + vec[i - 1]; } for (int i = 0; i \u0026lt; len + 1; ++i) { if (i != 0) { // 满足性质就将队列尾部的索引给删除不再考虑 while (dque.size() \u0026amp;\u0026amp; sum[dque.back()] \u0026gt;= sum[i]) { dque.pop_back(); } // 从队列头部开始寻找满足条件的最短区间 // 如果不满足条件就要将队列头部元素删除 while (dque.size() \u0026amp;\u0026amp; sum[i] - sum[dque.front()] \u0026gt;= threshold) { minLength = min(minLength, i - dque.front()); dque.pop_front(); } } dque.push_back(i); } return minLength == 5e4 + 11 ? -1 : minLength; } }; ","date":"2021-01-18T00:00:00Z","permalink":"https://MUNLELEE.github.io/post/leetcode-862-%E5%92%8C%E8%87%B3%E5%B0%91%E4%B8%BAk%E7%9A%84%E6%9C%80%E7%9F%AD%E5%AD%90%E6%95%B0%E7%BB%84/","title":"LeetCode 862 和至少为K的最短子数组"}]